.packageName <- "MLwrap"
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/Bayesian_Optimization.R"
tune_models_bayesian <- function(analysis_object, sampling_method, metrics, seed = 123){

  bayes_control <-
    tune::control_bayes(
      no_improve    = 5L,
      time_limit    = 20,
      save_pred     = TRUE,
      save_workflow = TRUE
    )

  extracted_hyperparams <- extract_hyperparams(analysis_object)

  tuner_object <- tryCatch(
    suppressMessages(suppressWarnings(
      tune::tune_bayes(
        object     = analysis_object$workflow,
        resamples  = sampling_method,
        iter       = 25L,
        control    = bayes_control,
        initial    = 20,
        param_info = extracted_hyperparams,
        metrics    = metrics
      )
    )),
    error = function(e) {
      msg <- conditionMessage(e)
      if (grepl("\\$ operator is invalid for atomic vectors", msg, fixed = TRUE)) {
        if (base::interactive()) {
          cli::cli_alert_warning("Error interno de metricas suprimido; el tuning continua.")
        }
        return(tibble::tibble(.config = NA)[0, ])
      }
      stop(e)
    }
  )

  return(tuner_object)
}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/GridSearchCV.R"
tune_models_grid_search_cv <- function(analysis_object, sampling_method, metrics, seed = 123){

  grid_hyperparams = hyperparams_grid(analysis_object$hyperparameters)

  grid_control <- tune::control_grid(

    allow_par     = TRUE,
    save_pred     = TRUE,
    save_workflow = TRUE,
    parallel_over = NULL

  )

  tuner_object <- tune::tune_grid(

    object = analysis_object$workflow,
    resamples = sampling_method,
    metrics = metrics,
    control = grid_control,
    grid = grid_hyperparams

  )

  return(tuner_object)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/Hstats.R"
calc_hstats <- function(analysis_object, use_test = FALSE){

  task <- analysis_object$task
  outcome_levels <- analysis_object$outcome_levels

  if (task == "regression"){

    h2_tables <- calc_hstats_regression(analysis_object, use_test)

  } else if (outcome_levels == 2){

    h2_tables <- calc_hstats_binary(analysis_object, use_test)

  } else {

    h2_tables <- calc_hstats_multiclass(analysis_object, use_test)

  }

  return(h2_tables)

}


calc_hstats_regression <- function(analysis_object, use_test){

  if (use_test){

    train_data <- analysis_object$data$raw$test_data %>%
      dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  } else {

  train_data <- analysis_object$data$raw$train_data %>%
    dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  }

  hstats_object <- comp_hstats(analysis_object$final_model,
                               train_data,
                               task = "regression",
                               outcome_levels = analysis_object$outcome_levels)

  # Total H2

  num   <- hstats_object$h2_overall$num
  denom <- hstats_object$h2_overall$denom
  h2_normalized <- as.data.frame(round(sweep(num, 2, denom, "/"), 5))
  colnames(h2_normalized) <- "H^2 Normalized"

  h2_table <- h2_normalized %>%
    dplyr::mutate(Feature = rownames(.)) %>%
    dplyr::relocate(Feature, .before = 1) %>%
    dplyr::arrange(dplyr::desc(rowMeans(dplyr::select(., -Feature))))

  rownames(h2_table) <- NULL

  # Pairwise H2

  num   <- hstats_object$h2_pairwise$num
  denom <- hstats_object$h2_pairwise$denom

  h2_pairwise_norm <- round(num / denom, 5)
  h2_pairwise_norm <- h2_pairwise_norm[order(-h2_pairwise_norm$h2_value),, drop = FALSE ]
  h2_pairwise_norm_table <- as.data.frame(h2_pairwise_norm)
  colnames(h2_pairwise_norm_table) <- "H^2 Normalized"

  h2_pairwise_raw <- round(num, 5)
  h2_pairwise_raw <- h2_pairwise_raw[order(-h2_pairwise_raw$h2_value),, drop = FALSE ]
  h2_pairwise_raw_table <- as.data.frame(h2_pairwise_raw)
  colnames(h2_pairwise_raw_table) <- "H^2 Raw"

  h2_pairwise_norm_table <- h2_pairwise_norm_table %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1)

  h2_pairwise_raw_table <- h2_pairwise_raw_table %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1)

  rownames(h2_pairwise_norm_table) <- NULL
  rownames(h2_pairwise_raw_table) <- NULL

  return(list(h2_total = h2_table,
              h2_pairwise_norm = h2_pairwise_norm_table,
              h2_pairwise_raw = h2_pairwise_raw_table))

}


calc_hstats_binary <- function(analysis_object, use_test){

  if (use_test){

    train_data <- analysis_object$data$raw$test_data %>%
      dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  } else {

    train_data <- analysis_object$data$raw$train_data %>%
      dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  }

  hstats_object <- comp_hstats(analysis_object$final_model,
                               train_data,
                               task = "classification",
                               outcome_levels = analysis_object$outcome_levels)

  # Total H2

  num   <- hstats_object$h2_overall$num
  denom <- hstats_object$h2_overall$denom
  h2_normalized <- as.data.frame(round(sweep(num, 2, denom, "/"), 5))
  colnames(h2_normalized) <- "H^2 Normalized"

  h2_table <- h2_normalized %>%
    dplyr::mutate(Feature = rownames(.)) %>%
    dplyr::relocate(Feature, .before = 1) %>%
    dplyr::arrange(dplyr::desc(rowMeans(dplyr::select(., -Feature))))

  rownames(h2_table) <- NULL

  # Pairwise H2

  num   <- hstats_object$h2_pairwise$num
  denom <- hstats_object$h2_pairwise$denom

  h2_pairwise_norm <- round(num / denom, 5)
  h2_pairwise_norm <- h2_pairwise_norm[order(-h2_pairwise_norm$h2_value), , drop = FALSE]
  h2_pairwise_norm_table <- as.data.frame(h2_pairwise_norm)
  colnames(h2_pairwise_norm_table) <- "H^2 Normalized"

  h2_pairwise_raw <- round(num, 5)
  h2_pairwise_raw <- h2_pairwise_raw[order(-h2_pairwise_raw$h2_value),, drop = FALSE ]
  h2_pairwise_raw_table <- as.data.frame(h2_pairwise_raw)
  colnames(h2_pairwise_raw_table) <- "H^2 Raw"

  h2_pairwise_norm_table <- h2_pairwise_norm_table %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1)

  h2_pairwise_raw_table <- h2_pairwise_raw_table %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1)

  rownames(h2_pairwise_norm_table) <- NULL
  rownames(h2_pairwise_raw_table) <- NULL

  return(list(h2_total = h2_table,
              h2_pairwise_norm = h2_pairwise_norm_table,
              h2_pairwise_raw = h2_pairwise_raw_table))

}


calc_hstats_multiclass <- function(analysis_object, use_test){

  if (use_test){

    train_data <- analysis_object$data$raw$test_data %>%
      dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  } else {

    train_data <- analysis_object$data$raw$train_data %>%
      dplyr::select(-dplyr::all_of(analysis_object$dep_var))

  }

  hstats_object <- comp_hstats_mult(analysis_object$final_model,
                               train_data,
                               task = "classification",
                               outcome_levels = analysis_object$outcome_levels)

  # Total H2

  num   <- hstats_object$h2_overall$num
  denom <- hstats_object$h2_overall$denom
  h2_normalized <- as.data.frame(round(sweep(num, 2, denom, "/"), 5))
  colnames(h2_normalized) <- gsub("^\\.pred_", "", colnames(h2_normalized))

  h2_table <- h2_normalized %>%
    dplyr::mutate(Feature = rownames(.)) %>%
    dplyr::relocate(Feature, .before = 1) %>%
    dplyr::arrange(dplyr::desc(rowMeans(dplyr::select(., -Feature))))

  rownames(h2_table) <- NULL

  # Pairwise H2

  num   <- hstats_object$h2_pairwise$num
  denom <- hstats_object$h2_pairwise$denom
  h2_pairwise_norm <- as.data.frame(round(num / denom, 5))
  colnames(h2_pairwise_norm) <- gsub("^\\.pred_", "",
                                     colnames(h2_pairwise_norm))

  h2_pairwise_raw <- as.data.frame(round(num, 5))
  colnames(h2_pairwise_raw) <- gsub("^\\.pred_", "",
                                     colnames(h2_pairwise_raw))

  h2_pairwise_norm_table <- h2_pairwise_norm %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1) %>%
    dplyr::arrange(dplyr::desc(rowMeans(dplyr::select(., -"Pairwise Interaction"))))

  h2_pairwise_raw_table <- h2_pairwise_raw %>%
    dplyr::mutate("Pairwise Interaction" = rownames(.)) %>%
    dplyr::relocate("Pairwise Interaction", .before = 1) %>%
    dplyr::arrange(dplyr::desc(rowMeans(dplyr::select(., -"Pairwise Interaction"))))

  rownames(h2_pairwise_norm_table) <- NULL
  rownames(h2_pairwise_raw_table) <- NULL

  return(list(h2_total = h2_table,
              h2_pairwise_norm = h2_pairwise_norm_table,
              h2_pairwise_raw = h2_pairwise_raw_table))

}

hstat_total_plot <- function(h2_total, outcome_levels){

  if (outcome_levels <= 2){

    max_x <- 1.5 * max(h2_total[["H^2 Normalized"]], na.rm = TRUE)

    p <- ggplot2::ggplot(h2_total, ggplot2::aes(x = .data[["H^2 Normalized"]],
                                      y = stats::reorder(Feature, .data[["H^2 Normalized"]]))) +
      ggplot2::geom_col(orientation = "y", fill = "orange") +
      ggplot2::geom_text(ggplot2::aes(label = sprintf("%.3f",
                                                      .data[["H^2 Normalized"]]),
                                      x = .data[["H^2 Normalized"]] + 0.1*.data[["H^2 Normalized"]]  # slight offset
      ),
      hjust = -0.2,
      size = 3) +

      ggplot2::scale_x_continuous(
        limits = c(0, max_x),
        expand = ggplot2::expansion(mult = c(0, 0.02))
      ) +

      ggplot2::labs(x = expression(H^2~"Normalized"), y = NULL,
           title = "Friedman's H-statistic")

  } else {

    h2_long <- h2_total %>%
      tidyr::pivot_longer(
        cols = -Feature,
        names_to = "Class",
        values_to = "H2"
      )

    max_x <- 1.5 * max(h2_long$H2, na.rm = TRUE)

    h2_long <- h2_long %>%
      dplyr::group_by(Class) %>%
      dplyr::mutate(Feature_ord = reorder(Feature, H2)) %>%
      dplyr::ungroup()

    p <- ggplot2::ggplot(
      h2_long,
      ggplot2::aes(
        x = H2,
        y = Feature_ord,
        fill = Class
      )
    ) +

      ggplot2::geom_col(orientation = "y") +

      ggplot2::geom_text(ggplot2::aes(label = sprintf("%.3f",
                                                      H2),
                                              x = H2 + 0.1*H2  # slight offset
      ),
      hjust = -0.2,
      size = 3) +

      ggplot2::facet_wrap(~ Class) +

      ggplot2::scale_x_continuous(
        limits = c(0, max_x),
        expand = ggplot2::expansion(mult = c(0, 0.02))
      ) +

      ggplot2::labs(
        x = expression(H^2~"Normalized"),
        y = "Feature",
        title = "Friedman's H-statistic per Class"
      ) +

      ggplot2::theme_grey() +
      ggplot2::theme(
        strip.text = ggplot2::element_text(size = 12, face = "bold"),
        axis.text.y = ggplot2::element_text(size = 8)
      )
  }

  return(p)

}

hstat_pairwise_plot <- function(h2_pairwise, outcome_levels, normalized = TRUE){

    if (normalized){

      h2_col <- "H^2 Normalized"

    } else {

      h2_col <- "H^2 Raw"

    }

    if (outcome_levels <= 2){

      max_x <- 1.5 * max(h2_pairwise[[h2_col]], na.rm = TRUE)

      p <- ggplot2::ggplot(h2_pairwise,
                        ggplot2::aes(x = .data[[h2_col]],
                        y = stats::reorder(.data[["Pairwise Interaction"]],
                                    .data[[h2_col]]))) +
        ggplot2::geom_col(orientation = "y", fill = "orange") +
        ggplot2::geom_text(ggplot2::aes(label = sprintf("%.3f",
                                                        .data[[h2_col]]),
                                        x = .data[[h2_col]] + 0.1 * .data[[h2_col]]  # slight offset
        ),
        hjust = -0.2,
        size = 3) +
        ggplot2::scale_x_continuous(
          limits = c(0, max_x),
          expand = ggplot2::expansion(mult = c(0, 0.02))
        )

    }

    else {

      h2_long <- h2_pairwise %>%
        tidyr::pivot_longer(
          cols = -'Pairwise Interaction',
          names_to = "Class",
          values_to = "H2"
        )

      max_x <- 1.5 * max(h2_long$H2, na.rm = TRUE)

      p <- ggplot2::ggplot(h2_long,
                        ggplot2::aes(x = H2,
                                  y = stats::reorder(.data[["Pairwise Interaction"]], H2),
                                  fill = Class)) +
        ggplot2::geom_col(orientation = "y") +
        ggplot2::geom_text(ggplot2::aes(label = sprintf("%.3f",
                                                        H2),
                                        x = H2 + 0.1 * H2
        ),
        hjust = -0.2,
        size = 3) +
        ggplot2::facet_wrap("Class") +
        ggplot2::scale_x_continuous(
          limits = c(0, max_x),
          expand = ggplot2::expansion(mult = c(0, 0.02))
        )


    }

    if (normalized){

      p <- p + ggplot2::labs(x = expression(H^2~"Normalized"), y = "Feature Interaction",
                            title = "Feature Interaction Plot")

    } else {

      p <- p + ggplot2::labs(x = expression(H^2~"Unnormalized"), y = "Feature Interaction",
                             title = "Feature Interaction Plot")

    }

    return(p)

}

##### Comp #####

comp_hstats <- function(model, df, task, grid_size = 20, n_sample = 300, outcome_levels = 0){

  pd <- list()
  num <- list()
  denom <- list()

  df <- df[sample(nrow(df), min(n_sample, nrow(df))), ]

  f_pred <- pred_fun(model, df, task, outcome_levels)


  f_centered <- f_pred - mean(f_pred)

  for (feature in names(df)){

    ice_df <- ice_data(model, df, task, feature,
                       outcome_levels = outcome_levels, grid_size = grid_size)

    pd_j <- ice_df %>%
      dplyr::group_by(feature_value) %>%
      dplyr::summarise(pd = mean(prediction))

    if (is.numeric(df[[feature]])){

      pd_j <- stats::approx(x = pd_j$feature_value,
                            y = pd_j$pd,
                            xout = df[[feature]],
                            rule = 2)$y

    } else {

      levels <- levels(as.factor(df[[feature]]))

      pd_j <- pd_j$pd[ match(df[[feature]], levels) ]

    }

    pd[[feature]] <- pd_j

    pd_minus_j <- ice_df %>%
      dplyr::group_by(id) %>%
      dplyr::summarise(pd_minus_j = mean(prediction))

    pd_j_centered <- pd_j - mean(pd_j)
    pd_minus_j_centered <- pd_minus_j$pd_minus_j - mean(pd_minus_j$pd_minus_j)

    num[[feature]] <- sum((f_centered - pd_j_centered  - pd_minus_j_centered)^2)

  }

  num <- as.data.frame(unlist(num), )
  colnames(num) <- "h2_value"
  rownames(num) <- names(df)

  denom <- sum((f_centered)^2)

  selected_features <- rownames(num)[order(num$h2_value, decreasing = TRUE)][1:min(5, nrow(num))]

  num_ij <- c()

  denom_ij <- c()

  interaction_name_list <- c()

  for (i in 1:length(selected_features)){

    for (j in i:length(selected_features)){

      if (i == j){next}

      feat1 <- selected_features[[i]]
      feat2 <- selected_features[[j]]

      interaction_name <- paste0(feat1,":", feat2)

      h_ij <- hstat_interaction(model, df, feat1, feat2,
                                pd[[feat1]], pd[[feat2]],
                                task = task,
                                outcome_levels = outcome_levels)

      interaction_name_list <- c(interaction_name_list, interaction_name)
      num_ij <- c(num_ij, h_ij$num)
      denom_ij <- c(denom_ij, h_ij$denom)

    }
  }

  num_df <- data.frame(
    h2_value = num_ij
  )

  denom_df <- data.frame(
    h2_value = denom_ij
  )

  rownames(num_df) <- interaction_name_list
  rownames(denom_df) <- interaction_name_list

  ord <- order(num_df$h2_value, decreasing = TRUE)

  # 2. Reorder both dataframes using the index
  num_df   <- num_df[ord, , drop = FALSE]
  denom_df <- denom_df[ord, , drop = FALSE]


  return(list(h2_overall = list(num = num, denom = denom),
              h2_pairwise = list(num = num_df, denom = denom_df))
  )

}

comp_hstats_mult <- function(model, df, task, grid_size = 20, n_sample = 300, outcome_levels = 0){

  pd <- list()
  num <- list()
  denom <- list()

  df <- df[sample(nrow(df), min(n_sample, nrow(df))), ]

  f_pred <- pred_fun(model, df, task, outcome_levels)

  f_centered <- f_pred - colMeans(f_pred)

  denom <- colSums((f_centered)^2)

  dep_levels <- names(f_centered)

  for (feature in names(df)){

    ice_df <- ice_data(model, df, task, feature,
                       outcome_levels = outcome_levels, grid_size = grid_size)

    pd_j <- ice_df %>%
      dplyr::group_by(pred_class, feature_value) %>%
      dplyr::summarise(pd = mean(prediction), .groups = "drop")

    pd_minus_j <- ice_df %>%
      dplyr::group_by(pred_class, id) %>%
      dplyr::summarise(pd_minus_j = mean(prediction), .groups = "drop")

    for (level in dep_levels){

      class_name <- sub("^\\.pred_", "", level)

      pd_j_filter <- pd_j %>% dplyr::filter(pred_class == class_name)
      pd_minus_j_filter <- pd_minus_j %>% dplyr::filter(pred_class == class_name)

      if (is.numeric(df[[feature]])){

        pd_j_filter <- stats::approx(x = pd_j_filter$feature_value,
                                     y = pd_j_filter$pd,
                                     xout = df[[feature]],
                                     rule = 2)$y

      } else {

        cat_levels <- levels(as.factor(df[[feature]]))

        pd_j_filter <- pd_j_filter$pd[ match(df[[feature]], cat_levels) ]

      }

      pd[[level]][[feature]] <- pd_j_filter

      pd_j_centered <- pd_j_filter - mean(pd_j_filter)
      pd_minus_j_centered <- pd_minus_j_filter$pd_minus_j - mean(pd_minus_j_filter$pd_minus_j)

      num[[level]][[feature]] <- sum((f_centered[[level]] - pd_j_centered  - pd_minus_j_centered)^2)

    }

  }

  for (level in dep_levels){

    num[[level]] <- unlist(num[[level]], use.names = FALSE)

  }

  num <- as.data.frame(num)
  colnames(num) <- dep_levels
  rownames(num) <- names(df)

  avg_h2 <- rowMeans(num / denom)

  selected_features <- rownames(num)[order(avg_h2, decreasing = TRUE)][1:min(5, nrow(num))]

  interaction_name_list <- c()

  num_ij <- list()
  denom_ij <- list()

  for (i in 1:length(selected_features)){

    for (j in i:length(selected_features)){

      if (i == j){next}

      feat1 <- selected_features[[i]]
      feat2 <- selected_features[[j]]

      interaction_name <- paste0(feat1,":", feat2)

      for (level in dep_levels){

        h_ij <- hstat_interaction(model, df, feat1, feat2,
                                  pd[[level]][[feat1]], pd[[level]][[feat2]],
                                  task = task,
                                  outcome_levels = outcome_levels,
                                  level = level)

        num_ij[[level]] <- c(num_ij[[level]], h_ij$num)
        denom_ij[[level]] <- c(denom_ij[[level]], h_ij$denom)

      }

      interaction_name_list <- c(interaction_name_list, interaction_name)

    }
  }

  num_df <- as.data.frame(num_ij)
  colnames(num_df) <- dep_levels
  rownames(num_df) <- interaction_name_list

  denom_df <- as.data.frame(denom_ij)
  colnames(denom_df) <- dep_levels
  rownames(denom_df) <- interaction_name_list

  avg_h2 <- rowMeans(num_df[dep_levels] / denom_df[dep_levels])

  ord <- order(avg_h2, decreasing = TRUE)

  # 2. Reorder both dataframes using the index
  num_df   <- num_df[ord, , drop = FALSE]
  denom_df <- denom_df[ord, , drop = FALSE]


  return(list(h2_overall = list(num = num, denom = denom),
              h2_pairwise = list(num = num_df, denom = denom_df))
  )

}

hstat_interaction <- function(model, data, feat1, feat2, pd_j, pd_k,
                              grid.size = 20, task = "regression",
                              outcome_levels = 0,
                              level = NULL){

  xj <- data[[feat1]]
  xk <- data[[feat2]]

  # Determine if numeric or categorical
  is.num1 <- is.numeric(xj)
  is.num2 <- is.numeric(xk)

  pd_2D <- calc_pd_2D(model, data, feat1, feat2, grid.size, task, outcome_levels, level = level)

  if (is.num1 && is.num2){

    pd_jk <- pd2D_interpolate(pd_2D$grid1, pd_2D$grid2, pd_2D$pd.matrix, xj, xk)

  }

  if (!is.num1 && is.num2){

    pd_jk <- pd_1d_interpolate_vec(pd_2D$grid, xj, xk, cat_var = 1)

  }

  if (is.num1 && !is.num2){

    pd_jk <- pd_1d_interpolate_vec(pd_2D$grid, xj, xk, cat_var = 2)

  }

  if (!is.num1 && !is.num2){

    idx <- match(
      paste(data[[feat1]], data[[feat2]]),
      paste(pd_2D$grid[[feat1]], pd_2D$grid[[feat2]])
    )

    pd_jk <- pd_2D$grid$pd[idx]

  }

  pd_jk_centered <- pd_jk - mean(pd_jk)
  pd_j_centered <- pd_j - mean(pd_j)
  pd_k_centered <- pd_k - mean(pd_k)

  num <- sum((pd_jk_centered - pd_j_centered  - pd_k_centered)^2)

  denom <- sum((pd_jk_centered)^2)

  return(list(num = num, denom = denom))


}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/HyperparametersNN.R"
HyperparamsNN <- R6::R6Class("Neural Network Hyperparameters",
                             inherit = HyperparametersBase,
                             public = list(

                               hidden_units_tune = TRUE,
                               learn_rate_tune = TRUE,
                               activation_tune = TRUE,
                               epochs = 25,

                               default_hyperparams = function() {
                                 list(learn_rate = dials::learn_rate(range = c(-3, -1)),
                                      hidden_units = dials::hidden_units(range = c(5, 20)),
                                      activation = dials::activation(values = c("relu", "tanh", "sigmoid"))
                                 )
                               },

                               check_hyperparams = function(hyperparams){

                                 valid_hparams <- c("learn_rate", "hidden_units", "activation")

                                 if (!is.null(hyperparams)){

                                   if (all(names(hyperparams) %in% valid_hparams)){

                                      for (hyp_name in names(hyperparams)){

                                        hyperparam <- hyperparams[[hyp_name]]

                                          if (length(hyperparam) > 1){

                                            if (hyperparam[1] >= hyperparam[2]){

                                              print(names(hyperparams))

                                              stop(paste0("For '", hyp_name, "' lower range (", hyperparam[1],
                                                          ") is greater or equal to upper range (", hyperparam[2],")!"))

                                            }

                                          }
                                       }

                                   }

                                   else {

                                     stop(paste0("Incorrect hyperparameter list. Valid hyperparameters are:",

                                                 paste(valid_hparams, collapse = ",")))

                                   }

                                 }


                               },

                               set_hyperparams = function(hyperparams = NULL) {

                                 default_hyperparameters <- self$default_hyperparams()

                                 # Actualizar solo los valores proporcionados

                                 if (!is.null(hyperparams)) {

                                   if ("learn_rate" %in% names(hyperparams)) {

                                     if (length(hyperparams$learn_rate) > 1){

                                       default_hyperparameters$learn_rate <- dials::learn_rate(range = hyperparams$learn_rate)

                                     } else if (!is.null(hyperparams$learn_rate)){

                                       default_hyperparameters$learn_rate <- hyperparams$learn_rate

                                       self$learn_rate_tune = F

                                       }

                                   }

                                   if ("hidden_units" %in% names(hyperparams)) {

                                     if (length(hyperparams$hidden_units) > 1){

                                       default_hyperparameters$hidden_units <- dials::hidden_units(range = hyperparams$hidden_units)

                                     } else if (!is.null(hyperparams$hidden_units)){

                                       default_hyperparameters$hidden_units <- hyperparams$hidden_units

                                       self$hidden_units_tune = F
                                     }

                                   }

                                   if ("activation" %in% names(hyperparams)) {

                                     if (length(hyperparams$activation) > 1){

                                       default_hyperparameters$activation <- dials::activation(values = hyperparams$activation)

                                     } else if (!is.null(hyperparams$activation)){

                                       default_hyperparameters$activation <- hyperparams$activation

                                       self$activation_tune = F

                                     }

                                   }

                                 }

                                 return(default_hyperparameters)

                               }

                             )

)
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/HyperparametersRF.R"
HyperparamsRF <- R6::R6Class("Random Forest Hyperparameters",

                              inherit = HyperparametersBase,

                              public = list(

                                mtry_tune = TRUE,
                                trees_tune = TRUE,
                                min_n_tune = TRUE,

                                default_hyperparams = function(){

                                  list(

                                    mtry = dials::mtry(range = c(3, 8)),
                                    trees = dials::trees(range = c(100, 300)),
                                    min_n = dials::min_n(range = c(5, 25))

                                  )

                                },

                                check_hyperparams = function(hyperparams){

                                  valid_hparams <- c("mtry", "trees", "min_n")

                                  if (!is.null(hyperparams)){

                                    if (all(names(hyperparams) %in% valid_hparams)){

                                      for (hyp_name in names(hyperparams)){

                                        hyperparam <- hyperparams[[hyp_name]]

                                        if (length(hyperparam) > 1){

                                          if (hyperparam[1] >= hyperparam[2]){

                                            print(names(hyperparams))

                                            stop(paste0("For '", hyp_name, "' lower range (", hyperparam[1],
                                                        ") is greater or equal to upper range (", hyperparam[2],")!"))

                                          }

                                        }
                                      }
                                    }

                                    else {

                                      stop(paste0("Incorrect hyperparameter list. Valid hyperparameters are:",
                                                  paste(valid_hparams, collapse = ",")))

                                    }
                                  }
                                },

                                set_hyperparams = function(hyperparams = NULL){

                                  def_hyperparams = self$default_hyperparams()

                                    if (!is.null(hyperparams)) {

                                      def_hyperparams[names(hyperparams)] <- Map(function(name, value) {

                                        if (length(value) > 1) {

                                          func <- get(name, envir = asNamespace("dials"))
                                          func(range = value)

                                        } else if (!is.null(value)){

                                          self[[paste0(name, "_tune")]] <- FALSE
                                          value
                                        } else{

                                          def_hyperparams[[name]]

                                        }
                                      }, names(hyperparams), hyperparams)
                                    }

                                  return(def_hyperparams)

                                }
                              )
)
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/HyperparametersSVM.R"
HyperparamsSVM <- R6::R6Class("Neural Network Hyperparameters",
                             inherit = HyperparametersBase,
                             public = list(

                               cost_tune = TRUE,
                               margin_tune = TRUE,
                               rbf_sigma_tune = FALSE,
                               degree_tune = FALSE,
                               scale_factor_tune = FALSE,

                               default_hyperparams = function(){
                                 list(

                                      cost = dials::cost(range = c(-3, 3)),
                                      margin = dials::svm_margin(range = c(0, 0.2))

                                 )

                               },

                               check_hyperparams = function(hyperparams){

                                 valid_hparams_linear <- c("cost", "margin", "type")
                                 valid_hparams_rbf <- c("cost", "margin", "rbf_sigma", "type")
                                 valid_hparams_poly <- c("cost", "margin", "degree", "scale_factor", "type")

                                 if (!is.null(hyperparams)){

                                   if (hyperparams$type == "linear"){

                                      if (all(names(hyperparams) %in% valid_hparams_linear)){

                                      }

                                      else {

                                     stop(paste0("Incorrect hyperparameter list for Linear Kernel. Valid hyperparameters are: ",
                                                 paste(valid_hparams_linear, collapse = ", ")))

                                      }
                                    } else if (hyperparams$type == "rbf"){

                                      if (all(names(hyperparams) %in% valid_hparams_rbf)){

                                      }

                                      else {

                                        stop(paste0("Incorrect hyperparameter list for RBF Kernel. Valid hyperparameters are: ",
                                                    paste(valid_hparams_rbf, collapse = ", ")))

                                      }
                                    } else if (hyperparams$type == "poly"){

                                      if (all(names(hyperparams) %in% valid_hparams_poly)){

                                      }

                                      else {

                                        stop(paste0("Incorrect hyperparameter list for Polynomial Kernel. Valid hyperparameters are: ",
                                                    paste(valid_hparams_poly, collapse = ", ")))

                                      }

                                    } else {

                                      stop(paste0("Incorrect kernel type. Valid options are: 'linear', 'rbf', 'poly'."))

                                    }

                                   for (hyp_name in names(hyperparams)){

                                     hyperparam <- hyperparams[[hyp_name]]

                                     if (length(hyperparam) > 1){

                                       if (hyperparam[1] >= hyperparam[2]){

                                         print(names(hyperparams))

                                         stop(paste0("For '", hyp_name, "' lower range (", hyperparam[1],
                                                     ") is greater or equal to upper range (", hyperparam[2],")!"))

                                       }

                                     }
                                   }

                                  }
                               },

                               set_hyperparams = function(hyperparams = NULL){

                                 def_hyperparams = self$default_hyperparams()

                                 if (hyperparams$type == "rbf"){

                                   self$rbf_sigma_tune = TRUE
                                   def_hyperparams$rbf_sigma = dials::rbf_sigma(range = c(-5, 0))

                                 } else if (hyperparams$type == "poly"){

                                   self$degree_tune = TRUE
                                   self$scale_factor_tune = TRUE


                                   def_hyperparams$degree = dials::degree(range = c(1,3))
                                   def_hyperparams$scale_factor = dials::scale_factor(range = c(-5, -1))

                                 }

                                 type <- hyperparams$type

                                 hyperparams$type  <- NULL

                                 if (!is.null(hyperparams)) {

                                   def_hyperparams[names(hyperparams)] <- Map(function(name, value) {

                                     if (length(value) > 1) {

                                       if (name == "margin"){name = "svm_margin"}

                                       func <- get(name, envir = asNamespace("dials"))
                                       func(range = value)

                                     } else if (!is.null(value)){

                                       self[[paste0(name, "_tune")]] <- FALSE
                                       value
                                     } else {

                                       def_hyperparams[[name]]

                                     }
                                   }, names(hyperparams), hyperparams)
                                 }

                                 def_hyperparams$type <- type

                                 return(def_hyperparams)

                               }
                             )
                          )













#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/HyperparametersXGBoost.R"
HyperparamsXGBoost <- R6::R6Class("XGBOOST Hyperparameters",
                             inherit = HyperparametersBase,
                             public = list(

                               mtry_tune = TRUE,
                               trees_tune = TRUE,
                               min_n_tune = TRUE,
                               tree_depth_tune = TRUE,
                               learn_rate_tune = TRUE,
                               loss_reduction_tune = TRUE,

                               default_hyperparams = function(){

                                 list(

                                   mtry = dials::mtry(range = c(3, 8)),
                                   trees = dials::trees(range = c(100, 300)),
                                   min_n = dials::min_n(range = c(5, 25)),
                                   tree_depth = dials::tree_depth(range = c(3L, 8L)),
                                   learn_rate = dials::learn_rate(range = c(-3, -1)),
                                   loss_reduction = dials::loss_reduction(range = c(-3, 1.5))


                                 )

                               },

                               check_hyperparams = function(hyperparams){

                                 valid_hparams <- c("mtry", "trees", "min_n", "tree_depth", "learn_rate", "loss_reduction", "sample_size")

                                 if (!is.null(hyperparams)){

                                   if (all(names(hyperparams) %in% valid_hparams)){

                                     for (hyp_name in names(hyperparams)){

                                       hyperparam <- hyperparams[[hyp_name]]

                                       if (length(hyperparam) > 1){

                                         if (hyperparam[1] >= hyperparam[2]){

                                           print(names(hyperparams))

                                           stop(paste0("For '", hyp_name, "' lower range (", hyperparam[1],
                                                       ") is greater or equal to upper range (", hyperparam[2],")!"))

                                         }

                                       }
                                     }
                                   }

                                   else {

                                     stop(paste0("Incorrect hyperparameter list. Valid hyperparameters are:",
                                                 paste(valid_hparams, collapse = ",")))

                                   }
                                 }
                               },

                               set_hyperparams = function(hyperparams = NULL){

                                 def_hyperparams = self$default_hyperparams()

                                 if (!is.null(hyperparams)) {

                                   def_hyperparams[names(hyperparams)] <- Map(function(name, value) {

                                     if (length(value) > 1) {

                                       func <- get(name, envir = asNamespace("dials"))
                                       func(range = value)

                                     } else if (!is.null(value)){

                                       self[[paste0(name, "_tune")]] <- FALSE
                                       value
                                     } else {

                                       def_hyperparams[[name]]

                                     }
                                   }, names(hyperparams), hyperparams)
                                 }

                                 return(def_hyperparams)

                               }
                             )
)

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/Integrated_Gradients.R"
IntGrad_calc <- function(model, train, test, y, task, outcome_levels, use_test = FALSE){

  if (!use_test){

    test <- train

  }

  test <- test[which(names(test) != y)]

  if (outcome_levels > 2){

    intgrads <- IntGrad_mul(model, train, test, y, outcome_levels)

  } else if (outcome_levels ==2){

    intgrads <- IntGrad_bin(model, train, test, y)

  } else {

    intgrads <- IntGrad_reg(model, train, test, y)

  }

  return(intgrads)

}


IntGrad_reg <- function(model, train, test, y){

  torch_model = torch::torch_load(model$fit$model_obj)

  # Convertir modelo a float64 (double) para compatibilidad con innsight + torch 0.16.1
  torch_model$model$to(dtype = torch::torch_float64())

  # Convertir test data a numeric
  test_normalized <- as.data.frame(lapply(test, function(x) {
    if(is.numeric(x)) as.numeric(x) else x
  }))

  # Estrategia de conversión (2 intentos con dtype = "double")
  converted_model <- tryCatch({
    # Intento 1: Conversión estándar con dtype double
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")

  }, error = function(e1) {
    # Intento 2: Eval + conversión con dtype double
    torch_model$model$eval()
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")
  })

  intgrads <- innsight::run_intgrad(converted_model, data = test_normalized)

  intgrads = as.data.frame(intgrads$get_result())
  names(intgrads) = names(test)
  intgrads = dplyr::select(intgrads, names(test))

  return(intgrads)

}


IntGrad_bin <- function(model, train, test, y){

  torch_model = torch::torch_load(model$fit$model_obj)

  # Convertir modelo a float64 (double) para compatibilidad con innsight + torch 0.16.1
  torch_model$model$to(dtype = torch::torch_float64())

  # Convertir test data a numeric
  test_normalized <- as.data.frame(lapply(test, function(x) {
    if(is.numeric(x)) as.numeric(x) else x
  }))

  # Estrategia de conversión (2 intentos con dtype = "double")
  converted_model <- tryCatch({
    # Intento 1: Conversión estándar con dtype double
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")

  }, error = function(e1) {
    # Intento 2: Eval + conversión con dtype double
    torch_model$model$eval()
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")
  })

  intgrads <- innsight::run_intgrad(converted_model, data = test_normalized, verbose = F, n = 100, output_idx = 2)

  intgrads = as.data.frame(intgrads$get_result())
  names(intgrads) = names(test)
  intgrads = dplyr::select(intgrads, names(test))

  return(intgrads)

}


IntGrad_mul <- function(model, train, test, y, outcome_levels){

  y_classes = levels(train[[y]])

  torch_model = torch::torch_load(model$fit$model_obj)

  # Convertir modelo a float64 (double) para compatibilidad con innsight + torch 0.16.1
  torch_model$model$to(dtype = torch::torch_float64())

  # Convertir test data a numeric
  test_normalized <- as.data.frame(lapply(test, function(x) {
    if(is.numeric(x)) as.numeric(x) else x
  }))

  # Estrategia de conversión (2 intentos con dtype = "double")
  converted_model <- tryCatch({
    # Intento 1: Conversión estándar con dtype double
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")

  }, error = function(e1) {
    # Intento 2: Eval + conversión con dtype double
    torch_model$model$eval()
    innsight::convert(torch_model$model, input_dim = model$fit$dims$p, dtype = "double")
  })

  intgrads <- innsight::run_intgrad(converted_model, data = test_normalized, verbose = F)

  intgrads = as.data.frame(intgrads$get_result())

  result = list()

  for (i in 1:outcome_levels){
    idx <- seq((i-1)*ncol(test) + 1, ncol(test) * i)
    sub_result <- as.data.frame(intgrads[idx])
    names(sub_result) <- names(test)
    result[[y_classes[i]]] <- sub_result
  }

  return(result)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/InterpretableML.R"
###########################
#       Interpretable ML
###########################

#' Perform Sensitivity Analysis and Interpretable ML methods
#'
#' @name sensitivity_analysis
#' @aliases sensitivity_analysis
#'
#' @description
#' As the final step in the MLwrap package workflow, this function performs
#' Sensitivity Analysis (SA) on a fitted ML model stored in an `analysis_object`
#' (in the examples, e.g., tidy_object). It evaluates the importance of
#' features using various methods such as Permutation Feature Importance (PFI),
#' SHAP (SHapley Additive exPlanations), Integrated Gradients, Olden
#' sensitivity analysis, and Sobol indices. The function generates numerical
#' results and visualizations (e.g., bar plots, box plots, beeswarm plots) to
#' help interpret the impact of each feature on the model's predictions for
#' both regression and classification tasks, providing critical insights after
#' model training and evaluation.
#'
#' Following the steps of data preprocessing, model fitting, and performance
#' assessment in the MLwrap pipeline, *sensitivity_analysis()* processes the
#' training and test data using the preprocessing recipe stored in the
#' analysis_object, applies the specified SA methods, and stores the results
#' within the analysis_object. It supports different metrics for evaluation and
#' handles multi-class classification by producing class-specific analyses and
#' plots, ensuring a comprehensive understanding of model behavior
#' (Iooss & Lemaître, 2015).
#'
#' @param analysis_object analysis_object created from fine_tuning function.
#' @param methods Method to be used. A string of the method name: "PFI"
#'     (Permutation Feature Importance), "SHAP" (SHapley Additive exPlanations),
#'     "Integrated Gradients" (Neural Network only), "Olden" (Neural Networks
#'     only), "Sobol_Jansen" (only when all input features are continuous).
#' @param  metric Metric used for "PFI" method (Permutation Feature Importance).
#'     A string of the name of metric (see Metrics).
#' @details
#' As the concluding phase of the MLwrap workflow—after data preparation,
#' model training, and evaluation—this function interprets models by
#' quantifying and visualizing feature importance. It validates input with
#' `check_args_sensitivity_analysis()`, preprocesses data using the recipe
#' stored in `analysis_object$transformer`, then calculates feature importance
#' via the specified `methods`:
#' - **PFI (Permutation Feature Importance):** Assesses importance by shuffling
#'     feature values and measuring the change in model performance (using the
#'     specified or default `metric`).
#' - **SHAP (SHapley Additive exPlanations):** Computes SHAP values to explain
#'     individual predictions by attributing contributions to each feature.
#' - **Integrated Gradients:** Evaluates feature importance by integrating
#'     gradients of the model's output with respect to input features.
#' - **Olden:** Calculates sensitivity based on connection weights, typically
#'     for neural network models, to determine feature contributions.
#' - **Sobol_Jansen:** Variance-based global sensitivity analysis that
#'     decomposes model output variance into contributions from individual
#'     features and their interactions. Quantifies how much each feature
#'     accounts for prediction variability. Only for continuous outcomes.
#'     Estimates first-order and total-order Sobol indices using the Jansen
#'     (1999) Monte Carlo estimator.
#'
#' For classification tasks with more than two outcome levels, the function
#' generates separate results and plots for each class. Visualizations include
#' bar plots for importance metrics, box plots for distribution of values, and
#' beeswarm plots for detailed feature impact across observations. All results
#' are stored in the `analysis_object` under the `sensitivity_analysis` slot,
#' finalizing the MLwrap pipeline with a deep understanding of model drivers.
#' @returns An updated \code{analysis_object} containing sensitivity
#' analysis results. Results are stored in the
#' \code{sensitivity_analysis} slot as a list, with each method's
#' results accessible by name. Generates bar, box, and beeswarm
#' plots for feature importance visualization, completing the
#' workflow with actionable insights.
#' @examples
#' # Example: Using PFI
#'
#' wrap_object <- preprocessing(
#'        df = sim_data,
#'        formula = psych_well ~ depression + life_sat,
#'        task = "regression"
#'        )
#' wrap_object <- build_model(
#'                analysis_object = wrap_object,
#'                model_name = "Random Forest",
#'                hyperparameters = list(
#'                                  mtry = 2,
#'                                  trees = 3
#'                                  )
#'                            )
#' set.seed(123) # For reproducibility
#' wrap_object <- fine_tuning(wrap_object,
#'                 tuner = "Grid Search CV",
#'                 metrics = c("rmse")
#'                 )
#' wrap_object <- sensitivity_analysis(wrap_object, methods = "PFI")
#'
#' # Extracting Results
#'
#' table_pfi <- table_pfi_results(wrap_object)
#'
#' @references
#' Iooss, B., & Lemaître, P. (2015). A review on global sensitivity
#' analysis methods. In: G. Dellino & C. Meloni (Eds.),
#' \emph{Uncertainty Management in Simulation-Optimization of
#' Complex Systems. Operations Research/Computer Science Interfaces
#' Series} (vol. 59). Springer, Boston, MA.
#' \doi{10.1007/978-1-4899-7547-8_5}
#'
#' Jansen, M. J. W. (1999). Analysis of variance designs for model output.
#' *Computer Physics Communications, 117*(1-2), 35–43.
#' \doi{10.1016/S0010-4655(98)00154-4}
#' @export
sensitivity_analysis <- function(analysis_object, methods = c("PFI"), metric = NULL, use_test = FALSE){

  check_args_sensitivity_analysis(analysis_object = analysis_object, methods = methods, metric = metric)

  analysis_object = analysis_object$clone()

  task = analysis_object$task

  y = analysis_object$dep_var

  if (task == "classification"){

    y_classes = levels(analysis_object$data$transformed$train_data[[y]])

  }

  if (use_test){

    data <- analysis_object$data$transformed$test_data

  } else {

    data <- analysis_object$data$transformed$train_data

  }

  model_parsnip <- analysis_object$final_model %>%
    tune::extract_fit_parsnip()

  if (is.null(analysis_object$sensitivity_analysis)){

    sensitivity_analysis_list = list()

  } else {

    sensitivity_analysis_list = analysis_object$sensitivity_analysis

  }

  feature_names <- analysis_object$feature_names

  plot_ob = analysis_object$plots

  table_ob = analysis_object$tables

  if ("PFI" %in% methods){

    method_name = "PFI"

    if (is.null(metric)){

      if (task == "regression"){

        metric = "rmse"

      } else{

        metric = "bal_accuracy"

      }
    }

    results <- pfi_calc(model = model_parsnip, data = data, y = y,
                        task = task, metric = metric, outcome_levels = analysis_object$outcome_levels)

    sensitivity_analysis_list[["PFI"]] <- results

    if (analysis_object$outcome_levels > 2){

      final_table <- do.call(
        rbind,
        lapply(y_classes, function(cls) {
          df <- results[[cls]]
          df$output_class <- cls
          df
        })
      )

      table_ob[["PFI"]] <- final_table
      plot_ob[["PFI_barplot"]] <- plot_multi_pfi(final_table)

    } else{

      p <- plot_barplot(results, func = NULL, title = "Permutation Feature Importance", x_label = "Importance")

      plot_name <- paste0(method_name,"_barplot")

      plot_ob[[plot_name]] = p

      table_ob[["PFI"]] <- results

    }

  }

  if ("SHAP" %in% methods){

    method_name = "SHAP"

    results <- shap_calc(model = model_parsnip, train = analysis_object$data$transformed$train_data, test = analysis_object$data$transformed$test_data, y = y,
                         task = task, outcome_levels = analysis_object$outcome_levels, use_test = use_test)

    sensitivity_analysis_list[["SHAP"]] <- results

    X_orig <- data[analysis_object$feature_names]

    if (analysis_object$outcome_levels > 2){

      final_table <- do.call(
        rbind,
        lapply(y_classes, function(cls) {
          df <- summarize_importance(
            results[[cls]],
            analysis_object$data$transformed$test_data,
            feature_names
          )
          df$output_class <- cls
          df
        })
      )

      table_ob[["SHAP"]] <- final_table

      plot_ob[["SHAP_barplot"]]  <- plot_multi_abs(final_table, "Mean |SHAP|", "Mean |SHAP| Barplot")

      plot_ob[["SHAP_directional"]] <- plot_multi_directional(final_table, "SHAP Directional Plot")

      shap_long <- build_importance_long(
        results = results,
        X_orig = X_orig,
        y_classes = y_classes
      )

      plot_ob[["SHAP_boxplot"]] <- plot_boxplot_multi(shap_long, "SHAP value", "SHAP Boxplot")

      plot_ob[["SHAP_swarmplot"]] <- plot_beeswarm_multi(shap_long, "SHAP value", "SHAP Beeswarm Plot")

    } else{

    p <- plot_barplot(results, func = function(x) mean(abs(x)),
                 func_se = function(x) sd(abs(x)) / sqrt(length(x)),
                 x_label = "Mean |SHAP|",
                 title = "Mean |SHAP| value")

    plot_name <- paste0(method_name, "_barplot")

    plot_ob[[plot_name]] = p

    p <- plot2(results, X_orig, func = function(x) mean(x),
            func_se = function(x) sd(x),
            x_label = "Mean (SHAP * sign(X))",
            title = "Directional SHAP Values")

    plot_name <- paste0(method_name, "_directional")

    plot_ob[[plot_name]] = p

    p <- plot_boxplot(results, y_label = "SHAP value", title = "SHAP Value Distribution")

    plot_name <- paste0(method_name, "_boxplot")

    plot_ob[[plot_name]] = p

    p <- plot_beeswarm(results, X_orig = X_orig, x_label = "SHAP value", title = "SHAP Swarm Plot")

    plot_name <- paste0(method_name, "_swarmplot")

    plot_ob[[plot_name]] = p

    table_name <- paste0(method_name)

    table_ob[[table_name]] <- summarize_importance(results, X_orig, feature_names)

    }

  }

  if ("Integrated Gradients" %in% methods){

    method_name = "IntegratedGradients"

    results <- IntGrad_calc(model = model_parsnip, train = analysis_object$data$transformed$train_data, test = analysis_object$data$transformed$test_data, y = y,
                            task = task, outcome_levels = analysis_object$outcome_levels, use_test = use_test)

    sensitivity_analysis_list[["IntegratedGradients"]] <- results

    X_orig <- data[analysis_object$feature_names]

    if (analysis_object$outcome_levels > 2){

      final_table <- do.call(
        rbind,
        lapply(y_classes, function(cls) {
          df <- summarize_importance(
            results[[cls]],
            analysis_object$data$transformed$test_data,
            feature_names
          )
          df$output_class <- cls
          df
        })
      )

      table_ob[["IntegratedGradients"]] <- final_table

      plot_ob[["IntegratedGradients_barplot"]] <-
        plot_multi_abs(final_table,
                       y_label = "Mean |Integrated Gradients|",
                       title = "Mean |Integrated Gradients| (Multiclass)")

      plot_ob[["IntegratedGradients_directional"]] <-
        plot_multi_directional(final_table,
                               title = "Directional Integrated Gradients (Multiclass)")

      ig_long <- build_importance_long(
        results = results,
        X_orig = X_orig,
        y_classes = y_classes
      )

      plot_ob[["IntegratedGradients_boxplot"]] <-
        plot_boxplot_multi(ig_long, y_label = "Integrated Gradient", title = "Integrated Gradient Boxplot")

      plot_ob[["IntegratedGradients_swarmplot"]] <-
        plot_beeswarm_multi(ig_long, x_label = "Integrated Gradient", title = "Integrated Gradient Beeswarm")

    } else{

      p <- plot_barplot(results, func = function(x) mean(abs(x)),
                   func_se = function(x) sd(abs(x)) / sqrt(length(x)),
                   x_label = "Mean |Integrated Gradient|",
                   title = "Mean |Integrated Gradients| "
                   )

      plot_name <- paste0(method_name,"_barplot")

      plot_ob[[plot_name]] = p

      p <- plot2(results, X_orig, func = function(x) mean(x),
            func_se = function(x) sd(x),
            x_label = "Integradient Gradient Correlation",
            title = "Directional Sensitivity of Integrated Gradients")

      plot_name <- paste0(method_name,"_directional")

      plot_ob[[plot_name]] = p

      p <- plot_boxplot(results, y_label = "Integrated Gradient value", title = "Integrated Gradients Distribution")

      plot_name <- paste0(method_name,"_boxplot")

      plot_ob[[plot_name]] = p

      p <- plot_beeswarm(results, X_orig = X_orig, x_label = "Integrated Gradient value",
                    title = "Integrated Gradients Swarm Plot")

      plot_name <- paste0(method_name,"_swarmplot")

      plot_ob[[plot_name]] = p

      table_name <- paste0(method_name)

      table_ob[[table_name]] <- summarize_importance(results, X_orig, feature_names)

    }

  }

  if ("Olden" %in% methods){

    method_name = "Olden"

    results = olden_calc(model = model_parsnip, task,
                         outcome_levels = analysis_object$outcome_levels, y_classes = y_classes)

    df_results <- as.data.frame(t(results))

    colnames(df_results) <- feature_names

    df_results <- tibble::as_tibble(df_results)

    sensitivity_analysis_list[["Olden"]] <- df_results

    if (analysis_object$outcome_levels > 2) {

      # ---------- 1. Build wide Olden table (Feature × Class) ----------
      table_olden <- df_results %>%
        dplyr::mutate(iter = dplyr::row_number()) %>%
        tidyr::pivot_longer(
          -iter,
          names_to = "Feature",
          values_to = "Importance"
        ) %>%
        dplyr::mutate(
          iter = paste0("Importance (", y_classes[iter], ")")
        ) %>%
        tidyr::pivot_wider(
          names_from = "iter",
          values_from = "Importance"
        )

      table_ob[["Olden"]] <- table_olden

      # ---------- 2. Convert to long format for faceting ----------
      olden_long <- table_olden %>%
        tidyr::pivot_longer(
          cols = -Feature,
          names_to = "output_class",
          values_to = "Importance"
        ) %>%
        dplyr::mutate(
          output_class = gsub("Importance \\(|\\)", "", output_class)
        )

      # ---------- 3. Plot ----------
      plot_ob[["Olden"]] <- plot_olden_multi(olden_long)

    } else{

      p <- olden_barplot(results, feature_names)

      plot_name <- paste0(method_name,"_barplot")

      plot_ob[[plot_name]] = p

      table_olden <- df_results %>%
        dplyr::mutate(iter = dplyr::row_number()) %>%
        tidyr::pivot_longer(-iter, names_to = "Feature", values_to = "Importance") %>%
        dplyr::mutate(iter = paste0("Importance")) %>%
        tidyr::pivot_wider(names_from = iter, values_from = Importance)

      table_ob[["Olden"]] <- table_olden

    }
  }

    if ("Sobol_Jansen" %in% methods){

      sobol <- sobol_calc(model_parsnip, analysis_object$data$transformed$train_data, task, feature_names)

      sensitivity_analysis_list[["Sobol_Jansen"]] <- sobol
      p <- sobol_plot(sobol)

      plot_ob[["Sobol_Jansen"]] = p

      results_table <- tibble::tibble(

        "Feature" = base::rownames(sobol$S),
        "First Order (S)" = sobol$S$original,
        "S StErr" = sobol$S$`std. error`,
        "S Min CI" = sobol$S$`min. c.i.`,
        "S Max CI" = sobol$S$`max. c.i.`,
        "Total Order (T)" = sobol$T$original,
        "T StErr" = sobol$T$`std. error`,
        "T Min CI" = sobol$T$`min. c.i.`,
        "T Max CI" = sobol$T$`max. c.i.`

      )

      table_ob[["Sobol_Jansen"]] <- results_table


    }

  if ("Friedman H-stat" %in% methods){


    h2_tables <- calc_hstats(analysis_object, use_test)

    table_ob[["H^2 Total"]] <- h2_tables$h2_total
    table_ob[["H^2 Pairwise Normalized"]] <- h2_tables$h2_pairwise_norm
    table_ob[["H^2 Pairwise Raw"]] <- h2_tables$h2_pairwise_raw

    # plot

    plot_ob[["H^2 Total"]] <- hstat_total_plot(h2_tables$h2_total,
                                               outcome_levels = analysis_object$outcome_levels)
    plot_ob[["H^2 Pairwise Normalized"]] <- hstat_pairwise_plot(h2_tables$h2_pairwise_norm,
                                                                outcome_levels = analysis_object$outcome_levels)
    plot_ob[["H^2 Pairwise Raw"]] <- hstat_pairwise_plot(h2_tables$h2_pairwise_raw,
                                                         outcome_levels = analysis_object$outcome_levels,
                                                         normalized = FALSE)

  }


  analysis_object$modify("sensitivity_analysis", sensitivity_analysis_list)

  analysis_object$modify("plots", plot_ob)

  analysis_object$modify("tables", table_ob)

  return(analysis_object)

}

####################
#   Global Plots   #
####################

#### plot_global

plot_barplot <- function(X, func = NULL, func_se = stats::sd, title, x_label) {

  X <- base::as.data.frame(X)

  if (!is.null(func)){

  summary_df <- tibble::tibble(
      Feature = base::colnames(X),
      Importance = base::sapply(X, func),
      StDev = base::sapply(X, func_se)
    )

  } else{summary_df <- X}



  summary_df$Feature <- factor(summary_df$Feature,
                                levels = summary_df$Feature[order(summary_df$Importance, decreasing = F)])

    p <- ggplot2::ggplot(summary_df, ggplot2::aes(x = Importance, y = Feature)) +
      ggplot2::geom_col(fill = "steelblue") +
      ggplot2::geom_errorbar(ggplot2::aes(xmin = Importance - StDev, xmax = Importance + StDev), width = 0.2) +
      ggplot2::geom_text(ggplot2::aes(label = paste0(round(Importance, 3), " (", round(StDev, 3), ")"),
                                      x = Importance +  max(abs(StDev)) * 0.2  # slight offset
                                      ),
                vjust =  -0.25,
                hjust = -0.2) +
      ggplot2::labs(
        x = x_label,
        y = "Feature",
        title = title
        ) +
      ggplot2::theme_grey() +
      ggplot2::expand_limits(x = max(summary_df$Importance) * 1.5)

  return(p)

}


plot2 <- function(X, test, func = NULL, func_se = stats::sd, title, x_label) {

  X <- base::as.data.frame(X)

  test <- base::as.data.frame(test)

  N = base::ncol(test)

  feat_names <- names(test)

  collapse_imp <- list()

  for (i in 1:N){

    collapse_imp[[feat_names[i]]] <- stats::cov(X[[i]], test[[i]]) / stats::var(test[[i]])

  }

  collapse_imp <- base::as.data.frame(collapse_imp)


   df <- tibble::tibble(
    variable = base::colnames(collapse_imp),
    importance = base::unlist(collapse_imp, use.names = FALSE)
  )

  # Order decreasing
  df$variable <- factor(df$variable, levels = df$variable[order(df$importance, decreasing = T)])

  # Plot
  p <- ggplot2::ggplot(df, ggplot2::aes(x = variable, y = importance, fill = importance > 0)) +
    ggplot2::geom_col(show.legend = FALSE) +
    ggplot2::geom_text(ggplot2::aes(label = round(importance, 3)),
                       vjust = ifelse(df$importance >= 0, -0.5, 1.2)) +
    ggplot2::scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "firebrick")) +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed") +
    ggplot2::labs(
      title = title,
      x = "Feature",
      y = "Feature Importance"
    ) +
    ggplot2::theme_grey() +
    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1)) +
    ggplot2::coord_cartesian(clip="off")

  return(p)

}

plot_boxplot <- function(X, title, y_label){

  X <- base::as.data.frame(X)

  summary_df <- tibble::tibble(
    variable = base::colnames(X),
    value = base::sapply(X, function(x) mean(abs(x)))
  )

    X_long <- tidyr::pivot_longer(
      data = X,
      cols = tidyr::everything(),
      names_to = "variable",
      values_to = "value"
    )

    X_long$variable <- factor(X_long$variable,
                                  levels = summary_df$variable[order(summary_df$value, decreasing = T)])

    p <- ggplot2::ggplot(X_long, ggplot2::aes(x = variable, y = value)) +
      ggplot2::geom_boxplot(fill = "lightgray") +
      ggplot2::labs(
        x = "Feature",
        y = y_label,
        title = title
      ) +
      ggplot2::theme_grey() +
      ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))

  return(p)

}

###### Beeswarm

plot_beeswarm <- function(X_vals, X_orig, title, x_label, color_quantiles = c(0.02, 0.98)) {

  order_df <- tibble::tibble(
    variable = base::colnames(X_vals),
    value = base::sapply(X_vals, function(x) mean(abs(x)))
  )

  summary_df <- X_vals |>
    tidyr::pivot_longer(dplyr::everything(), names_to = "variable", values_to = "value")

  summary_df$variable <- factor(summary_df$variable,
                                levels = order_df$variable[order(order_df$value, decreasing = FALSE)])

  X <- X_orig |>
    tidyr::pivot_longer(dplyr::everything(), names_to = "variable", values_to = "val_color")

  X$variable <- factor(X$variable,
                       levels = order_df$variable[order(order_df$value, decreasing = FALSE)])

  summary_df[["val_color"]] <- X[["val_color"]]

  # 5th / 95th percentiles for the color scale
  q <- stats::quantile(summary_df$val_color, probs = color_quantiles, na.rm = TRUE, names = FALSE)
  if (diff(q) == 0) q <- q + c(-1e-8, 1e-8)  # guard for constant color

  p <- ggplot2::ggplot(summary_df, ggplot2::aes(x = value, y = variable, color = val_color)) +
    ggbeeswarm::geom_quasirandom(bandwidth = 0.2, method = "pseudorandom", cex = 2, orientation = "x") +
    ggplot2::labs(x = x_label, y = "Feature", title = title) +
    ggplot2::theme_grey() +
    ggplot2::scale_color_viridis_c(
      option = "A",
      limits = q,                 # clamp to 5th/95th pct
      oob = scales::squish        # keep points, squash extremes to ends
    )

  return(p)
}

###########################
#   Prediction Wrappers   #
###########################

pred_reg <- function(object, newdata){

  return(predict(object,new_data= newdata)$.pred)

}

pred_bin <- function(object, newdata){

  return(predict(object, new_data = newdata, type = "prob")[,2][[1]])

}

pred_bin_class <- function(object, newdata){

  return(predict(object, new_data = newdata, type = "class")$.pred_class)

}

summarize_importance <- function(importance_matrix, original_df,  feature_names) {

  # Calcular media y error estandar para cada variable
  mean_importance <- colMeans(abs(importance_matrix), na.rm = TRUE)
  std_error <- apply(abs(importance_matrix), 2, function(x) sd(x, na.rm = TRUE) / sqrt(length(x)))
  directional_list = c()

  for (i in 1:length(feature_names)){

    name = feature_names[i]

    if (base::is.factor(original_df[name])){

      m1 <- base::mean(importance_matrix[original_df[name] == 1], na.rm = TRUE)
      m0 <- base::mean(importance_matrix[original_df[name] == 0], na.rm = TRUE)

      directional_list[i] <- m1 - m0

    } else {


    directional_list[i] <- stats::cov(importance_matrix[name], original_df[name]) / stats::var(original_df[name])

    }
  }

  # Crear resumen ordenado
  summary_df <- tibble::tibble(
    "Feature" = feature_names,
    "Mean_Abs_Importance" = mean_importance,
    "StDev" = std_error,
    "Directional_Importance" = directional_list
  )

  # Ordenar de mayor a menor importancia

  summary_df <- summary_df[order(-summary_df[["Mean_Abs_Importance"]]), ]

  return(summary_df)
}


#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/MulticlassPlots.R"
plot_multi_pfi <- function(df){

  x_max <- max(df$Importance) * 1.2

  p <- ggplot2::ggplot(df, ggplot2::aes(x = reorder(Feature, Importance),
                               y = Importance)) +
    ggplot2::geom_col(fill = "steelblue") +
    ggplot2::geom_errorbar(ggplot2::aes(ymin = Importance - StDev,
                                        ymax = Importance + StDev),
                           width = 0.2,
                           color = "black") +


    # ---- NEW: text labels ----
  ggplot2::geom_text(ggplot2::aes(
    label = sprintf("%.3f (%.3f)", Importance, StDev),
    y = Importance + sign(Importance) * max(abs(StDev)) * 0.2  # slight offset
  ),
  size = 3.2,
  hjust = -0.2) +
    # ---------------------------

  ggplot2::coord_flip() +
    ggplot2::facet_wrap(~ output_class, scales = "free_y") +

    # ---- EXPAND X-AXIS ----
  ggplot2::scale_y_continuous(expand = ggplot2::expansion(add= c(0, x_max))) +

    ggplot2::labs(
      title = "Permutation Feature Importance by Output Class",
      x = "Feature",
      y = "Importance"
    ) +
    ggplot2::theme_grey() +
    ggplot2::theme(
      legend.position = "none",
      strip.text = ggplot2::element_text(size = 12, face = "bold")
    )

  return(p)
}


plot_multi_directional <- function(df, title) {

  offset <- 0.05 * max(abs(df$Directional_Importance))

  # ----- FIX -----
  df2 <- df %>%
    dplyr::group_by(output_class) %>%
    dplyr::arrange(output_class, dplyr::desc(Directional_Importance)) %>%
    dplyr::mutate(
      Feature_ord = factor(Feature, levels = Feature),  # niveles fijos por clase
      label_pos = Directional_Importance +
        ifelse(Directional_Importance > 0, offset, -offset),
      vjust_pos = ifelse(Directional_Importance > 0, -0.2, 1.2)
    ) %>%
    dplyr::ungroup()
  # ----------------

  ggplot2::ggplot(df2,
                  ggplot2::aes(
                    x = Feature_ord,
                    y = Directional_Importance,
                    fill = Directional_Importance > 0
                  )) +
    ggplot2::geom_col(show.legend = FALSE) +
    ggplot2::geom_text(
      ggplot2::aes(
        y = label_pos,
        label = sprintf("%.3f", Directional_Importance),
        vjust = vjust_pos
      ),
      size = 3
    ) +
    ggplot2::scale_fill_manual(
      values = c("TRUE" = "steelblue", "FALSE" = "firebrick")
    ) +
    ggplot2::facet_wrap(~ output_class) +
    ggplot2::labs(
      title = title,
      x = "Feature",
      y = "Feature Importance"
    ) +
    ggplot2::theme_grey() +
    ggplot2::theme(
      axis.text.x  = ggplot2::element_text(angle = 45, hjust = 1),
      strip.text   = ggplot2::element_text(size = 12, face = "bold")
    )
}

plot_multi_abs <- function(df, y_label, title) {

  x_max <- max(df$Mean_Abs_Importance) * 1.2

  ggplot2::ggplot(df, ggplot2::aes(x = reorder(Feature, Mean_Abs_Importance),
                          y = Mean_Abs_Importance)) +

    ggplot2::geom_col(fill = "steelblue") +

    ggplot2::geom_errorbar(ggplot2::aes(
      ymin = Mean_Abs_Importance - StDev,
      ymax = Mean_Abs_Importance + StDev
    ), width = 0.3) +

    ggplot2::geom_text(ggplot2::aes(label = sprintf("%.3f (%.3f)",
                                           Mean_Abs_Importance, StDev),
                                    y = Mean_Abs_Importance + sign(Mean_Abs_Importance) * max(abs(StDev)) * 0.2  # slight offset
                                    ),
                       hjust = -0.2,
                       size = 3) +

    ggplot2::coord_flip() +

    ggplot2::facet_wrap(~ output_class, scales = "free_y") +

    # ---- EXPAND X-AXIS ----
  ggplot2::scale_y_continuous(expand = ggplot2::expansion(add= c(0, x_max))) +
    # ------------------------

  ggplot2::labs(
    title = title,
    x = "Feature",
    y = y_label
  ) +

    ggplot2::theme_grey() +
    ggplot2::theme(
      legend.position = "none",
      strip.text = ggplot2::element_text(size = 12, face = "bold"),
      axis.text.x = ggplot2::element_text(size = 8),
      axis.text.y = ggplot2::element_text(size = 8)
    )
}

plot_beeswarm_multi <- function(df, x_label, title) {

  q <- quantile(df$val_color, c(0.02, 0.98), na.rm = TRUE)
  if (diff(q) == 0) q <- q + c(-1e-8, 1e-8)

  ggplot2::ggplot(df, ggplot2::aes(
    x = imp_value,
    y = variable,
    color = val_color
  )) +
    ggbeeswarm::geom_quasirandom(
      bandwidth = 0.20,
      method = "pseudorandom",
      cex = 1.8,
      orientation = "x"
    ) +

    ggplot2::facet_wrap(
      ~ output_class
    ) +

    ggplot2::scale_color_viridis_c(
      option = "A",
      limits = q,
      oob = scales::squish
    ) +

    ggplot2::labs(
      title = title,
      x = x_label,
      y = "Feature"
    ) +

    ggplot2::theme_grey() +
    ggplot2::theme(
      legend.position = "right",

      # ⭐ Smaller + tilted y-axis text
      axis.text.y = ggplot2::element_text(size = 7, angle = 45, hjust = 1),

      strip.text = ggplot2::element_text(size = 12, face = "bold")
    )
}

plot_boxplot_multi <- function(df, y_label,title) {

  ggplot2::ggplot(df, ggplot2::aes(
    x = variable,
    y = imp_value
  )) +

    ggplot2::geom_boxplot(outlier.size = 0.8, alpha = 0.8) +

    ggplot2::facet_wrap(
      ~ output_class
    ) +

    ggplot2::labs(
      title = title,
      x = "Feature",
      y = y_label
    ) +

    ggplot2::theme_grey() +
    ggplot2::theme(
      legend.position = "none",

      # ⭐ Tilt & reduce size of labels
      axis.text.x = ggplot2::element_text(size = 7, angle = 45, hjust = 1),
      axis.text.y = ggplot2::element_text(size = 7),

      strip.text = ggplot2::element_text(size = 12, face = "bold")
    ) +

    # reorder features from lowest to highest importance
    ggplot2::scale_x_discrete(limits = rev(levels(df$variable)))

}

build_importance_long <- function(results, X_orig, y_classes) {

  # ---------- 1. Build long table for each class ----------
  imp_list <- lapply(
    y_classes,
    function(cls) {
      df <- results[[cls]]
      df$row_id <- seq_len(nrow(df))

      df_long <- tidyr::pivot_longer(
        df,
        -row_id,
        names_to = "variable",
        values_to = "imp_value"    # <--- ALWAYS "imp_value"
      )

      df_long$output_class <- cls
      df_long
    }
  )

  imp_long <- do.call(rbind, imp_list)

  # ---------- 2. Convert original X to long ----------
  X_long <- X_orig |>
    dplyr::mutate(row_id = dplyr::row_number()) |>
    tidyr::pivot_longer(
      -row_id,
      names_to = "variable",
      values_to = "val_color"
    )

  # ---------- 3. Join ----------
  imp_long <- dplyr::left_join(
    imp_long, X_long,
    by = c("row_id", "variable")
  )

  # ---------- 4. Order variables ----------
  order_df <- imp_long |>
    dplyr::group_by(variable) |>
    dplyr::summarise(order_val = mean(abs(imp_value))) |>
    dplyr::arrange(order_val)

  imp_long$variable <- factor(imp_long$variable,
                              levels = order_df$variable)

  return(imp_long)
}

plot_olden_multi <- function(df, title = "Olden Feature Importance") {

  offset <- 0.05 * max(abs(df$Importance))

  df2 <- df %>%
    dplyr::group_by(output_class) %>%
    dplyr::mutate(
      # per-class ordering: max positive → most negative
      Feature_ord = reorder(Feature, -Importance),

      # label placement
      label_pos  = Importance + ifelse(Importance > 0, offset, -offset),
      vjust_pos  = ifelse(Importance > 0, -0.2, 1.2)
    ) %>%
    dplyr::ungroup()

   p <- ggplot2::ggplot(df2,
                  ggplot2::aes(
                    x = Feature_ord,
                    y = Importance,
                    fill = Importance > 0
                  )
  ) +
    ggplot2::geom_col() +
    ggplot2::geom_text(
      ggplot2::aes(
        y = label_pos,
        label = sprintf("%.3f", Importance),
        vjust = vjust_pos
      ),
      size = 3
    ) +
    ggplot2::scale_fill_manual(
      values = c("TRUE" = "steelblue", "FALSE" = "firebrick")
    ) +
    ggplot2::facet_wrap(~ output_class, scales = "free_x") +
    ggplot2::labs(
      title = title,
      x = "Feature",
      y = "Importance"
    ) +
    ggplot2::theme_grey() +
    ggplot2::theme(
      legend.position = "none",
      strip.text = ggplot2::element_text(size = 12, face = "bold"),
      axis.text.x = ggplot2::element_text(size = 7, angle = 45, hjust = 1)
    )

   return(p)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/Objects.R"
#' @importFrom R6 R6Class

AnalysisObject <- R6::R6Class("AnalysisObject",

                              active = list(

                                stage = function(value) {
                                  if (missing(value)) {
                                    private$.stage
                                  } else {
                                    stop("`$stage` is read only", call. = FALSE)
                                  }
                                },

                                data = function(value) {
                                  if (missing(value)) {
                                    private$.data
                                  } else {
                                    stop("`$data` is read only", call. = FALSE)
                                  }
                                },

                                transformer = function(value) {
                                  if (missing(value)) {
                                    private$.transformer
                                  } else {
                                    stop("`$transformer` is read only", call. = FALSE)
                                  }
                                },

                                task = function(value) {
                                  if (missing(value)) {
                                    private$.task
                                  } else {
                                    stop("`$task` is read only", call. = FALSE)
                                  }
                                },

                                hyperparameters = function(value) {
                                  if (missing(value)) {
                                    private$.hyperparameters
                                  } else {
                                    stop("`$hyperparameters` is read only", call. = FALSE)
                                  }
                                },

                                model = function(value) {
                                  if (missing(value)) {
                                    private$.model
                                  } else {
                                    stop("`$model` is read only", call. = FALSE)
                                  }
                                },

                                model_name = function(value) {
                                  if (missing(value)) {
                                    private$.model_name
                                  } else {
                                    stop("`$model_name` is read only", call. = FALSE)
                                  }
                                },

                                workflow= function(value) {
                                  if (missing(value)) {
                                    private$.workflow
                                  } else {
                                    stop("`$workflow` is read only", call. = FALSE)
                                  }
                                },

                                metrics = function(value) {
                                  if (missing(value)) {
                                    private$.metrics
                                  } else {
                                    stop("`$metrics` is read only", call. = FALSE)
                                  }
                                },

                                tuner = function(value) {
                                  if (missing(value)) {
                                    private$.tuner
                                  } else {
                                    stop("`$tuner` is read only", call. = FALSE)
                                  }
                                },

                                tuner_fit = function(value) {
                                  if (missing(value)) {
                                    private$.tuner_fit
                                  } else {
                                    stop("`$tuner_fit` is read only", call. = FALSE)
                                  }
                                },

                                final_model = function(value) {
                                  if (missing(value)) {
                                    private$.final_model
                                  } else {
                                    stop("`$final_model` is read only", call. = FALSE)
                                  }
                                },

                                formula = function(value) {
                                  if (missing(value)) {
                                    private$.formula
                                  } else {
                                    stop("`$formula` is read only", call. = FALSE)
                                  }
                                },

                                fit_summary = function(value) {
                                  if (missing(value)) {
                                    private$.fit_summary
                                  } else {
                                    stop("`$fit_summary` is read only", call. = FALSE)
                                  }
                                },

                                predictions = function(value){
                                  if (missing(value)) {
                                    private$.predictions
                                  } else {
                                    stop("`$predictions` is read only", call. = FALSE)
                                  }
                                },

                                outcome_levels = function(value) {
                                  if (missing(value)) {
                                    private$.outcome_levels
                                  } else {
                                    stop("`$outcome_levels` is read only", call. = FALSE)
                                  }
                                },

                                sensitivity_analysis = function(value) {
                                  if (missing(value)) {
                                    private$.sensitivity_analysis
                                  } else {
                                    stop("`$sensitivity_analysis` is read only", call. = FALSE)
                                  }
                                },

                                plots = function(value) {
                                  if (missing(value)) {
                                    private$.plots
                                  } else {
                                    stop("`$plots` is read only", call. = FALSE)
                                  }
                                },

                                tables = function(value) {
                                  if (missing(value)) {
                                    private$.tables
                                  } else {
                                    stop("`$tables` is read only", call. = FALSE)
                                  }
                                },

                                best_hyperparameters = function(value) {
                                  if (missing(value)) {
                                    private$.best_hyperparameters
                                  } else {
                                    stop("`$best_hyperparameters` is read only", call. = FALSE)
                                  }
                                },

                                dep_var = function(value){
                                  if (missing(value)){
                                    private$.dep_var
                                  } else {
                                    stop("`$dep_var` is read only", call. = FALSE)
                                  }
                                },

                                feature_names = function(value){
                                  if (missing(value)){
                                    private$.feature_names
                                  } else {
                                    stop("`$feature_names` is read only", call. = FALSE)
                                  }
                                }

                              ),

                              private = list(
                                .stage = NULL,
                                .transformer = NULL,
                                .hyperparameters = NULL,
                                .model = NULL,
                                .model_name = NULL,
                                .workflow = NULL,
                                .metrics = NULL,
                                .tuner = NULL,
                                .tuner_fit = NULL,
                                .final_model = NULL,
                                .task = NULL,
                                .formula = NULL,
                                .fit_summary = NULL,
                                .predictions = NULL,
                                .outcome_levels = NULL,
                                .sensitivity_analysis = NULL,
                                .plots = NULL,
                                .tables = NULL,
                                .best_hyperparameters = NULL,
                                .data = NULL,
                                .dep_var = NULL,
                                .feature_names = NULL,

                                add_stage = function(stage){

                                  private$.stage <- stage

                                },

                                add_formula = function(formula){

                                  private$.formula <- formula

                                },

                                add_model = function(model){

                                  private$.model <- model

                                },

                                add_model_name = function(model_name){

                                  private$.model_name <- model_name

                                },

                                add_hyperparameters = function(hyperparameters){

                                  private$.hyperparameters <- hyperparameters

                                },

                                add_workflow = function(workflow){

                                  private$.workflow <- workflow

                                },

                                add_tuner = function(tuner){

                                  private$.tuner <- tuner

                                },

                                add_metrics = function(metrics){

                                  private$.metrics <- metrics

                                },

                                add_tuner_fit = function(tuner_fit){

                                  private$.tuner_fit <- tuner_fit

                                },

                                add_final_model = function(final_model){

                                  private$.final_model = final_model

                                },

                                add_task = function(task){

                                  private$.task = task

                                },

                                add_fit_summary = function(fit_summary){

                                  private$.fit_summary = fit_summary

                                },

                                add_predictions = function(predictions){

                                  private$.predictions = predictions

                                },

                                add_outcome_levels = function(outcome_levels){

                                  private$.outcome_levels = outcome_levels

                                },

                                add_sensitivity_analysis = function(sensitivity_analysis){

                                  private$.sensitivity_analysis = sensitivity_analysis

                                },

                                add_plots = function(plot_object){

                                  private$.plots = plot_object

                                },

                                add_tables = function(table_object){

                                  private$.tables = table_object

                                },

                                add_best_hyperparameters = function(best_hyperparameters){

                                  private$.best_hyperparameters = best_hyperparameters

                                }

                              ),

                              public = list(

                                initialize = function(full_data, transformer, task ,formula, outcome_levels){

                                  private$.transformer <- transformer
                                  private$.task <- task
                                  private$.formula <- formula
                                  private$.outcome_levels <- outcome_levels
                                  private$.stage <- "preprocessing"

                                  private$.dep_var <- all.vars(formula)[1]

                                  private$.data <- DataObject$new(full_data, task, y = private$.dep_var)

                                  private$.data$preprocess(transformer)

                                  private$.feature_names <- names(private$.data$transformed$train_data)[which(names(private$.data$transformed$train_data) != private$.dep_var)]

                                },

                                modify = function(type, value) {

                                  private_names <- names(private)

                                  # Filtrar solo los atributos privados que comienzan con "."
                                  auxiliary_funcs <- base::grep("^\\.", private_names, value = TRUE)
                                  auxiliary_funcs <- base::sub("^\\.", "", auxiliary_funcs)

                                  if (!(type %in% auxiliary_funcs)){
                                    stop("Type not valid. Use one of the following types: ", paste(auxiliary_funcs), collapse = ", ")
                                  }

                                  method_name <- paste0("add_", type)  # "add_train", "add_test", "add_hyperparameters"
                                  private_method <- get(method_name, envir = private)

                                  private_method(value)
                                }
                              )
)


HyperparametersBase <- R6::R6Class("HyperparametersBase",
                                   public = list(

                                     tuning = NULL,
                                     hyperparams_constant = NULL,
                                     hyperparams_ranges = NULL,

                                     initialize = function(hyperparams = NULL){

                                       self$tuning = FALSE
                                       self$hyperparams_constant = NULL

                                       self$check_hyperparams(hyperparams)

                                       hyperparameters <- self$set_hyperparams(hyperparams)



                                       # List values are converted to dials::value_set, else a single value


                                       # Change hyperparams to dials::parameters

                                       # Convertir a objetos dials::parameters

                                       hyperparams_ranges <- Filter(function(x) inherits(x, "param"), hyperparameters)

                                       hyperparams_constant <- Filter(function(x) !inherits(x, "param"), hyperparameters)

                                       self$hyperparams_ranges <- hyperparams_ranges

                                       self$hyperparams_constant = hyperparams_constant

                                       if (length(hyperparams_ranges) > 0){self$tuning <- TRUE}

                                     },

                                     set_hyperparams = function(hyperparams){
                                       stop("Must be implemented in the subclass")
                                     },

                                     default_hyperparams = function() {
                                       stop("Must be implemented in the subclass")
                                     },

                                     check_hyperparams = function(hyperparams){
                                       stop("Must be implemented in the subclass")
                                     }

                                   )
)

DataObject <- R6::R6Class("DataObject",
                       public = list(

                         raw = list(full_data = NULL,
                                    train_data = NULL,
                                    test_data = NULL),

                         transformed = list(train_data = NULL,
                                            test_data = NULL),

                         data_id = list(train_id = NULL,
                                        test_id = NULL),

                         initialize = function(full_data, task, y = NULL){

                          self$raw$full_data <- full_data

                           if (task == "classification"){

                             train_test_split = rsample::initial_split(self$raw$full_data, prop = 0.75,
                                                                       strata = !!y)

                           }

                           else {

                             train_test_split = rsample::initial_split(self$raw$full_data, prop = 0.75)

                           }

                           self$raw$train_data <- rsample::training(train_test_split)
                           self$raw$test_data <- rsample::testing(train_test_split)

                           self$data_id$train_id <- train_test_split$in_id
                           self$data_id$test_id <- setdiff(1:nrow(self$raw$full_data), self$data_id$train_id)


                         },

                         preprocess = function(transformer){

                            train_data <- self$raw$train_data
                            test_data <- self$raw$test_data

                            rec =  transformer %>%
                              recipes::prep(training = train_data)

                            self$transformed$train_data = recipes::bake(rec, new_data = train_data)
                            self$transformed$test_data = recipes::bake(rec, new_data = test_data)

                         }

                         )



                       )




#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/SOBOL.R"
sobol_calc <- function(model, train, task, feature_names){

  ncols = ncol(train) - 1

  X1 <- as.data.frame(matrix(stats::rnorm(1000 * ncols * ncols), ncol = ncols))
  X2 <- as.data.frame(matrix(stats::rnorm(1000 * ncols * ncols), ncol = ncols))

  names(X1) <- feature_names
  names(X2) <- feature_names



  if (task == "regression"){

    func_model_reg <- function(X) {

      predict(model, new_data = X)$.pred

    }

    res <- sensitivity::soboljansen(model = func_model_reg, X1, X2, nboot = 100, conf = 0.95)

  } else{

    func_model_bin <- function(X) {

      predict(model, new_data = X, type = "prob")[,2][[1]]

    }

    res <- sensitivity::soboljansen(model = func_model_bin, X1, X2, nboot = 100, conf = 0.95)

  }

  return(res)

}

sobol_plot <- function(sobol_result) {

  first_order <- sobol_result$S
  total_order <- sobol_result$T

  df_plot <- data.frame(
    variable = rownames(first_order),
    S1 = first_order$original,
    S1_se = first_order$`std. error`,
    ST = total_order$original,
    ST_se = total_order$`std. error`
  )

  # Order
  df_plot <- df_plot[order(df_plot$ST, decreasing = TRUE), ]
  df_plot$variable <- factor(df_plot$variable, levels = df_plot$variable)

  # Pivot
  df_long <- df_plot %>%
    tidyr::pivot_longer(cols = c(S1, ST), names_to = "type", values_to = "value")

  df_long$se <- c(df_plot$S1_se, df_plot$ST_se)

  df_long$label <- paste0(signif(df_long$value, 3), " +/- ", signif(df_long$se, 1))

  p <- ggplot2::ggplot(df_long, ggplot2::aes(x = value, y = variable, fill = type)) +
    ggplot2::geom_bar(stat = "identity",
                      position = ggplot2::position_dodge(width = 0.7),
                      width = 0.6) +
    ggplot2::geom_errorbarh(ggplot2::aes(xmin = value - se, xmax = value + se),
                            position = ggplot2::position_dodge(width = 0.7),
                            width = 0.2,
                            color = "black") +
    ggplot2::geom_text(ggplot2::aes(label = label),
                       position = ggplot2::position_dodge(width = 0.7),
                       hjust = -0.3,
                       size = 3.5) +
    ggplot2::labs(title = "Sobol Indices (First Order vs Total)",
                  x = "Sobol index", y = "Variable") +
    ggplot2::scale_fill_manual(values = c("S1" = "steelblue", "ST" = "darkorange"),
                               labels = c("First order", "Total order"),
                               name = "Type") +
    ggplot2::xlim(0, max(df_long$value + df_long$se) * 1.2) +  # espacio para etiquetas
    ggplot2::theme_minimal()

  return(p)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/Tuners.R"
create_workflow <- function(analysis_object){

  workflow = workflows::workflow() %>%
    workflows::add_recipe(analysis_object$transformer) %>%
    workflows::add_model(analysis_object$model)

  return(workflow)
}

create_metric_set <- function(metrics){

  set_metrics <- yardstick::metric_set(!!!rlang::syms(metrics))

  return(set_metrics)

}


extract_hyperparams <- function(analysis_object){

  extracted_hyperparams <-
    analysis_object$workflow %>%
    workflows::extract_parameter_set_dials() %>%
    update(!!!analysis_object$hyperparameters$hyperparams_ranges)

  return(extracted_hyperparams)

}


hyperparams_grid <- function(hyperparams, levels = 10){

  grid = dials::grid_regular(hyperparams$hyperparams_ranges, levels = levels)

  return(grid)

}

tuning_results <- function(analysis_object){

  if (analysis_object$tuner == "Bayesian Optimization"){

    p <- analysis_object$tuner_fit %>%
      tune::autoplot(type = "performance") +
      ggplot2::labs(title = "Bayesian Optimization Iteration Loss")

    plot_ob = analysis_object$plots

    plot_ob$bayesian_opt_iter_loss = p

    analysis_object$modify("plots", plot_ob)

    p <- analysis_object$tuner_fit %>%
      tune::autoplot(., search_res, type = "parameters") +
      ggplot2::labs(x = "Iterations", y = NULL, title = "Bayesian Optimization Iteration Results")

    plot_ob = analysis_object$plots

    plot_ob$bayesian_opt_iter_results = p

    analysis_object$modify("plots", plot_ob)

  }

  p <- analysis_object$tuner_fit %>%
       tune::autoplot() +
       ggplot2::labs(title = paste0(analysis_object$tuner, " Search Results"))

  plot_ob = analysis_object$plots

  plot_ob$tuner_search_results = p

  analysis_object$modify("plots", plot_ob)

  return(analysis_object)

}

check_mtry <- function(analysis_object, hyperparameters){

  analysis_object = analysis_object$clone()

  n_features = length(analysis_object$feature_names)

  if (!is.null(hyperparameters$hyperparams_constant$mtry)){

      if (hyperparameters$hyperparams_constant$mtry > n_features){

        hyperparameters$hyperparams_constant$mtry = n_features

        if (base::interactive()){cli::cli_alert_warning(paste0("'mtry' is greater than total number of features.
                                                     Setting its value to ", n_features, "."))}

      }
  }

  if (!is.null(hyperparameters$hyperparams_ranges$mtry)){

      if (hyperparameters$hyperparams_ranges$mtry$range$upper > n_features){

        hyperparameters$hyperparams_ranges$mtry$range$upper = n_features

        if (base::interactive()){
          cli::cli_alert_warning(paste0(
          "'mtry' upper range is greater than total number of features (",n_features,"). Setting its value to ", n_features, "."))}

      }

      if (hyperparameters$hyperparams_ranges$mtry$range$lower > n_features){

        stop(paste0("Lower range of 'mtry' (",hyperparameters$hyperparams_ranges$mtry$range$lower,") is greater than the number of features: ", n_features, "!"))

      }

  }

  return(hyperparameters)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/ale.R"
comp_ale <- function(model, X, feature, K = 20,
                     group = NULL, task = "regression",
                     outcome_levels = 1) {

  pred_fun <- function(model, new_data){
    if (task == "regression")
      return(predict(model, new_data = new_data, type = "numeric")[[1]])
    if (task == "classification" && outcome_levels == 2)
      return(predict(model, new_data = new_data, type = "prob")[[2]])
    if (task == "classification" && outcome_levels > 2)
      return(as.data.frame(predict(model, new_data = new_data, type = "prob")))
  }

  # ---- GROUPING MODE ----
  if (!is.null(group)) {
    grp <- X[[group]]

    if (is.numeric(grp)) {
      r <- rank(grp, na.last = "keep") / sum(!is.na(grp))
      grp_vec <- cut(r, breaks = c(0, .25, .50, .75, 1),
                     include.lowest = TRUE, labels = paste0("Q", 1:4))
    } else {
      grp_vec <- grp
    }

    groups <- unique(grp_vec)

    # compute ALE per group → each becomes long-format
    out <- lapply(groups, function(g) {
      df_sub <- X[grp_vec == g, , drop = FALSE]
      ale_df <- comp_ale(model, df_sub, feature, K, group = NULL,
                         task = task, outcome_levels = outcome_levels)
      ale_df$Level <- as.character(g)
      ale_df
    })

    return(dplyr::bind_rows(out))
  }

  # ---- MAIN ALE (NO GROUP) ----
  x <- X[[feature]]
  qs <- quantile(x, seq(0, 1, length.out = K + 1))

  # storage for all classes
  ale_vals <- matrix(0, nrow = K, ncol = max(1, outcome_levels))
  class_names <- NULL

  for (k in 1:K) {
    lo <- qs[k]
    hi <- qs[k + 1]

    idx <- which(x >= lo & x <= hi)
    if (length(idx) == 0) next

    X_lo <- X[idx, , drop = FALSE]
    X_hi <- X_lo
    X_lo[[feature]] <- lo
    X_hi[[feature]] <- hi

    pred_lo <- pred_fun(model, X_lo)
    pred_hi <- pred_fun(model, X_hi)

    # regression or binary → numeric vector
    if (is.numeric(pred_lo) && is.numeric(pred_hi)) {
      ale_vals[k, 1] <- mean(pred_hi - pred_lo)
      class_names <- "Prediction"

    } else {
      # multiclass → matrix
      diff <- as.matrix(pred_hi) - as.matrix(pred_lo)
      ale_vals[k, ] <- colMeans(diff)
      class_names <- sub("^\\.pred_", "", colnames(pred_hi))
    }
  }

  # accumulate per class
  if (length(class_names) == 1) {
    ale_acc <- matrix(cumsum(ale_vals[,1]), ncol = 1)
  } else {
    ale_acc <- apply(ale_vals, 2, cumsum)
  }

  # center per class
  ale_centered <- sweep(ale_acc, 2, colMeans(ale_acc), FUN = "-")

  ale_centered <- unname(ale_centered)
  qs_vals <- unname(qs[-1])
  class_names <- unname(class_names)

  # return long format
  out <- data.frame(
    grid = rep(qs[-1], times = length(class_names)),
    ale  = as.vector(ale_centered),
    Class = rep(class_names, each = K),
    Level = NA,
    row.names = NULL
  )

  return(out)
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/check_arguments.R"
########## Preprocessing

check_args_preprocessing <- function(df, formula, task, num_vars, cat_vars,
                         norm_num_vars, encode_cat_vars){

  df_cols = names(df)

  ## Check formula

  model.frame(formula, data = df)

  ## Check task

  check_args_list(arg = task, arg_list = c("regression", "classification"), arg_name = "task", null_valid = F)

  #### OTHER checks

  if (any(!(num_vars %in% df_cols)) && !(is.null(num_vars))) {
    message("num_vars doesn't coincide with data columns")
  }

  if (any(!(cat_vars %in% df_cols)) && !(is.null(cat_vars))) {
    message("cat_vars doesn't coincide with data columns")
  }

  if (any(!(norm_num_vars %in% df_cols)) && !(is.null(norm_num_vars)) && !(norm_num_vars == "all")) {
    message("norm_num_vars doesn't coincide with data columns")
  }

  if (any(!(encode_cat_vars %in% df_cols)) && !(is.null(encode_cat_vars)) && !(encode_cat_vars == "all")) {
    message("encode_cat_vars doesn't coincide with data columns")
  }
}

########## Model building

check_args_build_model <- function(analysis_object, model_name, hyperparameters){

  ## Check tidy_object stage

  if (!(analysis_object$stage == "preprocessing")){

    stop(paste0("You must first add a preprocessing step using preprocessing() !!"))

  }

  ## Check model_names

  check_args_list(arg = model_name, arg_list = c("Neural Network",
                                                  "Random Forest",
                                                  "XGBOOST",
                                                  "SVM"
                                                  ),
                  arg_name = "model_name", null_valid = F)

  if (model_name == "Neural Network"){

    check_args_list(arg = hyperparameters$activation, arg_list = c("relu", "sigmoid", "tanh"),
                    arg_name = "Activation Function", null_valid = T)

  }

}

########## Check fine_tuning

check_args_fine_tuning <- function(analysis_object, tuner, metrics, verbose = FALSE){

  ## Check tidy_object stage

  if (!(analysis_object$stage == "build_model")){

    stop("You must first add a model with build_model() !!")

  }

  ## Check tuner

  check_args_list(arg = tuner, arg_list = c("Bayesian Optimization", "Grid Search CV"), arg_name = "tuner",
                  null_valid = F)

  ## Check metrics

  check_args_list(arg = metrics, arg_list = names(metrics_info), arg_name = "metrics", null_valid = T)


  ## Check verbose

  check_boolean(arg = verbose, arg_name = "verbose")


}

############ Check Evaluate Model

check_args_evaluate_model <- function(analysis_object){

  if (analysis_object$stage != "fit_model"){

    stop("You must first fit a model with 'fine_tuning()'!!")

  }

}

############ Check Results Plots

check_args_regression_plot <- function(analysis_object, data_set){

  if (analysis_object$task != "regression"){

    stop("This plot is for regression task only!")

  }

  if (analysis_object$stage != "fit_model"){

    stop("Please fit a model first with fine_tuning()!")

  }

}

check_args_classification_plot <- function(analysis_object, data_set){

  if (analysis_object$task != "classification"){

    stop("This plot is for classification task only!")

  }

  if (analysis_object$stage != "fit_model"){

    stop("Please fit a model first with fine_tuning()!")

  }

}



############ Check Results

# check_args_show_results <- function(analysis_object,
#                                     summary, roc_curve, pr_curve,
#                                     gain_curve, lift_curve,
#                                     dist_by_class, reliability_plot, confusion_matrix,
#                                     scatter_residuals, scatter_predictions, residuals_dist,
#                                     new_data){
#
#   ## Check tidy_object stage
#
#   if (!(analysis_object$stage == "fit_model")){
#
#     stop("You must first fit a model with fine_tuning() !!")
#
#   }
#
#   ## Check new_data
#
#   check_args_list(arg = new_data, arg_list = c("train", "test"), arg_name = "new_data", null_valid = F)
#
#   ## Check booleans
#
#   check_boolean(arg = summary, arg_name = "summary")
#
#   check_boolean(arg = roc_curve, arg_name = "roc_curve")
#
#   check_boolean(arg = pr_curve, arg_name = "pr_curve")
#
#   check_boolean(arg = gain_curve, arg_name = "gain_curve")
#
#   check_boolean(arg = lift_curve, arg_name = "lift_curve")
#
#   check_boolean(arg = dist_by_class, arg_name = "dist_by_class")
#
#   check_boolean(arg = reliability_plot, arg_name = "reliability_plot")
#
#   check_boolean(arg = confusion_matrix, arg_name = "confusion_matrix")
#
#   check_boolean(arg = scatter_residuals, arg_name = "scatter_residuals")
#
#   check_boolean(arg = scatter_predictions, arg_name = "scatter_predictions")
#
#   check_boolean(arg = residuals_dist, arg_name = "residuals_dist")
#
#   ## Check classification plots
#
#   regression_plots = c(scatter_residuals, scatter_predictions, residuals_dist)
#
#   if (any(regression_plots) && analysis_object$task == "classification"){
#
#     stop("scatter_residuals, scatter_predictions and residuals_dist are for regression taks only!")
#
#   }
#
#   ## Check regression plot
#
#   classification_plots = c(roc_curve, pr_curve, gain_curve, lift_curve,
#                            dist_by_class, reliability_plot, confusion_matrix)
#
#   if(any(classification_plots) && analysis_object$task == "regression"){
#
#     stop("roc_curve, pr_curve, gain_curve, lift_curve, dist_by_class, reliability_plot and confusion
#          matrix are for classification task only!")
#
#   }
#
#
# }

############ Check sensitivity_analysis

check_args_sensitivity_analysis <- function(analysis_object, methods, metric){

  ## Check tidy_object stage

  if (!(analysis_object$stage == "fit_model") && (analysis_object$stage != "evaluated_model")){

    stop("You must first fit a model with fine_tuning() !!")

  }

  ## Check methods

  check_args_list(arg = methods, arg_list = c("PFI", "SHAP", "Integrated Gradients", "Olden" ,"Sobol_Jansen",
                                              "Friedman H-stat"),
                  arg_name = "methods", null_valid = F)

  if (!(analysis_object$model_name == "Neural Network") && (any(c("Integrated Gradients","Olden") %in% methods))){

    stop("Integrated Gradients and Olden's method are for Neural Networks only!!")
  }

  if (analysis_object$model_name == "SVM" && "SHAP" %in% methods){

      if (!(analysis_object$hyperparameters$hyperparams_constant$type == "linear")){

        stop("SHAP method only implemented for linear kernel SVM!")

      }
  }

  y = all.vars(analysis_object$formula)[1]

  features_names <- names(analysis_object$full_data)[which(names(analysis_object$full_data) != y)]

  if ((!all(sapply(analysis_object$train_data[features_names], is.numeric))) && ("Sobol_Jansen" %in% methods)) {

    stop("Sobol_Jansen requires all features to be numeric!")

  }

  if (("Sobol_Jansen" %in% methods) && (analysis_object$outcome_levels > 2)){

    stop("Sobol_Jansen is not available for multiclass classification!")

  }

  ## Check metric

  check_args_list(arg = metric, arg_list = names(metrics_info), arg_name = "metric")

}


########## UTILS

check_args_list <- function(arg, arg_list, arg_name, null_valid = T){

  if (!is.null(arg)){

      if (any(!(arg %in% arg_list))){

        stop(paste0(arg_name, " option not valid. Valid options are: ", paste(arg_list, collapse = ",")))

      }
  } else if (null_valid == F){

    stop(paste0("NULL value for ", arg_name, " is not allowed!!! Valid options are: ",
                paste(arg_list, collapse = ",")))

  }

}

check_boolean <- function(arg, arg_name){

  if (!is.logical(arg)){

    stop(paste0(arg_name, " must be boolean! (TRUE or FALSE)"))

  }

}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/create_nn.R"
create_nn <- function(hyperparams, task, epochs){

  hidden_units <- if (hyperparams$hidden_units_tune) tune::tune() else as.integer(hyperparams$hyperparams_constant$hidden_units)
  learn_rate <- if (hyperparams$learn_rate_tune) tune::tune() else hyperparams$hyperparams_constant$learn_rate
  activation <- if (hyperparams$activation_tune) tune::tune() else hyperparams$hyperparams_constant$activation

  model = parsnip::mlp(
    hidden_units = !!hidden_units,
    epochs = !!epochs,
    learn_rate = !!learn_rate,
    activation = !!activation
  ) %>%
    parsnip::set_engine(
      "brulee",
      optimizer = "SGD",
      stop_iter = 5,
      early_stopping = TRUE,
      batch_size = 32
    ) %>%
    parsnip::set_mode(task)

  return(model)
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/create_rf.R"
create_rf <- function(hyperparams, task){

  mtry <- if (hyperparams$mtry_tune) tune::tune() else hyperparams$hyperparams_constant$mtry
  trees <- if (hyperparams$trees_tune) tune::tune() else hyperparams$hyperparams_constant$trees
  min_n <- if (hyperparams$min_n_tune) tune::tune() else hyperparams$hyperparams_constant$min_n

  model = parsnip::rand_forest(
    mtry = !!mtry,
    trees = !!trees,
    min_n = !!min_n
  ) %>%
    parsnip::set_engine("ranger") %>%
    parsnip::set_mode(task)

  return(model)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/create_svm.R"
create_svm_rbf <- function(hyperparams, task){

  cost <- if (hyperparams$cost_tune) tune::tune() else hyperparams$hyperparams_constant$cost
  margin <- if (hyperparams$margin_tune) tune::tune() else hyperparams$hyperparams_constant$margin
  rbf_sigma <- if (hyperparams$rbf_sigma_tune) tune::tune() else hyperparams$hyperparams_constant$rbf_sigma

  if (task == "regression"){

    model = parsnip::svm_rbf(
      cost = !!cost,
      margin = !!margin,
      rbf_sigma = !!rbf_sigma
    ) %>%
      parsnip::set_engine("kernlab") %>%
      parsnip::set_mode(task)

  } else {

    model = parsnip::svm_rbf(
      cost = !!cost,
      rbf_sigma = !!rbf_sigma
    ) %>%
      parsnip::set_engine("kernlab") %>%
      parsnip::set_mode(task)

  }

  return(model)
}

create_svm_linear <- function(hyperparams, task){

  cost <- if (hyperparams$cost_tune) tune::tune() else hyperparams$hyperparams_constant$cost
  margin <- if (hyperparams$margin_tune) tune::tune() else hyperparams$hyperparams_constant$margin

  if (task == "regression"){

  model = parsnip::svm_linear(
    cost = !!cost,
    margin = !!margin
  ) %>%
    parsnip::set_engine("kernlab") %>%
    parsnip::set_mode(task)

  } else{

    model = parsnip::svm_linear(
      cost = !!cost
    ) %>%
      parsnip::set_engine("kernlab") %>%
      parsnip::set_mode(task)
  }

  return(model)
}

create_svm_poly <- function(hyperparams, task){

  cost <- if (hyperparams$cost_tune) tune::tune() else hyperparams$hyperparams_constant$cost
  margin <- if (hyperparams$margin_tune) tune::tune() else hyperparams$hyperparams_constant$margin
  degree <- if (hyperparams$degree_tune) tune::tune() else hyperparams$hyperparams_constant$degree
  scale_factor <- if (hyperparams$scale_factor_tune) tune::tune() else hyperparams$hyperparams_constant$scale_factor

  if (task == "regression"){

  model = parsnip::svm_poly(
    cost = !!cost,
    margin = !!margin,
    degree = !!degree,
    scale_factor = !!scale_factor
  ) %>%
    parsnip::set_engine("kernlab") %>%
    parsnip::set_mode(task)

  } else{

    model = parsnip::svm_poly(
      cost = !!cost,
      degree = !!degree,
      scale_factor = !!scale_factor
    ) %>%
      parsnip::set_engine("kernlab") %>%
      parsnip::set_mode(task)

  }

  return(model)
}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/create_xgboost.R"
create_xgboost <- function(hyperparams, task){

  mtry <- if (hyperparams$mtry_tune) tune::tune() else as.integer(hyperparams$hyperparams_constant$mtry)
  trees <- if (hyperparams$trees_tune) tune::tune() else as.integer(hyperparams$hyperparams_constant$trees)
  min_n <- if (hyperparams$min_n_tune) tune::tune() else as.integer(hyperparams$hyperparams_constant$min_n)
  tree_depth <- if (hyperparams$tree_depth_tune) tune::tune() else as.integer(hyperparams$hyperparams_constant$tree_depth)
  learn_rate <- if (hyperparams$learn_rate_tune) tune::tune() else as.numeric(hyperparams$hyperparams_constant$learn_rate)
  loss_reduction <- if (hyperparams$loss_reduction_tune) tune::tune() else as.numeric(hyperparams$hyperparams_constant$loss_reduction)

  model = parsnip::boost_tree(

    mtry = !!mtry,
    trees = !!trees,
    min_n = !!min_n,
    tree_depth = !!tree_depth,
    learn_rate = !!learn_rate,
    loss_reduction = !!loss_reduction

  ) %>%
    parsnip::set_engine("xgboost") %>%
    parsnip::set_mode(task)

  return(model)
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/evaluate_model.R"
# ######################################################
# #         get_results                                #
# ######################################################
#

evaluate_model <- function(analysis_object){

  analysis_object = analysis_object$clone()

  predictions = get_predictions(analysis_object, "all")

  task = analysis_object$task

  analysis_object$modify("predictions", predictions)

  pred_train = predictions %>% dplyr::filter(data_set == "train")

  pred_test = predictions %>% dplyr::filter(data_set == "test")

  summary_train = summary_results(analysis_object, pred_train, new_data = "Train")

  summary_test = summary_results(analysis_object, pred_test, new_data = "Test")

  tables <- analysis_object$tables

  if (analysis_object$outcome_levels > 2){

    tables$summary_train <- summary_train

    tables$summary_test <- summary_test

  } else {

    summary_total <- dplyr::bind_rows(summary_train, summary_test)

    tables$summary_results <- summary_total

  }

  analysis_object$modify("tables", tables)

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p <- predictions %>%
        plot_roc_curve_binary(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "ROC Curve")


    } else {

      p <-  predictions %>%
        plot_roc_curve_multiclass(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "ROC Curve")

    }

    plot_ob = analysis_object$plots

    plot_ob$roc_curve = p

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p <- predictions %>%
        plot_pr_curve_binary(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Precision Recall Curve")


    } else {

      p <- predictions %>%
        plot_pr_curve_multiclass(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Precision Recall Curve")


    }

    plot_ob = analysis_object$plots

    plot_ob$pr_curve = p

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p <- predictions %>%
        plot_gain_curve_binary() %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Gain Curve")


    } else {

      p <-predictions %>%
        plot_gain_curve_multiclass() %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Gain Curve")

    }

    plot_ob = analysis_object$plots

    plot_ob$gain_curve = p

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p <- predictions %>%
        plot_lift_curve_binary(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Lift Curve")


    } else{


      p <- predictions %>%
        plot_lift_curve_multiclass(new_data = "all") %>%
        ggplot2::autoplot() +
        ggplot2::labs(title = "Lift Curve")


    }

    plot_ob = analysis_object$plots

    plot_ob$lift_curve = p

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p_train <- pred_train %>%
        plot_dist_probs_binary("train")

      p_test <- pred_test %>%
        plot_dist_probs_binary("test")

    } else {

      p_train <- pred_train %>%
        plot_dist_probs_multiclass(data_set = "train")

      p_test <- pred_test %>%
        plot_dist_probs_multiclass(data_set = "test")

    }

    plot_ob = analysis_object$plots

    plot_ob$dist_by_class_train = p_train

    plot_ob$dist_by_class_test = p_test

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "classification"){

    if (analysis_object$outcome_levels == 2){

      p_train <- pred_train %>%
        plot_calibration_curve_binary(new_data = "train")

      p_test <- pred_test %>%
        plot_calibration_curve_binary(new_data = "test")

      plot_ob = analysis_object$plots

      plot_ob$reliability_plot_train = p_train

      plot_ob$reliability_plot_test = p_test

      analysis_object$modify("plots", plot_ob)

    }

  }

  if (task == "classification"){

    cm_train <- pred_train %>%
      plot_conf_mat(new_data = "train")

    cm_test <- pred_test %>%
      plot_conf_mat(new_data = "test")

    p_train <- cm_train %>% ggplot2::autoplot(type = "heatmap") +
      ggplot2::labs(title = "Confusion Matrix Train Data")

    p_test <- cm_test %>% ggplot2::autoplot(type = "heatmap") +
      ggplot2::labs(title = "Confusion Matrix Test Data")

    plot_ob = analysis_object$plots

    plot_ob$confusion_matrix_train = p_train

    plot_ob$confusion_matrix_test = p_test

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "regression"){

    p_train <- pred_train %>%
      plot_scatter(new_data = "train", error = TRUE)

    p_test <- pred_test %>%
      plot_scatter(new_data = "test", error = TRUE)

    plot_ob = analysis_object$plots

    plot_ob$scatter_residuals_train = p_train

    plot_ob$scatter_residuals_test = p_test

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "regression"){

    p_train <- pred_train %>%
      plot_scatter(new_data = "train", error = F)

    p_test <- pred_test %>%
      plot_scatter(new_data = "test", error = F)

    plot_ob = analysis_object$plots

    plot_ob$scatter_predictions_train = p_train

    plot_ob$scatter_predictions_test = p_test

    analysis_object$modify("plots", plot_ob)

  }

  if (task == "regression"){

    p_train <- pred_train %>%
      plot_residuals_density(new_data = "train")

    p_test <- pred_test %>%
      plot_residuals_density(new_data = "test")

    plot_ob = analysis_object$plots

    plot_ob$residuals_dist_train = p_train

    plot_ob$residuals_dist_test = p_test

    analysis_object$modify("plots", plot_ob)

  }

  analysis_object$modify("stage", "evaluated_model")

  return(analysis_object)

}

######################################################
#         get_predictions                           #
######################################################

get_predictions <- function(analysis_object, new_data = "test"){

  if (analysis_object$task == "regression"){

    predictions = get_predictions_regression(analysis_object, new_data = new_data)

  } else if (analysis_object$task == "classification"){

    predictions = get_predictions_binary(analysis_object, new_data = new_data)

    if (analysis_object$outcome_levels == 2){

      predictions = get_predictions_binary(analysis_object, new_data = new_data)

    } else {

      predictions = get_predictions_multiclass(analysis_object, new_data = new_data)

    }

  }

  return(predictions)


}

######################################################
#         SUMMARY                                    #
######################################################

summary_results <- function(analysis_object, predictions, new_data = "test"){

  if (analysis_object$task == "regression"){

    return(summary_regression(predictions, new_data))

  } else if (analysis_object$task == "classification"){

    if (analysis_object$outcome_levels == 2){

      return(summary_binary(predictions, new_data))

    } else {

      return(summary_multiclass_per_class(predictions, new_data))

    }

  }

}







#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/fine_tuning.R"
#' Fine Tune ML Model
#'
#' @description
#'
#' The **fine_tuning()** function performs automated hyperparameter
#' optimization for ML workflows encapsulated within an AnalysisObject. It
#' supports two tuning strategies: **Bayesian Optimization** (with
#' cross-validation) and **Grid Search Cross-Validation**, allowing the user
#' to specify evaluation metrics and whether to visualize tuning results. The
#' function first validates arguments and updates the workflow and metric
#' settings within the AnalysisObject. If hyperparameter tuning is enabled,
#' it executes the selected tuning procedure, identifies the best
#' hyperparameter configuration based on the specified metrics, and updates
#' the workflow accordingly. For neural network models, it also manages the
#' creation and integration of new model instances and provides additional
#' visualization of training dynamics. Finally, the function fits the optimized
#' model to the training data and updates the AnalysisObject, ensuring a
#' reproducible and efficient model selection process (Bartz et al., 2023).
#'
#' @param analysis_object analysis_object created from build_model function.
#' @param tuner Name of the Hyperparameter Tuner. A string of the tuner name:
#'             "Bayesian Optimization" or "Grid Search CV".
#' @param metrics Metric used for Model Selection. A string of the name of
#'             metric (see Metrics). By default either "rmse" (regression)
#'             or "roc_auc" (classification).
#' @section Tuners:
#'
#' ## Bayesian Optimization (with cross-validation)
#'
#' * Number of Folds: 5
#' * Initial data points: 20
#' * Maximum number of iterations: 25
#' * Convergence after 5 iterations without improvement
#' * Train / Test : 0.75 / 0.25
#'
#' ## Grid Search CV
#'
#' * Number of Folds: 5
#' * Maximum levels per hyperparameter: 10
#' * Train / Test : 0.75 / 0.25
#'
#' @section Metrics:
#'
#' ## Regression Metrics
#'
#' * rmse
#' * mae
#' * mpe
#' * mape
#' * ccc
#' * smape
#' * rpiq
#' * rsq
#'
#' ## Classification Metrics
#'
#' * accuracy
#' * bal_accuracy
#' * recall
#' * sensitivity
#' * specificity
#' * kap
#' * f_meas
#' * mcc
#' * j_index
#' * detection_prevalence
#' * roc_auc
#' * pr_auc
#' * gain_capture
#' * brier_class
#' * roc_aunp
#'
#' @returns An updated analysis_object containing the fitted model with
#' optimized hyperparameters, the tuning results, and all relevant workflow
#' modifications. This object includes the final trained model, the best
#' hyperparameter configuration, tuning diagnostics, and, if applicable, plots
#' of the tuning process. It can be used for further model evaluation,
#' prediction, or downstream analysis within the package workflow.
#' @examples
#' # Fine tuning function applied to a regression task using Random Forest
#'
#' wrap_object <- preprocessing(
#'            df = sim_data[1:500 ,],
#'            formula = psych_well ~ depression + life_sat,
#'            task = "regression"
#'            )
#' wrap_object <- build_model(
#'                analysis_object = wrap_object,
#'                model_name = "Random Forest",
#'                hyperparameters = list(
#'                      mtry = 2,
#'                      trees = 3
#'                      )
#'                  )
#' set.seed(123) # For reproducibility
#' wrap_object <- fine_tuning(wrap_object,
#'                 tuner = "Grid Search CV",
#'                 metrics = c("rmse")
#'                 )
#' @references
#' Bartz, E., Bartz-Beielstein, T., Zaefferer, M., & Mersmann, O. (2023).
#' *Hyperparameter tuner for Machine and Deep Learning with R. A
#' Practical Guide*. Springer.
#' \doi{10.1007/978-981-19-5170-1}
#' @export
fine_tuning <- function(analysis_object, tuner, metrics = NULL){

  check_args_fine_tuning(analysis_object = analysis_object, tuner = tuner, metrics = metrics)

  analysis_object = analysis_object$clone()

  if (is.null(metrics)){

    if (analysis_object$task == "regression"){metrics = "rmse"}
    else {metrics = "roc_auc"}

  }

  analysis_object$modify("workflow", create_workflow(analysis_object))

  if (!all(metrics %in% names(metrics_info))) {
    invalid_metrics <- metrics[!(metrics %in% names(metrics_info))]
    stop(paste0(
      "Unrecognized metric(s):\n ", paste(invalid_metrics, collapse = ", "),
      ". \n\nChoose from:\n ", paste(names(metrics_info), collapse = ", ")
    ))
  }

  analysis_object$modify("metrics", metrics)

  analysis_object$modify("tuner", tuner)

  set_metrics <- create_metric_set(analysis_object$metrics)

  sampling_method <- rsample::vfold_cv(analysis_object$data$raw$train_data, v = 5)

  final_data <- analysis_object$data$raw$train_data

  if (analysis_object$hyperparameters$tuning == TRUE){

    if (base::interactive()){cli::cli_alert_info("Commencing Tuning...")}

    tuner_fit = tune_models(analysis_object,
                            tuner,
                            sampling_method,
                            metrics = set_metrics)

    if (base::interactive()){cli::cli_alert_success("Tuning Finalized!")}

    analysis_object$modify("tuner_fit", tuner_fit)

    analysis_object <- tuning_results(analysis_object)

    # FINAL TRAINING
    # ============================================================================

    best_hyper <- tune::select_best(tuner_fit, metric = analysis_object$metrics[1])

    final_hyperparams <- c(as.list(best_hyper),
                           analysis_object$hyperparameters$hyperparams_constant
    )

  } else{

    ### NO TUNING

    final_hyperparams <- analysis_object$hyperparameters$hyperparams_constant

  }

  if ((analysis_object$model_name == "Neural Network") && (analysis_object$model$engine == "brulee")){

    new_hyperparams_nn = HyperparamsNN$new(final_hyperparams[!names(final_hyperparams) %in% ".config"])

    new_mlp_model = create_nn(hyperparams = new_hyperparams_nn, task = analysis_object$task, epochs = 500)

    new_workflow <- analysis_object$workflow %>%
      workflows::update_model(new_mlp_model)

    analysis_object$modify("workflow", new_workflow)

  }

  final_model <-  tune::finalize_workflow(analysis_object$workflow ,final_hyperparams)

  final_model <- parsnip::fit(final_model, final_data)

  analysis_object$modify("final_model", final_model)

  if ((analysis_object$model_name == "Neural Network") && (analysis_object$model$engine == "brulee")){

    model_parsnip <- tune::extract_fit_parsnip(analysis_object$final_model)

    p <- brulee::autoplot(model_parsnip) +
      ggplot2::labs(title = "Neural Network Loss Curve")

    plot_ob = analysis_object$plots

    plot_ob$nn_loss_curve = p

    analysis_object$modify("plots", plot_ob)

  }

  analysis_object <- evaluate_model(analysis_object)

  analysis_object$modify("stage", "fit_model")

  return(analysis_object)

}

tune_models <- function(analysis_object, tuner, sampling_method, metrics){

  if (tuner == "Bayesian Optimization"){

    tuner_object <- tune_models_bayesian(analysis_object, sampling_method, metrics = metrics)

  } else if (tuner == "Grid Search CV"){

    tuner_object <- tune_models_grid_search_cv(analysis_object, sampling_method, metrics = metrics)

  }

  else {

    ##### ERRRORRRR

    stop("Unrecognized Tuner. Select from: 'Bayesian Optimization', 'Grid Search CV'.")

  }

  return(tuner_object)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/graph_nn.R"
graph_nn <- function(model){

  # ----- Define Hyperparameters -----
  input_units <- model$fit$dims$p
  hidden_units <- model$fit$dims$h
  output_units <- model$fit$dims$y
  learn_rate <- signif(model$fit$parameters$learn_rate,2)
  activation <- model$fit$parameters$activation
  compact_threshold <- 8

  # ----- Start Building Graph Code -----
  graph_code <- "digraph MLP {\n  rankdir=LR;\n  splines=false;\n   ranksep=2.0;\n"

  # ----- Generate Neurons -----
  generate_neurons <- function(prefix, total_units, compact_threshold, color, apply_compact = FALSE) {
    code <- ""

    if (!apply_compact || total_units <= compact_threshold) {
      # show all neurons
      for (i in 1:total_units) {
        code <- paste0(code, "  ", prefix, i,
                       " [shape=circle, style=filled, fillcolor=", color, ", label=''];\n")
      }
    } else {
      # Input neurons
      code <- paste0(code, "  ", prefix, 1,
                     " [shape=circle, style=filled, fillcolor=", color, ", label=''];\n")
      # Add hidden layers with dots
      for (d in 1:4) {
        code <- paste0(code, "  ", prefix, "dots", d,
                       " [shape=circle, label='...', fontcolor=black, style=filled, fillcolor=white];\n")
      }
      # Last neuron
      code <- paste0(code, "  ", prefix, total_units,
                     " [shape=circle, style=filled, fillcolor=", color, ", label=''];\n")
    }
    return(code)
  }

  # Input Layer
  graph_code <- paste0(graph_code, generate_neurons("I", input_units, compact_threshold, "lightblue", apply_compact = FALSE))
  # Hidden Layer
  graph_code <- paste0(graph_code, generate_neurons("H", hidden_units, compact_threshold, "lightgreen", apply_compact = TRUE))
  # Output Layer
  graph_code <- paste0(graph_code, generate_neurons("O", output_units, compact_threshold, "lightpink", apply_compact = FALSE))

  # ----- Add Edges -----
  add_edges <- function(from_prefix, from_units, to_prefix, to_units, compact_threshold, apply_compact_from = FALSE, apply_compact_to = FALSE) {
    code <- ""

    from_nodes <- if (apply_compact_from && from_units > compact_threshold)
      c(1, paste0("dots", 1:4), from_units)
    else
      1:from_units

    to_nodes <- if (apply_compact_to && to_units > compact_threshold)
      c(1, paste0("dots", 1:4), to_units)
    else
      1:to_units

    for (i in from_nodes) {
      for (j in to_nodes) {
        code <- paste0(code, "  ", from_prefix, i, " -> ", to_prefix, j, ";\n")
      }
    }
    return(code)
  }

  # Input -> Hidden (compact only for hidden)
  graph_code <- paste0(graph_code, add_edges("I", input_units, "H", hidden_units, compact_threshold, FALSE, TRUE))
  # Hidden -> Output (compact only for hidden)
  graph_code <- paste0(graph_code, add_edges("H", hidden_units, "O", output_units, compact_threshold, TRUE, FALSE))

  # ----- Group Neurons Horizontally -----
  group_neurons <- function(prefix, units, compact_threshold, apply_compact = FALSE) {
    nodes <- if (apply_compact && units > compact_threshold)
      c(paste0(prefix, 1), paste0(prefix, "dots", 1:4), paste0(prefix, units))
    else
      paste0(prefix, 1:units)

    paste0("  {rank=same; ", paste(nodes, collapse = "; "), ";}\n")
  }

  graph_code <- paste0(graph_code, group_neurons("I", input_units, compact_threshold, apply_compact = FALSE))
  graph_code <- paste0(graph_code, group_neurons("H", hidden_units, compact_threshold, apply_compact = TRUE))
  graph_code <- paste0(graph_code, group_neurons("O", output_units, compact_threshold, apply_compact = FALSE))

  # ----- Add Hyperparameter Box -----
  hyperparams <- paste0(
    "Hyperparameters\\n",
    "Input Units: ", input_units, "\\n",
    "Hidden Units: ", hidden_units, "\\n",
    "Output Units: ", output_units, "\\n",
    "Learning Rate: ", signif(learn_rate, 3), "\\n",
    "Activation: ", activation, "\\n"
  )
#
#   graph_code <- paste0(
#     graph_code,
#     "  Hyperparams [shape=note, style=filled, fillcolor=lightyellow, fontsize=30, ",
#     "label=\"", hyperparams, "\"];\n",
#     "  Hyperparams -> I1 [style=invis, arrowhead=none];\n",
#     "  {rank=same; Hyperparams; I1;}\n"
#   )

  graph_code <- paste0(
    graph_code,
    # Nodo invisible ancla a la izquierda
    "  dummy [style=invis, width=0];\n",
    # Nodo de hiperparámetros
    "  Hyperparams [shape=note, style=filled, fillcolor=lightyellow, fontsize=30, ",
    "label=\"", hyperparams, "\"];\n",
    # Poner ambos en el nivel más a la izquierda
    "  {rank=min; dummy; Hyperparams;}\n",
    # Forzar conexión invisible (alineación izquierda → derecha)
    "  dummy -> Hyperparams [style=invis];\n"
  )


  # ----- Close Graph -----
  graph_code <- paste0(graph_code, "}")

  # ----- Render Diagram -----
  graph <- DiagrammeR::grViz(graph_code)

  return(graph)

}



#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/metrics.R"
metrics_info <- list(

  #################
  #   Regression
  #################

  rmse = c("numeric", "minimize"),
  mae = c("numeric", "minimize"),
  mpe = c("numeric", "minimize"),
  mape = c("numeric", "minimize"),
  ccc = c("numeric", "maximize"),
  smape = c("numeric", "minimize"),
  rpiq = c("numeric", "maximize"),
  rsq = c("numeric", "maximize"),

  #################
  #  Classification
  #################

  accuracy = c("class", "maximize"),
  precision = c("class", "maximize"),
  recall = c("class", "maximize"),
  bal_accuracy = c("class", "maximize"),
  specificity = c("class", "maximize"),
  sensitivity = c("class", "maximize"),
  kap = c("class", "maximize"),
  f_meas = c("class", "maximize"),
  mcc = c("class", "maximize"),
  j_index = c("class", "maximize"),
  detection_prevalence = c("class", "maximize"),

  roc_auc = c("prob", "maximize"),
  pr_auc = c("prob", "maximize"),
  gain_capture = c("prob", "maximize"),
  brier_class = c("prob", "minimize"),
  roc_aunp = c("prob", "maximize")
)

# Función para crear las funciones de métricas
create_metric_function <- function(metric_name, metric_info) {

  func_name <- base::sub("_mul$", "", metric_name)
  metric_type = metric_info[1]
  metric_direction = metric_info[2]


  #Crear la expresión de la función yardstick personalizada

  if (metric_type == "prob"){

   expr_text <- glue::glue("

     {metric_name} <- function(data, truth, estimator = NULL, na_rm = TRUE,...) {{


     yardstick::{metric_type}_metric_summarizer(
         \"{metric_name}\",
         yardstick::{func_name}_vec,
         data = data,
         truth = !!dplyr::enquo(truth),
         estimator = estimator,
         na_rm = na_rm,
         ...
       )

     }}
    ")

    } else {

    expr_text <- glue::glue("

    {metric_name} <- function(data, truth, estimate, na_rm = TRUE,...) {{
       yardstick::{metric_type}_metric_summarizer(
         \"{metric_name}\",
         yardstick::{func_name}_vec,
         data = data,
         truth = !!dplyr::enquo(truth),
         estimate = !!dplyr::enquo(estimate),
         na_rm = na_rm,
         ...
       )

     }}
   ")
    }

  pkg_env <- base::getNamespace("MLwrap")

  # Evaluar la expresión para crear la función en el entorno pkg_env
  parsed_expr <- rlang::parse_expr(expr_text)
  eval_func <- rlang::eval_tidy(parsed_expr, env = pkg_env)
  base::assign(metric_name, eval_func, envir = pkg_env)

}

# Función para convertir la función en una métrica de yardstick utilizando las funciones almacenadas en metric_funcs
convert_to_metric <- function(metrics_info) {

    pkg_env <- getNamespace("MLwrap")

    lapply(names(metrics_info), function(metric) {
    metric_name <- metric
    metric_info <- metrics_info[[metric]]
    metric_type <- metric_info[1]
    metric_direction <- metric_info[2]

    metric_func <- base::get(metric_name, envir = pkg_env)

    # Convertir la función en una métrica de yardstick usando la función almacenada
    new_metric <- base::switch(metric_type,
                         "prob" = yardstick::new_prob_metric(metric_func, metric_direction),
                         "class" = yardstick::new_class_metric(metric_func, metric_direction),
                         "numeric" = yardstick::new_numeric_metric(metric_func, metric_direction))

    # Asignar la nueva métrica al entorno

    base::assign(metric_name, new_metric, envir = pkg_env)

  })
}

# Generar todas las métricas
generate_all_metrics <- function(metrics_info) {



  # Primero crear las funciones usando eval y almacenarlas en el entorno
  base::lapply(names(metrics_info), function(m) create_metric_function(m, metrics_info[[m]]))

  # Luego convertirlas en métricas pasando metrics_info a convert_to_metric
  convert_to_metric(metrics_info)

}

generate_all_metrics(metrics_info)


#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/model_creation.R"

#' Create ML Model
#'
#' @description
#'
#' The function **build_model()** is designed to construct and attach a ML model
#' to an existing analysis object,which contains the preprocessed dataset
#' generated in the previous step using the preprocessing() function. Based on
#' the specified model type and optional hyperparameters, it supports several
#' popular algorithms—including **Neural Network**, **Random Forest**,
#' **XGBOOST**, and **SVM** (James et al., 2021)— by initializing the
#' corresponding hyperparameter class, updating the analysis object with these
#' settings, and invoking the appropriate model creation function. For SVM
#' models, it further distinguishes between kernel types (rbf, polynomial,
#' linear) to ensure the correct implementation. The function also updates the
#' analysis object with the model name, the fitted model, and the current
#' processing stage before returning the enriched object, thereby streamlining
#' the workflow for subsequent training, evaluation, or prediction steps. This
#' modular approach facilitates flexible and reproducible ML pipelines by
#' encapsulating both the model and its configuration within a single
#' structured object.
#'
#' @param analysis_object analysis_object created from preprocessing function.
#'
#' @param model_name Name of the ML Model. A string of the model name:
#' "Neural Network", "Random Forest", "SVM" or "XGBOOST".
#' @param hyperparameters Hyperparameters of the ML model. List containing
#' the name of the hyperparameter and its value or range of values.
#'
#' @section Hyperparameters:
#'
#' ## Neural Network
#'
#' Parsnip model using **brulee** engine. Hyperparameters:
#'
#' * **hidden_units**: Number of Hidden Neurons.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(5, 20).
#'
#' * **activation**: Activation Function.
#' A vector with any of ("relu", "sigmoid", "tanh") or NULL for default values c("relu", "sigmoid", "tanh").
#'
#' * **learn_rate**: Learning Rate.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-3, -1) in log10 scale.
#'
#' ## Random Forest
#'
#' Parsnip model using **ranger** engine. Hyperparameters:
#'
#' * **trees**: Number of Trees.
#' A single value, a vector with range values `c(min_val, max_val)`. Default range c(100, 300).
#'
#' * **mtry**: Number of variables randomly selected as candidates at each split.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(3, 8).
#'
#' * **min_n**: Minimum Number of samples to split at each node.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(5, 25).
#'
#' ## XGBOOST
#'
#' Parsnip model using **xgboost** engine. Hyperparameters:
#'
#' * **trees**: Number of Trees.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(100, 300).
#'
#' * **mtry**: Number of variables randomly selected as candidates at each split.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(3, 8).
#'
#' * **min_n**: Minimum Number of samples to split at each node.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(5, 25).
#'
#' * **tree_depth**: Maximum tree depth.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(3, 8).
#'
#' * **learn_rate**: Learning Rate.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-3, -1) in log10 scale.
#'
#' * **loss_reduction**: Minimum loss reduction required to make a further partition on a leaf node.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-3, 1.5) in log10 scale.
#'
#' ## SVM
#'
#' Parsnip model using **kernlab** engine. Hyperparameters:
#'
#' * **cost**: Penalty parameter that regulates model complexity and misclassification tolerance.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-3, 3) in log2 scale.
#'
#' * **margin**: Distance between the separating hyperplane and the nearest data points.
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(0, 0.2).
#'
#' * **type**: Kernel to be used.
#' A single value from ("linear", "rbf", "polynomial"). Default: "linear".
#'
#' * **rbf_sigma**:
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-5, 0) in log10 scale.
#'
#' * **degree**: Polynomial Degree (polynomial kernel only).
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(1, 3).
#'
#' * **scale_factor**: Scaling coefficient applied to inputs. (polynomial kernel only)
#' A single value, a vector with range values `c(min_val, max_val)` or NULL for default range c(-5, -1) in log10 scale.
#'
#' @returns An updated analysis_object containing the fitted machine learning
#' model, the model name, the specified hyperparameters, and the current
#' processing stage. This enriched object retains all previously stored
#' information from the preprocessing step and incorporates the results of the
#' model-building process, ensuring a coherent and reproducible workflow for
#' subsequent training, evaluation, or prediction tasks.
#' @examples
#' # Example 1: Random Forest for regression task
#'
#' library(MLwrap)
#'
#' data(sim_data) # sim_data is a simulated dataset with psychological variables
#'
#' wrap_object <- preprocessing(
#'      df = sim_data,
#'      formula = psych_well ~ depression + emot_intel + resilience + life_sat,
#'      task = "regression"
#'      )
#'
#' wrap_object <- build_model(
#'                analysis_object = wrap_object,
#'                model_name = "Random Forest",
#'                hyperparameters = list(
#'                                  mtry = 2,
#'                                  trees = 10
#'                                  )
#'                            )
#' # It is safe to reuse the same object name (e.g., wrap_object, or whatever)
#' # step by step, as all previous results and information are retained within
#' # the updated analysis object.
#'
#' # Example 2: SVM for classification task
#'
#' data(sim_data) # sim_data is a simulated dataset with psychological variables
#'
#' wrap_object <- preprocessing(
#'          df = sim_data,
#'          formula = psych_well_bin ~ depression + emot_intel + resilience + life_sat,
#'          task = "classification"
#'          )
#'
#' wrap_object <- build_model(
#'                analysis_object = wrap_object,
#'                model_name = "SVM",
#'                hyperparameters = list(
#'                                  type = "rbf",
#'                                  cost = 1,
#'                                  margin = 0.1,
#'                                  rbf_sigma = 0.05
#'                                  )
#'                            )
#' @references
#' James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction
#' to Statistical Learning: with Applications in R (2nd ed.)*. Springer.
#' https://doi.org/10.1007/978-1-0716-1418-1
#' @export

build_model <- function(analysis_object, model_name, hyperparameters = NULL){

  check_args_build_model(analysis_object = analysis_object, model_name = model_name,
                         hyperparameters = hyperparameters)

  analysis_object = analysis_object$clone()

  task = analysis_object$task

  if (model_name == "Neural Network"){

    if (!requireNamespace("torch", quietly = TRUE)) {
      message("The 'torch' package is not installed. Please run:\n  install.packages('torch')\n  torch::install_torch()")
    }

    if (!requireNamespace("brulee", quietly = TRUE)) {
      message("The 'brulee' package is not installed. Please run:\n install.packages('brulee')\n")

    }

    hyperparams_nn = HyperparamsNN$new(hyperparameters)

    analysis_object$modify("hyperparameters", hyperparams_nn)

    model = create_nn(hyperparams = hyperparams_nn, task = task, epochs = 25)

  } else if (model_name == "XGBOOST"){

    if (!base::requireNamespace("xgboost", quietly = TRUE)) {
      message("The 'xgboost' package is not installed. Please run:\n install.packages('xgboost')\n")
    }

    hyperparams_xgboost = HyperparamsXGBoost$new(hyperparameters)

    hyperparams_xgboost <- check_mtry(analysis_object, hyperparams_xgboost)

    analysis_object$modify("hyperparameters", hyperparams_xgboost)

    model = create_xgboost(hyperparams = hyperparams_xgboost, task = task)

  } else if (model_name == "Random Forest"){

    if (!requireNamespace("ranger", quietly = TRUE)) {
      message("The 'ranger' package is not installed. Please run:\n install.packages('ranger')\n")

    }

    hyperparams_rf = HyperparamsRF$new(hyperparameters)

    hyperparams_rf <- check_mtry(analysis_object, hyperparams_rf)

    analysis_object$modify("hyperparameters", hyperparams_rf)

    model = create_rf(hyperparams = hyperparams_rf, task = task)


  } else if (model_name == "SVM"){

    if (!requireNamespace("kernlab", quietly = TRUE)) {
      message("The 'kernlab' package is not installed. Please run:\n install.packages('kernlab')\n")

    }

    if (is.null(hyperparameters$type)){hyperparameters$type = "linear"}

    type = hyperparameters$type

    hyperparams_svm = HyperparamsSVM$new(hyperparameters)

    # margin only available for regression

    if (task == "classification"){

      hyperparams_svm$margin_tune <- FALSE

      hyperparams_svm$hyperparams_ranges$margin <- NULL

      hyperparams_svm$hyperparams_constant$margin <- NULL

      if (length(hyperparams_svm$hyperparams_ranges) == 0){

        hyperparams_svm$tuning <- FALSE

      }

    }

    analysis_object$modify("hyperparameters", hyperparams_svm)

    if (type == "rbf"){

      model = create_svm_rbf(hyperparams = hyperparams_svm, task = task)

    } else if (type == "poly"){

      model = create_svm_poly(hyperparams = hyperparams_svm, task = task)

    } else if (type == "linear"){

      model = create_svm_linear(hyperparams = hyperparams_svm, task = task)

    }

  } else {

    stop("
    Unrecognized Model. Select from: 'Neural Network', 'XGBOOST', 'Random Forest',
    'SVM'.
          ")

  }

  analysis_object$modify("model_name", model_name)

  analysis_object$modify("model", model)

  analysis_object$modify("stage", "build_model")

  return(analysis_object)
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/olden_method.R"
olden_calc <- function(model, task, outcome_levels, y_classes){

  if (task == "regression"){

    net_imp = olden_reg(model)

  }

  else{

    if (outcome_levels == 2){

      net_imp = olden_bin(model)

    } else {

      net_imp = olden_mul(model)

    }

  }

  return(net_imp)

}

olden_reg <- function(model){

  w_1 = coef(model$fit)[[1]]
  w_2 = coef(model$fit)[[3]]

  net_imp = t(w_1) %*% t(w_2)

  norm_factor = sum(abs(net_imp))

  net_imp = net_imp / norm_factor

  return(net_imp)

}

olden_bin <- function(model){

  w_1 = coef(model$fit)[[1]]
  w_2 = coef(model$fit)[[3]]

  imp = t(w_1) %*% t(w_2)

  net_imp = imp[,2] - imp[,1]

  norm_factor = sum(abs(net_imp))

  net_imp = net_imp / norm_factor

  return(net_imp)

}

olden_mul <- function(model){

  w_1 = coef(model$fit)[[1]]
  w_2 = coef(model$fit)[[3]]

  imp = t(w_1) %*% t(w_2)

  return(imp)

}

olden_barplot <- function(net_importance, names_predictor, title = "Olden Feature Importance"){

  df <- data.frame(
    variable = names_predictor,
    importance = as.numeric(net_importance)
  )

  # Order decreasing
  df$variable <- factor(df$variable, levels = df$variable[order(df$importance, decreasing = T)])

  # Plot
  p <- ggplot2::ggplot(df, ggplot2::aes(x = variable, y = importance, fill = importance > 0)) +
    ggplot2::geom_col(show.legend = FALSE) +
    ggplot2::geom_text(ggplot2::aes(label = round(importance, 3)),
             vjust = ifelse(df$importance >= 0, -0.5, 1.2)) +
    ggplot2::scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "firebrick")) +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed") +
    ggplot2::labs(
      title = title,
      x = "Feature",
      y = "Olden Feature Importance"
    ) +
    ggplot2::theme_grey() +
    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1)) +
    ggplot2::coord_cartesian(clip="off")

  return(p)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/partial_dependence_plot.R"
ice_data <- function(model, data, task, feature,
                     outcome_levels = NULL, group_by = NULL, grid_size = 25) {

  # 1) Build trimmed numeric grid (1%–99%)
  x <- data[[feature]]
  if (is.numeric(x)){

    q <- stats::quantile(x, probs = c(0.001, 0.999), na.rm = TRUE)
    grid <- seq(q[1], q[2], length.out = grid_size)

  } else{

    grid <- levels(as.factor(x))

  }

  # 2) Prepare grouping vector (with numeric -> quartiles)
  group_vec <- NULL
  if (!is.null(group_by)) {
    stopifnot(group_by %in% names(data))
    g <- data[[group_by]]
    if (is.numeric(g)) {
      r <- rank(g, na.last = "keep", ties.method = "average") / sum(!is.na(g))
      group_vec <- cut(r, breaks = c(0, .25, .50, .75, 1), include.lowest = TRUE,
                       labels = paste0("Q", 1:4))
    } else {
      group_vec <- g
    }
  }

  # 3) Loop over grid, predict, and build rows
  ice_list <- lapply(grid, function(val) {
    xtemp <- data
    xtemp[[feature]] <- val

    if (identical(task, "regression")) {
      preds <- predict(model, new_data = xtemp)[[1]]

      df <- data.frame(
        id = seq_len(nrow(data)),
        feature_value = val,
        prediction = preds
      )

    } else if (!is.null(outcome_levels) && outcome_levels == 2) {
      # Binary classification: take prob of 2nd level as in your setup
      preds <- predict(model, new_data = xtemp, type = "prob")[[2]]

      df <- data.frame(
        id = seq_len(nrow(data)),
        feature_value = val,
        prediction = preds
      )

    } else {
      # Multiclass: get prob tibble and pivot longer
      prob_tbl <- predict(model, new_data = xtemp, type = "prob")
      # ensure it's a data.frame
      prob_df <- as.data.frame(prob_tbl)

      # names typically like ".pred_classA", ".pred_classB", ...
      cls_names <- sub("^\\.pred_", "", colnames(prob_df))

      # build long data (base-R, no tidyr required)
      id_vec <- rep(seq_len(nrow(prob_df)), times = ncol(prob_df))
      class_vec <- rep(cls_names, each = nrow(prob_df))
      pred_vec <- as.numeric(as.matrix(prob_df))

      df <- data.frame(
        id = id_vec,
        feature_value = val,
        pred_class = class_vec,
        prediction = pred_vec
      )
    }

    # attach grouping if requested
    if (!is.null(group_by)) {
      df[[group_by]] <- rep(group_vec, each = if ("class" %in% names(df)) 1 else 1)
    }

    df
  })

  ice_df <- do.call(rbind, ice_list)
  rownames(ice_df) <- NULL
  ice_df
}

pred_fun <- function(model, new_data, task, outcome_levels){

  if (task == "regression"){

    return(predict(model, new_data= new_data, type = "numeric")[[1]])

  }

  if (task == "classification" && outcome_levels == 2){

    return(predict(model, new_data= new_data, type = "prob")[[2]])

  }

  if (task == "classification" && outcome_levels != 2){

    return(predict(model, new_data= new_data, type = "prob"))

  }

}




calc_pd_2D <- function(model, data, feat1, feat2,
                 grid.size = 20, task = "regression", outcome_levels = 0,
                 level = NULL) {

  x1 <- data[[feat1]]
  x2 <- data[[feat2]]

  # Determine if numeric or categorical
  is.num1 <- is.numeric(x1)
  is.num2 <- is.numeric(x2)

  # ---- 1. BUILD GRID ----
  grid1 <- if (is.num1) seq(min(x1), max(x1), length.out = grid.size) else levels(as.factor(x1))
  grid2 <- if (is.num2) seq(min(x2), max(x2), length.out = grid.size) else levels(as.factor(x2))

  # Cartesian grid
  grid <- expand.grid(val1 = grid1, val2 = grid2, KEEP.OUT.ATTRS = FALSE, stringsAsFactors = FALSE)
  colnames(grid) <- c(feat1, feat2)
  # ---- 2. COMPUTE 2D ICE (over the grid) ----
  # For each (val1, val2), replace feat1 & feat2 and predict
  preds <- numeric(nrow(grid))

  for (g in seq_len(nrow(grid))) {
    temp <- data
    temp[[feat1]] <- if (is.num1) grid[[feat1]][g] else factor(grid[[feat1]][g], levels = grid1)
    temp[[feat2]] <- if (is.num2) grid[[feat2]][g] else factor(grid[[feat2]][g], levels = grid2)



    p <- pred_fun(model, temp, task, outcome_levels)

    if (!is.null(level)){

      p <- p[[level]]

    }

    preds[g] <- mean(p)   # partial dependence = average prediction
  }

  grid$pd <- preds

  # ---- 3. RESHAPE TO MATRIX FORM FOR INTERPOLATION OR PLOTTING ----
  # For numeric × numeric → produce matrix grid.size × grid.size
  if (is.num1 && is.num2) {
    pd.mat <- matrix(grid$pd, nrow = length(grid1), ncol = length(grid2), byrow = FALSE)
    out <- list(
      pd.matrix = pd.mat,
      grid = grid,
      grid1 = grid1,
      grid2 = grid2
    )
    return(out)
  }

  # For numeric × categorical
  if (is.num1 && !is.num2) {
    pd.mat <- matrix(grid$pd, nrow = length(grid1), ncol = length(grid2), byrow = FALSE)
    colnames(pd.mat) <- grid2
    rownames(pd.mat) <- grid1

    out <- list(
      pd.matrix = pd.mat,
      grid = grid,
      grid1 = grid1,
      grid2 = grid2
    )
    return(out)
  }

  # For categorical × numeric
  if (!is.num1 && is.num2) {
    pd.mat <- matrix(grid$pd, nrow = length(grid1), ncol = length(grid2), byrow = FALSE)
    rownames(pd.mat) <- grid1
    colnames(pd.mat) <- grid2
    out <- list(
      pd.matrix = pd.mat,
      grid = grid,
      grid1 = grid1,
      grid2 = grid2
    )
    return(out)
  }

  # For factor × factor, no interpolation needed
  if (!is.num1 && !is.num2) {
    pd.mat <- matrix(grid$pd, nrow = length(grid1), ncol = length(grid2), byrow = TRUE,
                     dimnames = list(grid1, grid2))
    out <- list(
      pd.matrix = pd.mat,
      grid = grid,
      grid1 = grid1,
      grid2 = grid2
    )
    return(out)
  }
}

pd2D_interpolate <- function(grid_j, grid_k, pd_mat, xj, xk) {

  # find interval indices for each observation
  find_interval <- function(x, grid) {
    pmax(1, pmin(length(grid) - 1, findInterval(x, grid)))
  }

  j1 <- find_interval(xj, grid_j)
  j2 <- j1 + 1

  k1 <- find_interval(xk, grid_k)
  k2 <- k1 + 1

  # compute interpolation weights
  wj <- (xj - grid_j[j1]) / (grid_j[j2] - grid_j[j1])
  wk <- (xk - grid_k[k1]) / (grid_k[k2] - grid_k[k1])

  # extract corner values
  Q11 <- pd_mat[cbind(j1, k1)]
  Q21 <- pd_mat[cbind(j2, k1)]
  Q12 <- pd_mat[cbind(j1, k2)]
  Q22 <- pd_mat[cbind(j2, k2)]

  # bilinear interpolation
  out <-
    Q11 * (1 - wj) * (1 - wk) +
    Q21 * (wj)      * (1 - wk) +
    Q12 * (1 - wj) * (wk) +
    Q22 * (wj)      * (wk)

  return(out)
}

pd_1d_interpolate <- function(grid, xj, xk, cat_var = 1){

  if (cat_var == 1){

    filter_pd <- grid %>%
                  dplyr::filter(grid[[1]] == xj)

    num_var = 2

    x_out = xk

  } else{

    filter_pd <- grid %>%
      dplyr::filter(grid[[2]] == xk)

    num_var = 1

    x_out = xj

  }

  pd_jk_i <- stats::approx(x = filter_pd[[num_var]],
                        y = filter_pd$pd,
                        xout = x_out,
                        rule = 2)$y

  return(pd_jk_i)

}

pd_1d_interpolate_vec <- function(grid, xj, xk, cat_var = 1) {
  mapply(
    function(a, b) pd_1d_interpolate(grid, a, b, cat_var = cat_var),
    xj, xk
  )
}




#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/permutation_feature_importance.R"
pfi_plot <- function(tidy_object, new_data = "test", metric = NULL){

  if (tidy_object$task == "regression"){

    pfi_reg(tidy_object, new_data = new_data, metric = metric)

  } else if (tidy_object$task == "classification"){

    pfi_bin(tidy_object, new_data = new_data, metric = metric)

  }

}


###########################
#     Regression          #
###########################

pfi_calc <- function(model, data, y, task, metric, outcome_levels){

  if (task == "regression"){

    pfi_results <- pfi_reg(model, data, y, metric)

  } else {

    if (outcome_levels == 2){

      pfi_results <- pfi_bin(model, data, y, metric)

    }

    else{

      pfi_results <- pfi_multiclass(model, data, y, metric)

    }

  }

  return(pfi_results)

}

pfi_reg <- function(model, new_data, y, metric){

  # Native Permutation Feature Importance Implementation
  # No external dependencies - uses MLwrap infrastructure

  X <- new_data[, !(names(new_data) %in% y), drop = FALSE]
  y_true <- new_data[[y]]
  feature_names <- names(X)
  n_features <- ncol(X)
  nsim <- 25

  # Baseline predictions
  predictions_baseline <- pred_reg(model, X)

  # Get metric function from MLwrap namespace
  metric_func <- get(metric, envir = asNamespace("MLwrap"))

  # Calculate baseline error using standard evaluation
  df_baseline <- data.frame(truth = y_true, estimate = predictions_baseline)
  error_baseline <- metric_func(df_baseline, truth = "truth", estimate = "estimate")$.estimate

  # Initialize importance matrix
  importance_matrix <- matrix(NA, nrow = n_features, ncol = nsim)
  rownames(importance_matrix) <- feature_names

  # Permutation loop
  for (i in 1:n_features){
    feature_name <- feature_names[i]

    for (sim in 1:nsim){
      X_permuted <- X
      X_permuted[[feature_name]] <- sample(X[[feature_name]], size = nrow(X), replace = FALSE)

      predictions_permuted <- pred_reg(model, X_permuted)
      df_permuted <- data.frame(truth = y_true, estimate = predictions_permuted)
      error_permuted <- metric_func(df_permuted, truth = "truth", estimate = "estimate")$.estimate

      # Importance based on metric direction
      if (metrics_info[[metric]][2] == "minimize"){
        importance_matrix[i, sim] <- error_permuted - error_baseline
      } else {
        importance_matrix[i, sim] <- error_baseline - error_permuted
      }
    }
  }

  importance_mean <- rowMeans(importance_matrix)
  importance_sd <- apply(importance_matrix, 1, sd)

  vis <- data.frame(
    Variable = feature_names,
    Importance = importance_mean,
    StDev = importance_sd,
    stringsAsFactors = FALSE
  )

  vis <- vis[order(-vis$Importance), ]
  rownames(vis) <- NULL
  colnames(vis)[1] <- "Feature"

  return(vis)

}


#####################################
#     Binary Classification         #
#####################################

pfi_bin <- function(model, new_data, y, metric){

  if (metrics_info[[metric]][1] == "prob"){
    pred_func = pred_bin
  } else {
    pred_func = pred_bin_class
  }

  # Native Permutation Feature Importance Implementation
  # No external dependencies - uses MLwrap infrastructure

  X <- new_data[, !(names(new_data) %in% y), drop = FALSE]
  y_true <- new_data[[y]]
  feature_names <- names(X)
  n_features <- ncol(X)
  nsim <- 25

  # Baseline predictions
  predictions_baseline <- pred_func(model, X)

  # Get metric function from MLwrap namespace
  metric_func <- get(metric, envir = asNamespace("MLwrap"))

  # Calculate baseline error using standard evaluation
  if (metrics_info[[metric]][1] == "prob"){
    df_baseline <- data.frame(truth = y_true, .pred_1 = predictions_baseline, check.names = FALSE)
    error_baseline <- metric_func(df_baseline, truth = "truth", ".pred_1", event_level = "second")$.estimate
  } else {
    df_baseline <- data.frame(truth = y_true, estimate = predictions_baseline)
    error_baseline <- metric_func(df_baseline, truth = "truth", estimate = "estimate")$.estimate
  }

  # Initialize importance matrix
  importance_matrix <- matrix(NA, nrow = n_features, ncol = nsim)
  rownames(importance_matrix) <- feature_names

  # Permutation loop
  for (i in 1:n_features){
    feature_name <- feature_names[i]

    for (sim in 1:nsim){
      X_permuted <- X
      X_permuted[[feature_name]] <- sample(X[[feature_name]], size = nrow(X), replace = FALSE)

      predictions_permuted <- pred_func(model, X_permuted)

      if (metrics_info[[metric]][1] == "prob"){
        df_permuted <- data.frame(truth = y_true, .pred_1 = predictions_permuted, check.names = FALSE)
        error_permuted <- metric_func(df_permuted, truth = "truth", ".pred_1", event_level = "second")$.estimate
      } else {
        df_permuted <- data.frame(truth = y_true, estimate = predictions_permuted)
        error_permuted <- metric_func(df_permuted, truth = "truth", estimate = "estimate")$.estimate
      }

      # Importance based on metric direction
      if (metrics_info[[metric]][2] == "minimize"){
        importance_matrix[i, sim] <- error_permuted - error_baseline
      } else {
        importance_matrix[i, sim] <- error_baseline - error_permuted
      }
    }
  }

  importance_mean <- rowMeans(importance_matrix)
  importance_sd <- apply(importance_matrix, 1, sd)

  vis <- data.frame(
    Variable = feature_names,
    Importance = importance_mean,
    StDev = importance_sd,
    stringsAsFactors = FALSE
  )

  vis <- vis[order(-vis$Importance), ]
  rownames(vis) <- NULL
  colnames(vis)[1] <- "Feature"

  return(vis)

}

pfi_multiclass <- function(model, new_data, y, metric){

  y_classes = levels(new_data[[y]])
  new_test <- new_data[, !(names(new_data) %in% y)]
  results = list()

  for (target_class in y_classes){

    new_y <- factor(ifelse(new_data[[y]] == target_class, 1, 0), levels = c(0,1))

    if (metrics_info[[metric]][1] == "prob"){

      predicted = paste0(".pred_", target_class)

      pred_func <- function(object, newdata){

        return(predict(object, new_data = newdata, type = "prob")[[predicted]])

      }

    }

    else{

      pred_func <- function(object, newdata){

        pred = predict(object, new_data = newdata, type = "class")$.pred_class

        bin_pred = factor(ifelse(pred == target_class, 1, 0), levels = c(0,1))

        return(bin_pred)
      }
    }

    # Native Permutation Feature Importance Implementation
    # No external dependencies - uses MLwrap infrastructure

    X <- new_test
    y_true <- new_y
    feature_names <- names(X)
    n_features <- ncol(X)
    nsim <- 25

    # Baseline predictions
    predictions_baseline <- pred_func(model, X)

    # Get metric function from MLwrap namespace
    metric_func <- get(metric, envir = asNamespace("MLwrap"))

    # Calculate baseline error using standard evaluation
    if (metrics_info[[metric]][1] == "prob"){
      df_baseline <- data.frame(truth = y_true, .pred_1 = predictions_baseline, check.names = FALSE)
      error_baseline <- metric_func(df_baseline, truth = "truth", ".pred_1", event_level = "second")$.estimate
    } else {
      df_baseline <- data.frame(truth = y_true, estimate = predictions_baseline)
      error_baseline <- metric_func(df_baseline, truth = "truth", estimate = "estimate")$.estimate
    }

    # Initialize importance matrix
    importance_matrix <- matrix(NA, nrow = n_features, ncol = nsim)
    rownames(importance_matrix) <- feature_names

    # Permutation loop
    for (i in 1:n_features){
      feature_name <- feature_names[i]

      for (sim in 1:nsim){
        X_permuted <- X
        X_permuted[[feature_name]] <- sample(X[[feature_name]], size = nrow(X), replace = FALSE)

        predictions_permuted <- pred_func(model, X_permuted)

        if (metrics_info[[metric]][1] == "prob"){
          df_permuted <- data.frame(truth = y_true, .pred_1 = predictions_permuted, check.names = FALSE)
          error_permuted <- metric_func(df_permuted, truth = "truth", ".pred_1", event_level = "second")$.estimate
        } else {
          df_permuted <- data.frame(truth = y_true, estimate = predictions_permuted)
          error_permuted <- metric_func(df_permuted, truth = "truth", estimate = "estimate")$.estimate
        }

        # Importance based on metric direction
        if (metrics_info[[metric]][2] == "minimize"){
          importance_matrix[i, sim] <- error_permuted - error_baseline
        } else {
          importance_matrix[i, sim] <- error_baseline - error_permuted
        }
      }
    }

    importance_mean <- rowMeans(importance_matrix)
    importance_sd <- apply(importance_matrix, 1, sd)

    vis <- data.frame(
      Variable = feature_names,
      Importance = importance_mean,
      StDev = importance_sd,
      stringsAsFactors = FALSE
    )

    vis <- vis[order(-vis$Importance), ]
    rownames(vis) <- NULL
    colnames(vis)[1] <- "Feature"

    results[[target_class]] <- vis
  }

  return(results)

}
#########################################
#     Multiclass Classification         #
#########################################
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/plotting_utils.R"
### Tuner Plots ###

#' Plotting Tuner Search Results
#'
#' @description
#'
#' The **plot_tuning_results()** function Visualizes hyperparameter optimization
#' search results adapting output format to the optimization methodology
#' employed. For Bayesian Optimization: displays iteration-by-iteration loss
#' function evolution across iterations, acquisition function values guiding
#' sequential hyperparameter sampling, and final hyperparameter configuration
#' with cross-validation performance metrics. For Grid Search: displays
#' performance surfaces across hyperparameter dimensions and rank-ordered
#' configurations by validation performance.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the plot with tuning results the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # Final call signature:
#' # plot_tuning_results(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_tuning_results <- function(analysis_object){

  if ((analysis_object$stage != "fit_model") && (analysis_object$stage != "evaluated_model")){

    stop("You must first fit a model with 'fine_tuning()'!")

  }

  if (analysis_object$hyperparameters$tuning == FALSE){

    stop("All hyperparameters are fixed values, no tuning was performed!")

  }

  plots <- analysis_object$plots

  if (analysis_object$tuner == "Bayesian Optimization"){

    plot(plots$bayesian_opt_iter_loss)

    plot(plots$bayesian_opt_iter_results)

  }

  plot(plots$tuner_search_results)

  invisible(analysis_object)

}

#' Plot Neural Network Loss Curve
#'
#' @description
#'
#' Displays training loss trajectory computed on the validation set across
#' training epochs. Enables visual diagnosis of convergence dynamics,
#' identification of appropriate early stopping points, detection of overfitting
#' patterns (where validation loss increases while training loss decreases), and
#' assessment of optimization stability throughout the training process.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the loss curve plot the user needs to
#' # complete till the fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # (Neural Network engine required)
#' # Final call signature:
#' # plot_loss_curve(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_loss_curve <- function(analysis_object){

  if ((analysis_object$stage != "fit_model") && (analysis_object$stage != "evaluated_model")){

    stop("You must first fit a Neural Network model with 'fine_tuning()'!")

  }

  if (analysis_object$model_name != "Neural Network"){

    stop("Loss curve is only available for Neural Network models!")

  }

  plots <- analysis_object$plots

  p <- plots$nn_loss_curve

  plot(p)

}

#' Plot Neural Network Architecture
#'
#' @description
#'
#' Renders a directed acyclic graph representation of Neural Network
#' architecture showing layer stacking order, layer-specific dimensions
#' (neurons per layer), activation functions applied at each layer, and
#' optimized hyperparameter values (learning rate, batch size, dropout rates,
#' regularization coefficients) obtained from hyperparameter tuning procedures.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the Neural Network architecture graph plot the user
#' # needs to complete till the fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # (Neural Network engine required)
#' # Final call signature:
#' # plot_graph_nn(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_graph_nn <- function(analysis_object){

  if (analysis_object$model_name != "Neural Network"){

    stop("Model should be 'Neural Network'!")

  }

  if (analysis_object$stage != "fit_model"){

    stop("You first need to fit a model with 'fine_tuning()'!")

  }

  final_model <- analysis_object$final_model

  model_parsnip <- tune::extract_fit_parsnip(final_model)

  p <- graph_nn(model_parsnip)

  print(p)

  invisible(analysis_object)

}

### Regression Plots ###

#' Plotting Residuals Distribution
#'
#' @description
#'
#' The **plot_residuals_distribution()** function generates histogram and kernel
#' density visualizations of residuals for regression models on training and
#' test datasets. Enables assessment of residual normality through visual
#' inspection of histogram shape, detection of systematic biases indicating
#' omitted variables or model specification errors, and identification of heavy
#' tails suggesting outliers or influential observations.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the residuals distribution plot the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # Final call signature:
#' # plot_residuals_distribution(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_residuals_distribution <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_regression_plot(analysis_object)

  p_train <- plots$residuals_dist_train

  p_test <- plots$residuals_dist_test

  plot(patchwork::wrap_plots(p_train, p_test, nrow = 2))

  invisible(analysis_object)

  }

#' Plotting Residuals vs Predictions
#'
#' @description
#'
#' The **plot_scatter_residuals()** function Visualizes residuals plotted
#' against fitted values to detect violations of ordinary least squares
#' assumptions including homoscedasticity (constant error variance), linearity,
#' and independence. Identifies heteroscedastic patterns (non-constant variance
#' across the predictor range), systematic curvature indicating omitted
#' polynomial terms, and outlier points with extreme residual magnitudes.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the residuals vs. predicted values plot the user needs
#' # to complete till fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # Final call signature:
#' # plot_scatter_residuals(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_scatter_residuals <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_regression_plot(analysis_object)

  p_train <- plots$scatter_residuals_train

  p_test <- plots$scatter_residuals_test

  plot(patchwork::wrap_plots(p_train, p_test, nrow = 2))
  invisible(analysis_object)

}

#' Plotting Observed vs Predictions
#'
#' @description
#'
#' The **plot_scatter_predictions()** function generates scatter plots with
#' 45-degree reference lines comparing observed values (vertical axis) against
#' model predictions (horizontal axis) for training and test data. Enables
#' visual assessment of prediction accuracy through distance from the reference
#' line, identification of systematic bias patterns, detection of
#' heteroscedastic prediction errors, and quantification of generalization
#' performance gaps between training and test sets.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the observed vs. predicted values plot the user needs
#' # to complete till fine_tuning( ) function of the MLwrap pipeline.
#' # See the full pipeline example under table_best_hyperparameters()
#' # Final call signature:
#' # plot_scatter_predictions(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
plot_scatter_predictions <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_regression_plot(analysis_object)

  p_train <- plots$scatter_predictions_train

  p_test <- plots$scatter_predictions_test

  plot(patchwork::wrap_plots(p_train, p_test, ncol = 2))

  invisible(analysis_object)

}

### Classification Plots ###

#' Plotting Confusion Matrix
#'
#' @description
#'
#' The **plot_confusion_matrix()** function generates confusion matrices from
#' classification predictions displaying the contingency table of true class
#' labels versus predicted class labels. Visualizes true positives, true
#' negatives, false positives, and false negatives for both training and test
#' sets, enabling computation of derived performance metrics (sensitivity,
#' specificity, precision, F1-score) and identification of specific class pair
#' misclassification patterns.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining confusion matrix plot the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline and
#' # only with categorical outcome.
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_confusion_matrix(wrap_object)
#' @seealso \code{\link{plot_calibration_curve}}
#' @export
plot_confusion_matrix <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p_train <- plots$confusion_matrix_train

  p_test <- plots$confusion_matrix_test

  plot(patchwork::wrap_plots(p_train, p_test, nrow = 2))

  invisible(analysis_object)

}

#' Plotting ROC Curve
#'
#' @description
#'
#' The **plot_roc_curve()** function plots Receiver Operating Characteristic
#' (ROC) curve displaying true positive rate versus false positive rate across
#' all classification probability thresholds. Computes Area Under Curve (AUC)
#' as an aggregate discrimination performance metric independent of threshold
#' selection, providing comprehensive assessment of classifier discrimination
#' ability across the entire decision boundary range.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining roc curve plot the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline and
#' # only with categorical outcome.
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_roc_curve(wrap_object)
#' @seealso \code{\link{plot_calibration_curve}}
#' @export
plot_roc_curve <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p <- plots$roc_curve

  plot(p)

  invisible(analysis_object)

}

#' Plotting Precision-Recall Curve
#'
#' @description
#'
#' The **plot_pr_curve()** function generates Precision-Recall curve tracing
#' the relationship between precision and recall across all classification
#' probability thresholds. Particularly informative for imbalanced datasets
#' where ROC curves may be misleading, as PR curves remain sensitive to class
#' distribution changes and provide intuitive performance assessment when one
#' class is substantially rarer than the other.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @seealso \code{\link{plot_calibration_curve}}
#' @examples
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_pr_curve(wrap_object)
#' @export
plot_pr_curve <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p <- plots$pr_curve

  plot(p)

  invisible(analysis_object)

}

#' Plotting Gain Curve
#'
#' @description
#'
#' The **plot_gain_curve()** plots cumulative gain as a function of sorted
#' population percentile when observations are ranked by descending predicted
#' probability. For each percentile threshold, calculates the ratio of positive
#' class proportion in the top-ranked subset relative to overall positive class
#' proportion, quantifying model's efficiency in concentrating target cases at
#' the top of rankings.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the gain curve plot the user needs to complete till
#' # fine_tuning( ) function of the MLwrap pipeline and only with categorical
#' # outcome.
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_gain_curve(wrap_object)
#' @seealso \code{\link{plot_calibration_curve}}
#' @export
plot_gain_curve <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p <- plots$gain_curve

  plot(p)

  invisible(analysis_object)

}

#' Plotting Lift Curve
#'
#' @description
#'
#' The **plot_lift_curve()** function plots lift factor as a function of
#' population percentile when observations are ranked by descending predicted
#' probability. The lift factor quantifies model's ranking efficiency relative
#' to random ordering baseline at each population cumulative segment, showing
#' how much better model selection performs compared to random case selection.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the lift curve plot the user needs to complete till
#' # fine_tuning( ) function of the MLwrap pipeline and only with categorical
#' # outcome.
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_lift_curve(wrap_object)
#' @seealso \code{\link{plot_calibration_curve}}
#' @export
plot_lift_curve <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p <- plots$lift_curve

  plot(p)

  invisible(analysis_object)

}

#' Plotting Output Distribution By Class
#'
#' @description
#'
#' The **plot_distribution_by_class()** function visualizes kernel density
#' estimates or histograms of predicted probability distributions stratified by
#' true class labels. Enables assessment of class separability through
#' probability overlap quantification and identification of prediction
#' probability ranges where different classes exhibit substantial overlap,
#' indicating classification ambiguity regions.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the distribution by class plot the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline
#' # and only with categorical outcome.
#' @seealso \code{\link{plot_calibration_curve}}
#' @examples
#' # See the full pipeline example under plot_calibration_curve()
#' # Final call signature:
#' # plot_distribution_by_class(wrap_object)
#' @export
plot_distribution_by_class <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  p_train <- plots$dist_by_class_train

  p_test <- plots$dist_by_class_test

  plot(patchwork::wrap_plots(p_train, p_test, nrow = 2))

  invisible(analysis_object)

}

#' Plotting Calibration Curve
#'
#' @name plot_calibration_curve
#' @aliases plot_calibration_curve
#'
#' @description
#'
#' The **plot_calibration_curve()** function generates calibration plots for
#' binary classification models evaluating the agreement between predicted
#' probabilities and observed class frequencies in binned prediction intervals.
#' Implements reliability diagrams comparing empirical success rates within
#' each probability bin against the predicted probability levels, identifying
#' systematic calibration errors including overconfidence (predicted
#' probabilities exceed observed frequencies) and underconfidence patterns
#' across prediction ranges.
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the calibration curve plot the user needs to
#' # complete till fine_tuning( ) function of the MLwrap pipeline and
#' # only with binary outcome.
#'
#' wrap_object <- preprocessing(df = sim_data[1:300 ,],
#'                              formula = psych_well_bin ~ depression + resilience,
#'                              task = "classification")
#' wrap_object <- build_model(wrap_object, "Random Forest",
#'                            hyperparameters = list(mtry = 2, trees = 5))
#' set.seed(123) # For reproducibility
#' wrap_object <- fine_tuning(wrap_object, "Grid Search CV")
#'
#' # And then, you can obtain the calibration curve plot.
#'
#' plot_calibration_curve(wrap_object)
#' @export
plot_calibration_curve <- function(analysis_object){

  plots <- analysis_object$plots

  check_args_classification_plot(analysis_object)

  if (analysis_object$outcome_levels > 2){

    stop("Calibration Curve is only implemented for binary classification!")

  }

  p_train <- plots$reliability_plot_train + ggplot2::ggtitle("Reliability Plot (train data)")

  p_test  <- plots$reliability_plot_test  + ggplot2::ggtitle("Reliability Plot (test data)")

  plot(patchwork::wrap_plots(p_train, p_test, nrow = 2))

  invisible(analysis_object)

}

#' Plotting Permutation Feature Importance Barplot
#'
#' @description
#'
#' The **plot_pfi()** function generates feature importance estimates via
#' Permutation Feature Importance measuring performance degradation when each
#' feature's values are randomly permuted while holding all other features
#' constant. Provides model-agnostic importance ranking independent of
#' feature-target correlation patterns, capturing both linear and non-linear
#' predictive contributions to model performance.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "PFI")'.
#' @param show_table Boolean. Whether to print PFI results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the PFI plot results the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using the PFI
#' # method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "PFI"))
#' # Final call signature:
#' # plot_pfi(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_pfi <- function(analysis_object, show_table = FALSE){

  plots <- analysis_object$plots

  pfi_names   <- grep("^PFI", names(plots), value = TRUE)

  if (length(pfi_names) == 0){

    stop("You need to calculate PFI values first with 'sensitivity_analysis()'!")

  }

  pfi_plot <- plots[["PFI_barplot"]]

  if (base::interactive()){

    if (show_table == TRUE){

      tables <- table_pfi_results(analysis_object, show_table = TRUE)

    }

    plot(pfi_plot)

  }

  invisible(analysis_object)

}

#' Plotting SHAP Plots
#'
#' @description
#'
#' The **plot_shap()** function implements comprehensive SHAP (SHapley Additive
#' exPlanations) value visualizations where SHAP values represent each
#' feature's marginal contribution to model output based on cooperative game
#' theory principles. Provides four visualization modalities: bar plots of mean
#' absolute SHAP values ranking features by average impact magnitude,
#' directional plots showing feature-value correlation with SHAP magnitude and
#' sign, box plots illustrating SHAP value distributions across instances, and
#' swarm plots combining individual prediction contributions with
#' distributional information.
#'
#' @param analysis_object Fitted analysis_object with 'sensitivity_analysis(methods = "SHAP")'.
#' @param show_table Boolean. Whether to print SHAP summarized results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the SHAP plots the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using the SHAP
#' # method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "SHAP"))
#' # Final call signature:
#' # plot_shap(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_shap <- function(analysis_object, show_table = FALSE){

  plots <- analysis_object$plots

  shap_names   <- grep("^SHAP", names(plots), value = TRUE)

  if (length(shap_names) == 0){

    stop("You need to calculate SHAP values first with 'sensitivity_analysis()'!")

  }

  mean_shap_plots <- plots[["SHAP_barplot"]]

  dir_shap_plots <- plots[["SHAP_directional"]]

  box_shap_plots <- plots[["SHAP_boxplot"]]

  swarm_shap_plots <- plots[["SHAP_swarmplot"]]

  if (base::interactive()){

    if (show_table == TRUE){

    tables <- table_shap_results(analysis_object, show_table = TRUE)

    }

    plot(mean_shap_plots)
    plot(dir_shap_plots)
    plot(box_shap_plots)
    plot(swarm_shap_plots)

    }

  invisible(analysis_object)

}

#' Plotting Integrated Gradients Plots
#'
#' @description
#'
#' The **plot_integrated_gradients()** function implements interpretability
#' visualizations of integrated gradient attributions measuring feature
#' importance through accumulated gradients along the interpolation path from
#' baseline (zero vector) to observed input. Provides four visualization
#' modalities: mean absolute attributions (bar plots), directional effects
#' showing positive and negative contribution patterns (directional plots),
#' distributional properties of attributions across instances (box plots),
#' and individual-level attribution contributions (swarm plots).
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Integrated Gradients")'.
#' @param show_table Boolean. Whether to print Integrated Gradients summarized
#' results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the Integrated Gradients plot the user needs to
#' # complete till sensitivity_analysis( ) function of the MLwrap pipeline
#' # using the Integrated Gradients method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "Integrated Gradients"))
#' # Final call signature:
#' # plot_integrated_gradients(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_integrated_gradients <- function(analysis_object, show_table = FALSE){

  plots <- analysis_object$plots

  ig_names   <- grep("^Integrated", names(plots), value = TRUE)

  if (length(ig_names) == 0){

    stop("You need to calculate Integrated Gradients values first with 'sensitivity_analysis()'!")

  }

  mean_ig_plots <- plots[["IntegratedGradients_barplot"]]

  dir_ig_plots <- plots[["IntegratedGradients_directional"]]

  box_ig_plots <- plots[["IntegratedGradients_boxplot"]]

  swarm_ig_plots <- plots[["IntegratedGradients_swarmplot"]]

  if (base::interactive()){

    if (show_table){

      tables <- table_integrated_gradients_results(analysis_object, show_table = TRUE)

    }

    plot(mean_ig_plots)
    plot(dir_ig_plots)
    plot(box_ig_plots)
    plot(swarm_ig_plots)

  }

  invisible(analysis_object)

}

#' Plotting Olden Values Barplot
#'
#' @description
#'
#' The **plot_olden()** function visualizes Olden sensitivity values computed
#' from products of input-to-hidden layer connection weights and
#' hidden-to-output layer connection weights for each feature. Provides relative
#' feature importance rankings specific to feedforward Neural Networks based on
#' synaptic weight magnitude and directionality analysis across network layers.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Olden")'.
#' @param show_table Boolean. Whether to print Olden results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the Olden plot the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using the Olden
#' # method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "Olden"))
#' # Final call signature:
#' # plot_olden(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_olden <- function(analysis_object, show_table = FALSE){

  plots <- analysis_object$plots

  olden_names   <- grep("^Olden", names(plots), value = TRUE)

  if (length(olden_names) == 0){

    stop("You need to calculate Olden values first with 'sensitivity_analysis()'!")

  }

  olden_plots <- plots[[olden_names]]

  #combined <- patchwork::wrap_plots(olden_plots)

  if (base::interactive()){

    if (show_table == TRUE){

      tables <- table_olden_results(analysis_object, show_table = TRUE)

    }

    plot(olden_plots)

  }

  invisible(analysis_object)

}

#' Plotting Sobol-Jansen Values Barplot
#'
#' @description
#'
#' The **plot_sobol_jansen()** function displays first-order and total-order
#' Sobol indices decomposing total output variance into contributions from
#' individual features and higher-order interaction terms. Implements
#' variance-based global sensitivity analysis providing comprehensive
#' understanding of feature contributions to output uncertainty, with
#' application restricted to continuous predictor variables.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Sobol_Jansen")'.
#' @param show_table Boolean. Whether to print Sobol-Jansen results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the Sobol_Jansen plot the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using
#' # the Sobol_Jansen method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "Sobol_Jansen"))
#' # Final call signature:
#' # plot_sobol_jansen(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_sobol_jansen <- function(analysis_object, show_table = FALSE){

  plots <- analysis_object$plots

  if (is.null(plots$Sobol_Jansen)){

    stop("You need to calculate Sobol_Jansen values first with 'sensitivity_analysis()'!")

  }

  if (base::interactive()){

    if (show_table == TRUE){

      tables <- table_sobol_jansen_results(analysis_object, show_table = TRUE)

    }

    plot(plots$Sobol_Jansen)

  }

  invisible(analysis_object)

}

#' Plotting Partial Dependence Plot
#'
#' @description
#'
#' The **plot_pfi()** function generates feature importance estimates via
#' Permutation Feature Importance measuring performance degradation when each
#' feature's values are randomly permuted while holding all other features
#' constant. Provides model-agnostic importance ranking independent of
#' feature-target correlation patterns, capturing both linear and non-linear
#' predictive contributions to model performance.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "PFI")'.
#' @param show_table Boolean. Whether to print PFI results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the PFI plot results the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using the PFI
#' # method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "PFI"))
#' # Final call signature:
#' # plot_pfi(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_pdp <- function(analysis_object, feature,
                                         group_by = NULL,
                                         grid_size = 25,
                                         show_ice = TRUE, ice_n = 50,
                                         pdp_line_size = 1.1,
                                         use_test = FALSE,
                                         plot = TRUE){

  model <- analysis_object$final_model
  if (use_test){

    data <- analysis_object$data$raw$test_data

  } else {

    data <- analysis_object$data$raw$train_data

  }
  task <- analysis_object$task
  outcome_levels <- analysis_object$outcome_levels

  # 1) Full ICE (no sampling) using your ice_data() with trimmed grid
  ice_full <- ice_data(
    model = model,
    data = data,
    task = task,
    outcome_levels = outcome_levels,
    feature   = feature,
    grid_size = grid_size,
    group_by  = group_by
  )

  # 2) PDP from full ICE
  if ("pred_class" %in% names(ice_full)) {
    # Multiclass: group by class
    if (is.null(group_by)) {
      pdp_df <- ice_full %>%
        dplyr::group_by(pred_class, feature_value) %>%
        dplyr::summarise(prediction = mean(prediction, na.rm = TRUE), .groups = "drop")
    } else {
      pdp_df <- ice_full %>%
        dplyr::group_by(pred_class, .data[[group_by]], feature_value) %>%
        dplyr::summarise(prediction = mean(prediction, na.rm = TRUE), .groups = "drop")
    }
  } else {
    # Regression or binary classification
    if (is.null(group_by)) {
      pdp_df <- ice_full %>%
        dplyr::group_by(feature_value) %>%
        dplyr::summarise(prediction = mean(prediction, na.rm = TRUE), .groups = "drop")
    } else {
      pdp_df <- ice_full %>%
        dplyr::group_by(.data[[group_by]], feature_value) %>%
        dplyr::summarise(prediction = mean(prediction, na.rm = TRUE), .groups = "drop")
    }
  }

  # 3) If plotting ICE, sample IDs from the full ICE (same dataframe)
  if (isTRUE(show_ice)) {
    sampled_ids <- sample(unique(ice_full$id), size = min(ice_n, length(unique(ice_full$id))))
    ice_plot <- ice_full[ice_full$id %in% sampled_ids, , drop = FALSE]
  }

  # Multiclass PDP + ICE facet version
  if ("pred_class" %in% names(ice_full)) {
    p <- ggplot2::ggplot()

    if (isTRUE(show_ice)) {
      p <- p +
        ggplot2::geom_line(data = ice_plot,
                           ggplot2::aes(x = feature_value, y = prediction,
                                        group = interaction(id, pred_class),
                                        color = if (!is.null(group_by)) .data[[group_by]] else "dodgerblue2"),
                           alpha = 0.3, linewidth = 0.4)
    }

    p <- p +
      ggplot2::geom_line(data = pdp_df,
                         ggplot2::aes(x = feature_value, y = prediction,
                                      color = if (!is.null(group_by)) .data[[group_by]] else "dodgerblue2"),
                         linewidth = pdp_line_size) +
      ggplot2::facet_wrap(~ pred_class) +
      ggplot2::scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
      ggplot2::labs(
        title = if (is.null(group_by)) glue::glue("PD Plot {feature}") else glue::glue("PD Plot {feature} by {group_by}"),
        x = feature, y = "Predicted probability",
        color = if (!is.null(group_by)) group_by else NULL
      ) +
      ggplot2::theme_gray()

    if (is.null(group_by)) {
      p <- p + ggplot2::guides(color = "none")
    }

  } else {
    # Regression or binary
    p <- ggplot2::ggplot()

    if (isTRUE(show_ice)) {
      p <- p +
        ggplot2::geom_line(data = ice_plot,
                           ggplot2::aes(x = feature_value, y = prediction, group = id,
                                        color = if (!is.null(group_by)) .data[[group_by]] else NULL),
                           alpha = 0.3, linewidth = 0.4
        )
    }

    p <- p +
      ggplot2::geom_line(data = pdp_df,
                         ggplot2::aes(x = feature_value, y = prediction,
                                      color = if (!is.null(group_by)) .data[[group_by]] else NULL),
                         linewidth = pdp_line_size) +
      ggplot2::scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
      ggplot2::labs(
        title = if (is.null(group_by)) glue::glue("PD Plot {feature}") else glue::glue("PD Plot {feature} by {group_by}"),
        x = feature, y = "Prediction",
        color = group_by
      ) +
      ggplot2::theme_gray()
  }

  x_data <- data[[feature]]

  p <- p + ggplot2::geom_rug(
    data = data.frame(x = x_data),
    ggplot2::aes(x = x),
    sides = "b",
    inherit.aes = FALSE,
    alpha = 1
  )

  if (plot){

    plot(p)

    invisible(analysis_object)

  } else{

    return(p)

  }
}

#' Plotting Accumulated Local Effects Plot
#'
#' @description
#'
#' The **plot_pfi()** function generates feature importance estimates via
#' Permutation Feature Importance measuring performance degradation when each
#' feature's values are randomly permuted while holding all other features
#' constant. Provides model-agnostic importance ranking independent of
#' feature-target correlation patterns, capturing both linear and non-linear
#' predictive contributions to model performance.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "PFI")'.
#' @param show_table Boolean. Whether to print PFI results table.
#' @returns analysis_object
#' @examples
#' # Note: For obtaining the PFI plot results the user needs to complete till
#' # sensitivity_analysis( ) function of the MLwrap pipeline using the PFI
#' # method.
#' # See the full pipeline example under sensitivity_analysis()
#' # (Requires sensitivity_analysis(methods = "PFI"))
#' # Final call signature:
#' # plot_pfi(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
plot_ale <- function(analysis_object,feature,
                          group = NULL, grid.size = 20,
                          use_test = FALSE,
                          plot = TRUE) {

  task            <- analysis_object$task
  outcome_levels  <- analysis_object$outcome_levels

  if (use_test){

    train <- analysis_object$data$raw$test_data

  } else {

    train <- analysis_object$data$raw$train_data

  }

  model           <- analysis_object$final_model

  ale_long <- comp_ale(model, train, feature, group = group, task = task,
                       outcome_levels = outcome_levels, K = grid.size)

  # Group

  if (is.null(group)) {

    p <- ggplot2::ggplot(ale_long,
                         ggplot2::aes(x = grid, y = ale)) +
      ggplot2::geom_line(linewidth = 1) +
      ggplot2::geom_point(size = 2)

  } else {

    p <- ggplot2::ggplot(ale_long,
                         ggplot2::aes(x = grid, y = ale, color = Level)) +
      ggplot2::geom_line(linewidth = 1) +
      ggplot2::geom_point(size = 2)
  }

  # Facet wrap

  if (outcome_levels > 2) {
    p <- p + ggplot2::facet_wrap(~ Class)
  }

  # Add rug

  x_data <- train[[feature]]
  p <- p + ggplot2::geom_rug(
    data = data.frame(x = x_data),
    ggplot2::aes(x = x),
    sides = "b",
    inherit.aes = FALSE,
    alpha = 1
  ) +
    ggplot2::scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
    ggplot2::labs(
      x = feature,
      y = "ALE",
      title = paste("ALE effect of", feature,
                    if (!is.null(group)) paste("grouped by", group))
    ) +
    ggplot2::theme_gray()

  if (plot){

    plot(p)

    invisible(analysis_object)

  } else{

    return(p)

  }
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/preprocessing.R"
subset_data <- function(formula, data){

  if (all.vars(formula)[2] != "."){
    data = data[all.vars(formula)]
  }

  return(data)
}

standarize_predictors <- function(rec, norm_num_vars){

  if (any(norm_num_vars == "all")){

    rec <- recipes::step_normalize(rec, recipes::all_numeric_predictors())

  } else{

    rec <- recipes::step_normalize(rec, all_of(norm_num_vars))

  }

  return(rec)

}

one_hot_predictors <- function(rec, encode_cat_vars, one_hot = T){

  if (any(encode_cat_vars == "all")){

    rec <- recipes::step_dummy(rec, recipes::all_factor_predictors(), one_hot = one_hot)

  } else{

    rec <- recipes::step_dummy(rec, all_of(encode_cat_vars), one_hot = one_hot)

  }

  return(rec)

}

#' Preprocessing Data Matrix
#'
#' @description
#'
#' The **preprocessing()** function streamlines data preparation for regression
#' and classification tasks by integrating variable selection, type conversion,
#' normalization, and categorical encoding into a single workflow. It takes a
#' data frame and a formula, applies user-specified transformations to numeric
#' and categorical variables using the recipes package, and ensures the outcome
#' variable is properly formatted. The function returns an AnalysisObject
#' containing both the processed data and the transformation pipeline,
#' supporting reproducible and efficient modeling (Kuhn & Wickham, 2020).
#'
#' @param df Input DataFrame. Either a data.frame or tibble.
#' @param formula Modelling Formula. A string of characters or formula.
#' @param task Modelling Task. Either "regression" or "classification".
#' @param num_vars Optional vector of names of the numerical features.
#' @param cat_vars Optional vector of names of the categorical features.
#' @param encode_cat_vars One Hot Encode Categorical Features. Either vector of
#'  names of categorical features to be encoded or "all" (default).
#' @param norm_num_vars Normalize numeric features as z-scores. Either vector
#'  of names of numerical features to be normalized or "all" (default).
#' @param y_levels Optional ordered vector with names of the target variable
#'  levels (Classification task only).
#' @returns The object returned by the preprocessing function encapsulates a
#' dataset specifically prepared for ML analysis. This object contains the
#' preprocessed data—where variables have been selected, standardized, encoded,
#' and formatted according to the requirements of the chosen modeling task
#' (regression or classification) —as well as a recipes::recipe object that
#' documents all preprocessing steps applied. By automating essential
#' transformations such as normalization, one-hot encoding of categorical
#' variables, and the handling of missing values, the function ensures the data
#' is optimally structured for input into machine learning algorithms. This
#' comprehensive preprocessing not only exposes the underlying structure of the
#' data and reduces the risk of errors, but also provides a robust foundation
#' for subsequent modeling, validation, and interpretation within the machine
#' learning workflow (Kuhn & Johnson, 2019).
#' @examples
#' # Example 1: Dataset with preformatted categorical variables
#' # In this case, internal options for variable types are not needed since
#' # categorical features are already formatted as factors.
#'
#' library(MLwrap)
#'
#' data(sim_data) # sim_data is a simulated dataset with psychological variables
#'
#' wrap_object <- preprocessing(
#'           df = sim_data,
#'           formula = psych_well ~ depression + emot_intel + resilience + life_sat + gender,
#'           task = "regression"
#'          )
#'
#' # Example 2: Dataset where neither the outcome nor the categorical features
#' # are formatted as factors and all categorical variables are specified to be
#' # formatted as factors
#'
#' wrap_object <- preprocessing(
#'            df = sim_data,
#'            formula = psych_well_bin ~ gender + depression + age + life_sat,
#'            task = "classification",
#'            cat_vars = c("gender")
#'          )
#' @references
#' Kuhn, M., & Johnson, K. (2019). *Feature Engineering and Selection: A
#' Practical Approach for Predictive Models*. Chapman and Hall/CRC.
#' \doi{10.1201/9781315108230}
#'
#' Kuhn, M., & Wickham, H. (2020). *Tidymodels: a collection of packages for
#' modeling and machine learning using tidyverse principles*.
#' \url{https://www.tidymodels.org}.
#' @export
preprocessing <- function(df, formula, task = "regression", num_vars = NULL, cat_vars = NULL,
                          norm_num_vars = "all", encode_cat_vars = "all", y_levels = NULL){

          formula = as.formula(formula)

          check_args_preprocessing(df = df, formula = formula, task = task, num_vars = num_vars,
                                   cat_vars = cat_vars, norm_num_vars = norm_num_vars,
                                   encode_cat_vars = encode_cat_vars)

          # Subset data from formula

          df <- subset_data(formula = formula, data = df)

          outcome_levels = 0

          if (task == "classification"){

            y = all.vars(formula)[1]

            if (!is.null(y_levels)){

              if (any(!(y_levels %in% levels(as.factor(df[[y]]))))){

                stop("y_levels do not correspond to original levels")

              }

              df[[y]] <- factor(df[[y]], levels = y_levels)

            } else{

                df[[y]] = as.factor(df[[y]])

            }

            outcome_levels = length(levels(df[[y]]))

          }

          # Create recipe

          rec = recipes::recipe(formula = formula, data = df)


          # Check numerical variables are numeric

          if (!is.null(num_vars)) {

              rec <- rec %>% recipes::step_mutate_at(all_of(num_vars),
                                            fn = as.numeric)
          }

          # Check categorical variables are factor

          if (!is.null(cat_vars)) {

              rec <- rec %>% recipes::step_mutate_at(all_of(cat_vars),
                                            fn = as.factor)
          }

          # Normalize selected numerical columns

          if (!is.null(norm_num_vars)) {

            rec <- standarize_predictors(rec = rec, norm_num_vars = norm_num_vars)

          }

          # Encode selected categorical columns

          if (!is.null(encode_cat_vars)){

              rec <- one_hot_predictors(rec = rec, encode_cat_vars = encode_cat_vars)

          }

          # Create AnalysisObject with data and recipe

          analysis_object <- AnalysisObject$new(full_data = df, transformer = rec, task = task,
                                          formula = formula, outcome_levels = outcome_levels)

          return(analysis_object)

}


#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/results_binary_classification.R"
    ########################################
    #             Predictions              #
    ########################################

get_predictions_binary <- function(analysis_object, new_data = "test"){

  model_workflow <- analysis_object$final_model

  y = analysis_object$dep_var

  if (new_data == "all"){

      data_sets = c("train", "test")

    temp = list()

    for (data_set in data_sets){

      dat = analysis_object$data$raw[[paste0(data_set, "_data")]]

      predictions_class = predict(model_workflow, new_data = dat)
      predictions_prob = predict(model_workflow, new_data = dat, type = "prob")
      predictions = cbind(predictions_class, predictions_prob, y = as.factor(dat[[y]]))
      predictions$data_set = data_set

      temp[[data_set]] = predictions
    }

    predictions = rbind(temp[["train"]], temp[["test"]])

  } else {

    dat = analysis_object$data$raw[[paste0(new_data, "_data")]]

    predictions_class = predict(model_workflow, new_data = dat)
    predictions_prob = predict(model_workflow, new_data = dat, type = "prob")
    predictions = cbind(predictions_class, predictions_prob, y = as.factor(dat[[y]]))
    predictions$data_set = new_data

  }

  return (predictions)

}

    ########################################
    #             SUMMARY                  #
    ########################################

summary_binary <- function(predictions, new_data = "test"){

  metric_funcs <- list(

    Accuracy = function(data) accuracy(data, y, .pred_class),
    Balanced_Accuracy = function(data) bal_accuracy(data, y, .pred_class),
    Precision = function(data) precision(data, y, .pred_class),
    Recall = function(data) recall(data, y, .pred_class),
    Specificity = function(data) specificity(data, y, .pred_class),
    Sensitivity = function(data) sensitivity(data, y, .pred_class),
    Kappa = function(data) kap(data, y, .pred_class),
    F1_score = function(data) f_meas(data, y, .pred_class),
    MCC = function(data) mcc(data, y, .pred_class),
    J_index = function(data) j_index(data, y, .pred_class),
    Detection_Prevalence = function(data) detection_prevalence(data, y, .pred_class, event_level = "second"),
    AUC_ROC = function(data) yardstick::roc_auc(data, y, predicted, event_level = "second"),
    AUC_PR = function(data) yardstick::pr_auc(data, y, predicted, event_level = "second"),
    Gain_Capture = function(data) yardstick::gain_capture(data, y, predicted, event_level = "second"),
    Brier_Score = function(data) yardstick::brier_class(data, y, predicted, event_level = "second")

  )

  positive_class = levels(predictions$y)[2]

  predicted = paste0(".pred_", positive_class)

  results <- lapply(metric_funcs, function(f) f(predictions)$.estimate)

  results <- as.data.frame(results)

  rownames(results) <- new_data

  results <- results %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ base::signif(.x, 3)))

  return(results)

}

    ########################################
    #             Plots                  #
    ########################################

plot_roc_curve_binary <- function(predictions, new_data = "all"){

  positive_class = levels(predictions$y)[2]

  predicted = paste0(".pred_", positive_class)

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::roc_curve(y, predicted, event_level = "second")

    return(curve_plot)

  }

}

plot_pr_curve_binary <- function(predictions, new_data = "all"){

  positive_class = levels(predictions$y)[2]

  predicted = paste0(".pred_", positive_class)

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::pr_curve(y, predicted, event_level = "second")

    return(curve_plot)

  }


}

plot_gain_curve_binary <- function(predictions, new_data = "all"){

  positive_class = levels(predictions$y)[2]

  predicted = paste0(".pred_", positive_class)

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::gain_curve(y, predicted, event_level = "second")

    return(curve_plot)

  }


}

plot_lift_curve_binary <- function(predictions, new_data = "all"){

  positive_class = levels(predictions$y)[2]

  predicted = paste0(".pred_", positive_class)

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::lift_curve(y, predicted, event_level = "second")

    return(curve_plot)

  }
}

plot_dist_probs_binary <- function(predictions, new_data = "test"){

  positive_class <- levels(predictions$y)[2]
  predicted       <- paste0(".pred_", positive_class)

  p <- predictions %>%
    dplyr::filter(data_set == new_data) %>%
    dplyr::group_by(y) %>%
    ggplot2::ggplot(
      ggplot2::aes(
        x    = !!rlang::sym(predicted),
        fill = y
      )
    ) +
    ggplot2::geom_density(alpha = 0.5) +
    ggplot2::labs(
      title = paste0("Probability Distribution by Class (", new_data, " data)"),
      x     = "Predicted Probability",
      y     = "Density",
      fill  = "Class"
    ) +
    ggplot2::theme_minimal()

  return(p)

}


plot_calibration_curve_binary <- function(predictions, new_data = "test"){

  positive_class = levels(predictions$y)[2]

  predicted = rlang::sym(paste0(".pred_", positive_class))

  p <- predictions %>%
    dplyr::filter(data_set == new_data) %>%
    dplyr::mutate(y = sapply(y, function(x) if(x == positive_class) 1 else 0)) %>%
    dplyr::mutate(pred_bin = base::cut(predictions[[predicted]],
                                 breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%  # Crear bins de probabilidad
    dplyr::group_by(pred_bin) %>%
    dplyr::summarise(
      prob_pred = mean({{predicted}}),  # Promedio de las probabilidades predichas en cada bin
      prob_observed = mean(y)  # Promedio de 1s observados (probabilidad observada)
    ) %>%
    ggplot2::ggplot(ggplot2::aes(x = prob_pred, y = prob_observed)) +
    ggplot2::geom_point() +  # Graficar los puntos
    ggplot2::geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +  # Línea de calibración ideal
    ggplot2::labs(title = "Reliability Plot", x = "Predicted Probability", y = "Observed Probability") +
    ggplot2::theme_minimal()

  return(p)

}


plot_conf_mat <- function(predictions, new_data = "test"){

  confusion_matrix = yardstick::conf_mat(predictions, truth = y, estimate = .pred_class)

  return(confusion_matrix)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/results_multiclass_classification.R"
########################################
#             Predictions              #
########################################

get_predictions_multiclass <- function(analysis_object, new_data = "test"){

  model_workflow <- analysis_object$final_model

  y = analysis_object$dep_var

  if (new_data == "all"){

      data_sets = c("train", "test")

    temp = list()

    for (data_set in data_sets){

      dat = analysis_object$data$raw[[paste0(data_set, "_data")]]

      predictions_class = predict(model_workflow, new_data = dat)
      predictions_prob = predict(model_workflow, new_data = dat, type = "prob")
      predictions = cbind(predictions_class, predictions_prob, y = as.factor(dat[[y]]))
      predictions$data_set = data_set

      temp[[data_set]] = predictions
    }

    predictions = rbind(temp[["train"]], temp[["test"]])

  } else {

    dat = analysis_object$data$raw[[paste0(new_data, "_data")]]

    predictions_class = predict(model_workflow, new_data = dat)
    predictions_prob = predict(model_workflow, new_data = dat, type = "prob")
    predictions = cbind(predictions_class, predictions_prob, y = as.factor(dat[[y]]))
    predictions$data_set = new_data

  }

  return (predictions)

}

########################################
#             SUMMARY                  #
########################################

binarize_class <- function(data, target_class) {
  data_bin <- data
  data_bin$truth <- factor(ifelse(data$y == target_class, 1, 0), levels = c(0,1))
  data_bin$prob_estimate <- data[[paste0(".pred_", target_class)]]
  data_bin$estimate <- factor(ifelse(data$.pred_class == target_class, 1, 0), levels = c(0,1))
  data_bin <- data_bin[, c("truth", "estimate", "prob_estimate")]
  return(data_bin)

}

summary_multiclass_per_class <- function(predictions, new_data = "test"){

  class_metrics_funcs <- list(

    Accuracy = accuracy,
    Balanced_Accuracy = bal_accuracy,
    F1_score = f_meas,
    Recall = recall,
    Precision = precision,
    Specificity = specificity,
    Sensitivity = sensitivity,
    MCC = mcc,
    J_index = j_index,
    Detection_Prevalence = detection_prevalence,
    Kappa = kap

  )

  prob_metrics_funcs <- list(

    AUC_ROC = yardstick::roc_auc,
    AUC_PR = yardstick::pr_auc,
    Gain_Capture = yardstick::gain_capture,
    Brier_Score = yardstick::brier_class

  )

  y_classes = levels(predictions$y)

  prob_pred = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  results = lapply(y_classes, function(target_class){

    data_bin <- binarize_class(predictions, target_class)

    unlist(c(
      lapply(class_metrics_funcs, function(metric_fn){
        metric_fn(data_bin, truth = truth, estimate = estimate)$.estimate
      }),
      lapply(prob_metrics_funcs, function(metric_fn){
        metric_fn(data_bin, truth = truth, prob_estimate, event_level = "second")$.estimate
      })

    ))
  })

  names(results) <- y_classes

  results_df <- do.call(rbind, results)
  results_df <- as.data.frame(results_df)
  results_df["Class"] = y_classes
  results_df <- dplyr::relocate(results_df, "Class", .before = 1)

  rownames(results_df) <- NULL

  results_df <- results_df %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ base::signif(.x, 3)))

  return(results_df)

}

summary_multiclass_average <- function(predictions, new_data = "test"){

  metric_funcs <- list(

    Accuracy = function(data) accuracy(data, y, .pred_class),
    Balanced_Accuracy = function(data) bal_accuracy(data, y, .pred_class),
    Precision = function(data) precision(data, y, .pred_class),
    Recall = function(data) recall(data, y, .pred_class),
    Specificity = function(data) specificity(data, y, .pred_class),
    Sensitivity = function(data) sensitivity(data, y, .pred_class),
    Kappa = function(data) kap(data, y, .pred_class),
    F1_score = function(data) f_meas(data, y, .pred_class),
    MCC = function(data) mcc(data, y, .pred_class),
    J_index = function(data) j_index(data, y, .pred_class),
    Detection_Prevalence = function(data) detection_prevalence(data, y, .pred_class, event_level = "second"),
    AUC_ROC = function(data) yardstick::roc_auc(data, y, prob_pred, event_level = "second"),
    AUC_PR = function(data) yardstick::pr_auc(data, y, prob_pred, event_level = "second"),
    Gain_Capture = function(data) yardstick::gain_capture(data, y, prob_pred, event_level = "second"),
    Brier_Score = function(data) yardstick::brier_class(data, y, prob_pred, event_level = "second")

  )

  y_classes = levels(predictions$y)

  prob_pred = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  results <- lapply(metric_funcs, function(f) f(predictions)$.estimate)

  results <- as.data.frame(results)

  return(results)

}

########################################
#             Plots                  #
########################################

plot_roc_curve_multiclass <- function(predictions, new_data = "all"){

  y_classes = levels(predictions$y)

  predicted = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::roc_curve(y, predicted)

    return(curve_plot)

  }

}

plot_pr_curve_multiclass <- function(predictions, new_data = "all"){

  y_classes = levels(predictions$y)

  predicted = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::pr_curve(y, predicted)

    return(curve_plot)

  }


}

plot_gain_curve_multiclass <- function(predictions, new_data = "all"){

  y_classes = levels(predictions$y)

  predicted = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::gain_curve(y, predicted)

    return(curve_plot)

  }


}

plot_lift_curve_multiclass <- function(predictions, new_data = "all"){

  y_classes = levels(predictions$y)

  predicted = unlist(lapply(y_classes, function(target_class) paste0(".pred_", target_class)))

  if (new_data == "all"){

    curve_plot <- predictions %>%
      dplyr::group_by(data_set) %>%
      yardstick::lift_curve(y, predicted)

    return(curve_plot)

  }
}

plot_dist_probs_multiclass <- function(predictions, data_set = "all"){

  df_long <- predictions %>%
    dplyr::filter(data_set == data_set) %>%
    dplyr::select(-c(.pred_class)) %>%
    tidyr::pivot_longer(cols = dplyr::starts_with(".pred_"),
                 names_to = "Class",
                 values_to = "Probability") %>%
    dplyr::mutate(Class = base::sub("^\\.pred_", "Output ", Class))

  p <- ggplot2::ggplot(df_long, ggplot2::aes(x = Probability, fill = y, color = y)) +
    ggplot2::geom_density(alpha = 0.5, bw = 0.1) +
    ggplot2::facet_wrap(~Class) +  # Facet por clase verdadera
    ggplot2::labs(title = paste0("Probability Density for each Class (", data_set, " data)"),
         x = "Output", y = "Density") +
    ggplot2::theme_minimal()

  return(p)

}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/results_regression.R"
    ########################################
    #             Predictions              #
    ########################################

get_predictions_regression <- function(analysis_object, new_data = "test"){

  model_workflow <- analysis_object$final_model

  y = analysis_object$dep_var

  if (new_data == "all"){

      data_sets = c("train", "test")

    temp = list()

    for (data_set in data_sets){

      dat = analysis_object$data$raw[[paste0(data_set, "_data")]]

      predictions = predict(model_workflow, new_data = dat)
      predictions = cbind(predictions, y = dat[[y]])
      predictions$data_set = data_set

      temp[[data_set]] = predictions
    }

    predictions = rbind(temp[["train"]], temp[["test"]])

  } else {

    dat = analysis_object$data$raw[[paste0(new_data, "_data")]]

    predictions = predict(model_workflow, new_data = dat)
    predictions = cbind(predictions, y = dat[[y]])
    predictions$data_set = new_data

  }

  return (predictions)
}



    ########################################
    #             SUMMARY                  #
    ########################################

summary_regression <- function(predictions, new_data = "test"){

  metric_funcs <- list(

    RMSE = function(data) rmse(data, y, .pred),
    MAE = function(data) mae(data, y, .pred),
    MAPE = function(data) mape(data, y, .pred),
    MPE = function(data) mpe(data, y, .pred),
    CCC = function(data) ccc(data, y, .pred),
    SMAPE = function(data) smape(data, y, .pred),
    RPIQ = function(data) rpiq(data, y, .pred),
    RSQ = function(data) rsq(data, y, .pred)

  )

  results <- lapply(metric_funcs, function(f) f(predictions)$.estimate)

  results <- as.data.frame(results)

  rownames(results) <- new_data

  results <- results %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~ base::signif(.x, 3)))

  return(results)


}

    ########################################
    #             PLOTS                    #
    ########################################


plot_scatter <- function(predictions, new_data = "test", error = F){

  if (error == T){

    predictions %>%
      dplyr::mutate(error = y - .pred) %>%

      ggplot2::ggplot(ggplot2::aes(x = .pred, y = error)) +
      ggplot2::geom_point() +
      ggplot2::labs(title = paste0("Residuals vs Predictions (", new_data, " set)"), , x = "Predictions", y = "Residuals") +
      ggplot2::theme_minimal()

  } else {

    predictions %>%
      ggplot2::ggplot(ggplot2::aes(x = .pred, y = y)) +
      ggplot2::geom_point() +
      ggplot2::labs(title = paste0("Observed vs Predictions (", new_data, " set)"), x = "Predictions", y = "Observed") +
      ggplot2::theme_minimal()
  }

}

plot_residuals_density <- function(predictions, new_data = "test") {
  predictions %>%
    dplyr::mutate(error = y - .pred) %>%
    ggplot2::ggplot(ggplot2::aes(x = error)) +
    ggplot2::geom_histogram(ggplot2::aes(y = ggplot2::after_stat(density)),
                            bins = 30, fill = "lightgray", color = "white", alpha = 0.5) +
    ggplot2::geom_density(color = "steelblue", linewidth = 1.2, alpha = 0.6) +
    ggplot2::labs(title = paste0("Residual Density (", new_data, " set)"),
                  x = "Residuals", y = "Density") +
    ggplot2::theme_minimal()
}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/shap_values.R"
shap_calc <- function(model, train, test, y, task, outcome_levels, use_test = FALSE){

  if (!use_test){

    test <- train

  }

  if (task == "regression"){

    shap_reg(model, train, test, y)

  } else if (task == "classification"){

    if (outcome_levels == 2){

      shap_bin(model, train, test, y)

    } else {

      shap_mul(model, train, test, y)

    }

  }

}



###########################
#     Regression          #
###########################

shap_reg <- function(model, train, test, y){

  y_vals = train[[y]]

  train <- train[which(names(train) != y)]
  test <- test[which(names(test) != y)]


  shap_vals <- fastshap::explain(model, X = as.data.frame(train),
                                 pred_wrapper = pred_reg, nsim = 50, adjust = T,
                                 newdata = as.data.frame(test))

  shap_vals <- as.data.frame(shap_vals)

  return(shap_vals)

}

##################################
#     Binary Classification      #
##################################

shap_bin <- function(model, train, test, y){

  target_class = levels(train[[y]])[1]

  y_vals = factor(ifelse(train[[y]] == target_class, 1, 0), levels = c(0,1))

  phi0 = mean(y_vals == levels(y_vals)[1])

  train <- train[which(names(train) != y)]
  test <- test[which(names(test) != y)]

  shap_vals <- fastshap::explain(model, X = as.data.frame(train),
                                 pred_wrapper = pred_bin, nsim = 50, adjust = T,
                                 newdata = as.data.frame(test))

  shap_vals <- as.data.frame(shap_vals)

  return(shap_vals)

}

######################################
#     Multiclass Classification      #
######################################

shap_mul <- function(model, train, test, y){

  results = list()

  y_classes = levels(train[[y]])

  new_train <- train[which(names(train) != y)]
  new_test <- test[which(names(test) != y)]

  for (target_class in y_classes){

    y_vals = factor(ifelse(test[[y]] == target_class, 1, 0), levels = c(0,1))

    pred_class = paste0(".pred_", target_class)

    phi0 = mean(y_vals == 1)

    pred_func <- function(object, newdata){return(predict(model, newdata, type = "prob")[[pred_class]])}

    shap_vals <- fastshap::explain(model, X = as.data.frame(new_train),
                                   pred_wrapper = pred_func, nsim = 50, adjust = T,
                                   newdata = as.data.frame(new_test))

    results[[target_class]] <- as.data.frame(shap_vals)


  }

  return(results)

}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/sim_data.R"
#' sim_data
#'
#' This dataset, included in the MLwrap package, is a simulated dataset
#' (Martínez-García et al., 2025) designed to capture relationships among
#' psychological and demographic variables influencing psychological wellbeing,
#' the primary outcome variable. It comprises data for 1,000 individuals.
#'
#' The predictor variables include gender (50.7% female), age (range: 18-85
#' years, mean = 51.63, median = 52, SD = 17.11), and socioeconomic status,
#' categorized as Low (n = 343), Medium (n = 347), and High (n = 310).
#' Additional predictors (features) are emotional intelligence (range: 24-120,
#' mean = 71.97, median = 71, SD = 23.79), resilience (range: 4-20,
#' mean = 11.93, median = 12, SD = 4.46), life satisfaction (range: 5-35,
#' mean = 20.09, median = 20, SD = 7.42), and depression (range: 0-63,
#' mean = 31.45, median = 32, SD = 14.85). The primary outcome variable is
#' emotional wellbeing, measured on a scale from 0 to 100 (mean = 50.22,
#' median = 49, SD = 24.45).
#'
#' The dataset incorporates correlations as conditions for the simulation.
#' Psychological wellbeing is positively correlated with emotional intelligence
#' (r = 0.50), resilience (r = 0.40), and life satisfaction (r = 0.60),
#' indicating that higher levels of these factors are associated with better
#' emotional health outcomes. Conversely, a strong negative correlation exists
#' between depression and psychological wellbeing (r = -0.80), suggesting that
#' higher depression scores are linked to lower emotional wellbeing. Age shows
#' a slight positive correlation with emotional wellbeing (r = 0.15),
#' reflecting the expectation that older individuals might experience greater
#' emotional stability. Gender and socioeconomic status are included as
#' potential predictors, but the simulation assumes no statistically
#' significant differences in psychological wellbeing across these categories.
#'
#' Additionally, the dataset includes categorical transformations of
#' psychological wellbeing into binary and polytomous formats: a binary version
#' ("Low" = 477, "High" = 523) and a polytomous version with four levels: "Low"
#' (n = 161), "Somewhat" (n = 351), "Quite a bit" (n = 330), and "Very much"
#' (n = 158). The polytomous transformation uses the 25th, 50th, and 75th
#' percentiles as thresholds for categorizing psychological wellbeing scores.
#' These transformations enable analyses using machine learning models for
#' regression (continuous outcome) and classification (binary or polytomous
#' outcomes) tasks.
#' @docType data
#' @name sim_data
#' @usage data(sim_data)
#' @format A data frame with 1,000 rows and 10 columns:
#' \describe{
#'   \item{psych_well}{Psychological Wellbeing Indicator. Continuous with (0,100)}
#'   \item{psych_well_bin}{Psychological Wellbeing Binary Indicator. Factor with ("Low", "High")}
#'   \item{psych_well_pol}{Psychological Wellbeing Polytomic Indicator. Factor with ("Low", "Somewhat", "Quite a bit", "Very Much")}
#'   \item{gender}{Patient Gender. Factor ("Female", "Male")}
#'   \item{age}{Patient Age. Continuous (18, 85)}
#'   \item{socioec_status}{Socioeconomial Status Indicator. Factor ("Low", "Medium", "High")}
#'   \item{emot_intel}{Emotional Intelligence Indicator. Continuous (24, 120)}
#'   \item{resilience}{Resilience Indicator. Continuous (4, 20)}
#'   \item{depression}{Depression Indicator. Continuous (0, 63)}
#'   \item{life_sat}{Life Satisfaction Indicator. Continuous (5, 35)}
#' }
#' @section Test Performance Exceeding Training Performance:
#'
#' If machine learning models, including SVMs, show better evaluation metrics
#' on the test set than the training set, this anomaly usually signals
#' methodological issues rather than genuine model quality. Typical causes
#' reported in the literature (Hastie et al., 2017) include:
#'
#' \itemize{
#'   \item \strong{Statistical variance in small samples}: Random train-test
#'         splits may produce partitions where the test set contains
#'         easier-to-classify examples by chance, especially with small sample
#'         sizes or difficult tasks (Vabalas et al., 2019; An et al., 2021).
#'   \item \strong{Synthetic data characteristics}: Simulated data may contain
#'         artificial patterns or non-uniform distributions that create easier
#'         test sets compared to training sets.
#'   \item \strong{Excessive regularization}: High regularization parameters
#'         may limit model capacity to fit training data while paradoxically
#'         generalizing better to simpler test patterns, indicating
#'         underfitting.
#'   \item \strong{Train-test contamination}: Preprocessing (scaling,
#'         normalization) performed before train-test split leaks statistical
#'         information from test to train, producing overoptimistic performance
#'         estimates (Kapoor &  Narayanan, 2023).
#'   \item \strong{Kernel-data interaction}: Inappropriate kernel parameters
#'         may create decision boundaries that better fit test distribution
#'         than training distribution.
#' }
#' \strong{MLwrap implementation:}
#' MLwrap's hyperparameter optimization (via Bayesian Optimization or Grid
#' Search CV) implements 5-fold cross-validation during the tuning process,
#' which provides more robust parameter selection than single train-test
#' splits. Users should examine evaluation metrics across both training and
#' test sets, and review diagnostic plots (residuals, predictions) to identify
#' potential distribution differences between partitions. When working with
#' small datasets where partition variability may be substantial, running the
#' complete workflow with different random seeds can help assess the stability
#' of results and conclusions. The \code{sim_data} dataset included in MLwrap
#' is a simulated matrix provided for demonstration purposes only. As
#' synthetic data, it may occasionally exhibit some of these anomalous
#' phenomena (e.g., better test than training performance) due to artificial
#' patterns in the data generation process. Users working with real-world data
#' should always verify results through careful examination of evaluation
#' metrics and diagnostic plots across multiple runs.
#' @references
#' An, C., Park, Y. W., Ahn, S. S., Han, K., Kim, H., & Lee, S. K. (2021).
#' Radiomics machine learning study with a small sample size: Single random
#' training-test set split may lead to unreliable results. \emph{PLOS ONE},
#' \emph{16}(8), e0256152. \doi{10.1371/journal.pone.0256152}
#'
#' Hastie, T., Tibshirani, R., & Friedman, J. (2017). \emph{The elements of
#' statistical learning: Data mining, inference, and prediction} (2nd ed.,
#' corrected 12th printing, Chapter 7). Springer.
#' \doi{10.1007/978-0-387-84858-7}
#'
#' Kapoor, S., & Narayanan, A. (2023). Leakage and the reproducibility crisis
#' in machine-learning-based science. \emph{Patterns}, \emph{4}(9), 100804.
#' \doi{10.1016/j.patter.2023.100804}
#'
#' Martínez-García, J., Montaño, J. J., Jiménez, R., Gervilla, E.,
#' Cajal, B., Núñez, A., Leguizamo, F., & Sesé, A. (2025).
#' Decoding Artificial Intelligence: A Tutorial on Neural Networks
#' in Behavioral Research. \emph{Clinical and Health, 36}(2), 77-95.
#' \doi{10.5093/clh2025a13}
#'
#' Vabalas, A., Gowen, E., Poliakoff, E., & Casson, A. J. (2019). Machine
#' learning algorithm validation with a limited sample size. \emph{PLOS ONE},
#' \emph{14}(11), e0224365. \doi{10.1371/journal.pone.0224365}
#'
NULL


#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/summary.AnalysisObject.R"
#' @export
summary.AnalysisObject <- function(object, ...) {

  y = all.vars(object$formula)[1]

  full_data <- object$full_data

  features_names <- names(full_data)[which(names(full_data) != y)]

  cli::cat_line()
  cli::cli_h1("Summary of AnalysisObject")
  cli::cat_line()
  cli::cli_text("Stage: ", object$stage)
  cli::cli_text("Outcome Variable: ", y)
  cli::cli_text("Features: ")
  cli::cat_line()
  cli::cli_bullets(features_names)
  cli::cat_line()
  cli::cli_text("Task: ", object$task)
  cli::cli_h2("Preprocessor Steps")
  print(object$transformer$steps)

  if (object$stage == "build_model"){

    cli::cli_h2("Model Specification")
    cli::cat_line()
    cli::cli_text("Model Name: ", object$model_name)
    cli::cli_text("Model:")
    print(object$model)

  }

  if (object$stage == "fit_model"){

    cli::cli_h2("Final Model Fit")
    cli::cat_line()
    print(tune::extract_fit_parsnip(object$final_model))
    cli::cat_line()

    cli::cli_h2("Summary Results")
    print(table_evaluation_results(object))

  }

  invisible(object)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/table_utils.R"
### Tuning Results
#' Best Hyperparameters Configuration
#'
#' @name table_best_hyperparameters
#' @aliases table_best_hyperparameters
#'
#' @description
#'
#' The **table_best_hyperparameters()** function extracts and presents the
#' optimal hyperparameter configuration identified during the model fine-tuning
#' process. This function validates that the model has been properly trained
#' and that hyperparameter tuning has been performed, combining both constant
#' and optimized hyperparameters to generate a comprehensive table with the
#' configuration that maximizes performance according to the specified primary
#' metric. The function includes optional interactive visualization
#' capabilities through the show_table parameter.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble with best hyperparameter configuration.
#' @examples
#' # Note: For obtaining hyoperparameters table the user needs to
#' # complete till fine_tuning( ) function.
#'
#' set.seed(123) # For reproducibility
#' wrap_object <- preprocessing(df = sim_data[1:300 ,],
#'                              formula = psych_well ~ depression + resilience,
#'                              task = "regression")
#' wrap_object <- build_model(wrap_object, "Random Forest",
#'                            hyperparameters = list(mtry = 2, trees = 3))
#' wrap_object <- fine_tuning(wrap_object, "Grid Search CV")
#'
#' # And then, you can obtain the best hyperparameters table.
#'
#' table_best_hyp <- table_best_hyperparameters(wrap_object)
#' @export
table_best_hyperparameters <- function(analysis_object, show_table = FALSE){

  if (analysis_object$stage != "fit_model"){

    stop("You must first fit a model with 'train_model()'!")

  }

  if (is.null(analysis_object$tuner_fit)){

    stop("All hyperparameters had fixed values, no hyperparameter tuning was performed!")

  }

  best_hyper <- tune::show_best(analysis_object$tuner_fit, metric = analysis_object$metrics[1], n = 1)

  best_hyper <- c(analysis_object$hyperparameters$hyperparams_constant,
                         as.list(best_hyper))

  best_hyp <- tibble::as_tibble(best_hyper)

  if (base::interactive() && show_table == TRUE){

    cli::cat_line()

    cli::cli_h1("Best Hyperparameter Configuration")

    cli::cat_line()

    print(best_hyp)

  }

  invisible(best_hyp)

}


### Results

#' Evaluation Results
#'
#' @description
#'
#' The **table_evaluation_results()** function provides access to trained model
#' evaluation metrics, automatically adapting to the type of problem being
#' analyzed. For binary classification problems, it returns a unified table
#' with performance metrics, while for multiclass classification it generates
#' separate tables for training and test data, enabling comparative performance
#' evaluation and detection of potential overfitting.
#'
#' @param analysis_object Fitted analysis_object with 'fine_tuning()'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with
#' evaluation results.
#' @examples
#' # Note: For obtaining the evaluation table the user needs to
#' # complete till fine_tuning( ) function.
#' # See the full pipeline example under table_best_hyperparameters()
#' # Final call signature:
#' # table_evaluation_results(wrap_object)
#' @seealso \code{\link{table_best_hyperparameters}}
#' @export
table_evaluation_results <- function(analysis_object, show_table = FALSE){

  if (analysis_object$stage != "fit_model"){

    stop("You must first fit a model with 'train_model()'!")

  }

  tables <- analysis_object$tables

  result = list()

  if (analysis_object$outcome_levels > 2){

    result$summary_train <- tibble::as_tibble(tables$summary_train)

    result$summary_test <- tibble::as_tibble(tables$summary_test)

  } else {

    result <- tibble::as_tibble(tables$summary_results)

    result$Dataset <- c("Train", "Test")

    result <- result %>% dplyr::select("Dataset", dplyr::everything())

  }

  if (base::interactive() && show_table){

    cli::cli_h1("Evaluation Results")

    cli::cat_line()

    if (analysis_object$outcome_levels > 2){

      cli::cli_h2("Train Data Evaluation Results")

      print(tables$summary_train)

      cli::cat_line()

      cli::cli_h2("Test Data Evaluation Results")

      print(tables$summary_test)

    } else {

      print(result)


    }

  }

  invisible(result)

}

#### Sensitivity Analysis

#' Permutation Feature Importance Results Table
#'
#' @description
#'
#' The **table_pfi_results()** function extracts Permutation Feature Importance
#' results, a model-agnostic technique that evaluates variable importance
#' through performance degradation when randomly permuting each feature's values.
#'
#' @param analysis_object Fitted analysis_object with 'sensitivity_analysis(methods = "PFI")'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with PFI
#' results.
#' @examples
#' # Note: For obtaining the table with PFI method results the user needs to
#' # complete till sensitivity_analysis() function of the
#' # MLwrap pipeline using PFI method
#'
#' set.seed(123) # For reproducibility
#' wrap_object <- preprocessing(df = sim_data[1:300 ,],
#'                              formula = psych_well ~ depression + emot_intel,
#'                              task = "regression")
#' wrap_object <- build_model(wrap_object, "Random Forest",
#'                            hyperparameters = list(mtry = 2, trees = 3))
#' wrap_object <- fine_tuning(wrap_object, "Grid Search CV")
#' wrap_object <- sensitivity_analysis(wrap_object, methods = "PFI")
#'
#' # And then, you can obtain the PFI results table.
#'
#' table_pfi <- table_pfi_results(wrap_object)
#' @export
table_pfi_results <- function(analysis_object, show_table = FALSE){

  tables <- analysis_object$tables

  pfi_names   <- grep("^PFI", names(tables), value = TRUE)

  if (length(pfi_names) == 0){

    stop("You first need to compute PFI values using 'sensitivity_analysis()'!")

  }

  pfi_tables <- tables[["PFI"]]

  if (base::interactive() && show_table){

    cli::cat_line()

    cli::cli_h1("Permutation Feature Importance Results")

    if (analysis_object$outcome_levels > 2){

      y_classes <- unique(pfi_tables$output_class)

      for (target_class in y_classes){

        pfi_target <- pfi_tables %>%
          dplyr::filter(output_class == target_class) %>%
          dplyr::select(-output_class)

        cli::cli_h2(sub(".*_", "", target_class))

        print(pfi_target)

        cli::cat_line()

      }

    } else{

      print(pfi_tables)

    }

  }

  invisible(pfi_tables)

}

#' SHAP Summarized Results Table
#'
#' @description
#'
#' The **table_shap_results()** function processes previously calculated SHAP
#' (SHapley Additive exPlanations) values and generates summarized metrics
#' including mean absolute value, standard deviation of mean absolute value,
#' and a directional sensitivity value calculated as the covariance between
#' feature values and SHAP values divided by the variance of feature values.
#' This directional metric provides information about the nature of the
#' relationship between each variable and model predictions. To summarize the
#' SHAP values calculated, three different metrics are computed:
#'
#' * **Mean Absolute Value**
#' * **Standard Deviation of Mean Absolute Value**
#' * **Directional Sensitivity Value** (Cov(Feature values, SHAP values) / Var(Feature values))
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "SHAP")'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with SHAP
#' summarized results.
#' @examples
#' # Note: For obtaining the table with SHAP method results the user needs
#' # to complete till sensitivity_analysis() function of the
#' # MLwrap pipeline using the SHAP method.
#' # See the full pipeline example under sensitivity_analysis
#' # (Requires sensitivity_analysis(methods = "SHAP"))
#' # Final call signature:
#' # table_shap_results(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
table_shap_results <- function(analysis_object, show_table = FALSE){

  tables <- analysis_object$tables

  shap_names   <- grep("^SHAP", names(tables), value = TRUE)

  if (length(shap_names) == 0){

    stop("You first need to compute SHAP values using 'sensitivity_analysis()'!")

  }

  shap_tables <- tables[["SHAP"]]

  if (base::interactive() && show_table){

    cli::cat_line()

    cli::cli_h1("SHAP Importance Results")

    if (analysis_object$outcome_levels > 2){

      y_classes <- unique(shap_tables$output_class)

      for (target_class in y_classes){

        shap_target <- shap_tables %>%
          dplyr::filter(output_class == target_class) %>%
          dplyr::select(-output_class)

        cli::cli_h2(sub(".*_", "", target_class))

        print(shap_target)

        cli::cat_line()

      }

    } else{

      print(shap_tables)

    }

  }

  invisible(shap_tables)

}

#' Integrated Gradients Summarized Results Table
#'
#' @description
#'
#' The **table_integrated_gradients_results()** function implements a
#' summarized metrics scheme for Integrated Gradients values. This methodology,
#' specifically designed for neural networks, calculates feature importance
#' through gradient integration along paths from baseline to input. Three
#' different metrics are computed:
#'
#' * **Mean Absolute Value**
#' * **Standard Deviation of Mean Absolute Value**
#' * **Directional Sensitivity Value** (Cov(Feature values, IG values) / Var(Feature values))
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Integrated Gradients")'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with
#' Integrated Gradient summarized results.
#' @examples
#' # Note: For obtaining the table with Integrated Gradients method results
#' # the user needs to complete till sensitivity_analysis() function of the
#' # MLwrap pipeline using the Integrated Gradient method.
#' # See the full pipeline example under sensitivity_analysis
#' # (Requires sensitivity_analysis(methods = "Integrated Gradients"))
#' # Final call signature:
#' # table_integrated_gradients_results(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
table_integrated_gradients_results <- function(analysis_object, show_table = FALSE){

  tables <- analysis_object$tables

  ig_names   <- grep("^Integrated", names(tables), value = TRUE)

  if (length(ig_names) == 0){

    stop("You first need to compute Integrated Gradients values using 'sensitivity_analysis()'!")

  }

  ig_tables <- tables[["IntegratedGradients"]]

  if (base::interactive() && show_table){

    cli::cat_line()

    cli::cli_h1("Integrated Gradients Importance Results")

    if (analysis_object$outcome_levels > 2){

      y_classes <- unique(ig_tables$output_class)

      for (target_class in y_classes){

        ig_target <- ig_tables %>%
          dplyr::filter(output_class == target_class) %>%
          dplyr::select(-output_class)

        cli::cli_h2(sub(".*_", "", target_class))

        print(ig_target)

        cli::cat_line()

      }

    } else{

      print(ig_tables)

    }

  }

  invisible(ig_tables)

}

#' Olden Results Table
#'
#' @description
#'
#' The **table_olden_results()** function extracts results from the Olden
#' method, a technique specific to neural networks that calculates relative
#' importance of input variables through analysis of connection weights between
#' network layers. This method provides a measure of each variable's
#' contribution based on the magnitude and direction of synaptic connections.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Olden")'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with Olden
#' results.
#' @examples
#' # Note: For obtaining the table with Olden method results the user needs to
#' # complete till sensitivity_analysis() function of the MLwrap pipeline using
#' # the Olden method. Remember Olden method only can be used with neural
#' # network model.
#' # See the full pipeline example under sensitivity_analysis
#' # (Requires sensitivity_analysis(methods = "Olden"))
#' # Final call signature:
#' # table_olden_results(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
table_olden_results <- function(analysis_object, show_table = FALSE){

  olden <- analysis_object$tables$Olden

  if (is.null(olden)){

    stop("You first need to compute Olden values using 'sensitivity_analysis()'!")

  }

  if (analysis_object$outcome_levels < 2){

    olden <- olden[order(-abs(olden$Importance)), ]

  }

  if (base::interactive() && show_table){

    cli::cat_line()

    cli::cli_h1("Olden Importance Results")

    print(olden)

    cli::cat_line()

  }

  invisible(olden)

}

#' Sobol-Jansen Results Table
#'
#' @description
#'
#' The **table_sobol_jansen_results()** function processes results from
#' Sobol-Jansen global sensitivity analysis, a variance decomposition-based
#' methodology that quantifies each variable's contribution and their
#' interactions to the total variability of model predictions. This technique
#' is particularly valuable for identifying higher-order effects and complex
#' interactions between variables.
#'
#' @param analysis_object Fitted analysis_object with
#' 'sensitivity_analysis(methods = "Sobol_Jansen")'.
#' @param show_table Boolean. Whether to print the table.
#' @returns Tibble or list of tibbles (multiclass classification) with
#' Sobol-Jansen results.
#' @examples
#' # Note: For obtaining the table with Sobol_Jansen method results the user
#' # needs to complete till sensitivity_analysis() function of the MLwrap
#' # pipeline using the Sobol_Jansen method. Sobol_Jansen method only works
#' # when all input features are continuous.
#' # See the full pipeline example under sensitivity_analysis
#' # (Requires sensitivity_analysis(methods = "Sobol_Jansen"))
#' # Final call signature:
#' # table_sobol_jansen_results(wrap_object)
#' @seealso \code{\link{sensitivity_analysis}}
#' @export
table_sobol_jansen_results <- function(analysis_object, show_table = FALSE){

  ### Check_args

  sobol <- analysis_object$tables$Sobol_Jansen

  if (is.null(sobol)){

    stop("You first need to compute sobol_jansen values using 'sensitivity_analysis()'!")

  }

  if (base::interactive() && show_table){

    cli::cli_h1("Sobol_Jansen Importance Results")

    print(sobol)

    cli::cat_line()

  }


  invisible(sobol)

}
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/tutorial.R"
#' MLwrap Comprehensive Tutorial
#'
#' A comprehensive tutorial demonstrating the complete MLwrap workflow is
#' available. The tutorial provides detailed guidance on data preprocessing,
#' model building, hyperparameter tuning, model evaluation, and sensitivity
#' analysis across all supported machine learning algorithms (Neural Networks,
#' Random Forests, SVM, and XGBoost) within the Knowledge Discovery in
#' Databases (KDD) framework.
#'
#' **Citation**: Jiménez, R., Martínez-García, J., Montaño, J. J., & Sesé, A.
#' (2025). *MLwrap: Simplifying Machine Learning workflows in R. PsyarXiv.*
#' \doi{10.31234/osf.io/j6m4z_v1}
#'
#' @section Preprint:
#' Available at \doi{10.31234/osf.io/j6m4z_v1}
#' @section Why consult the tutorial:
#' While MLwrap provides a streamlined and user-friendly interface for
#' implementing machine learning workflows, the underlying models represent
#' sophisticated algorithms with substantial theoretical and computational
#' complexity. The tutorial bridges this gap by explaining the rationale
#' behind preprocessing decisions, hyperparameter choices, and interpretation
#' of model outputs. Understanding these concepts ensures appropriate
#' application of the methods, proper interpretation of results, and awareness
#' of potential limitations in specific contexts.
#'
#' The tutorial demonstrates practical applications through complete workflows,
#' helping users navigate the balance between methodological rigor and
#' implementation simplicity that MLwrap offers. This is particularly valuable
#' for researchers transitioning from traditional statistical methods to
#' machine learning approaches, or those seeking to ensure reproducible and
#' theoretically sound applications in their work.
#'
#' Users are strongly encouraged to consult the tutorial for detailed examples
#' and best practices.
#' @section Tutorial for implementing ML with Python:
#' This paper is also interesting for ML users as it serves as a primer
#' for estimating ML models using Python code, particularly in the context of
#' Social, Health, and Behavioral research.
#'
#' Martínez-García, J., Montaño, J. J., Jiménez, R., Gervilla, E.,
#' Cajal, B., Núñez, A., Leguizamo, F., & Sesé, A. (2025).
#' Decoding Artificial Intelligence: A Tutorial on Neural Networks
#' in Behavioral Research. \emph{Clinical and Health, 36}(2), 77-95.
#' \doi{10.5093/clh2025a13}
#' @return Character string with the arXiv URL
#'
#' @name tutorial
#' @rdname tutorial
#' @export
#' @examples
#' MLwrap_tutorial()
#'
MLwrap_tutorial <- function() {
  "https://doi.org/10.31234/osf.io/j6m4z_v1"
}

#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/utils-pipe.R"
#' Pipe operator
#'
#' See \code{magrittr::\link[magrittr:pipe]{\%>\%}} for details.
#'
#' @name %>%
#' @rdname pipe
#' @keywords internal
#' @export
#' @importFrom magrittr %>%
#' @usage lhs \%>\% rhs
#' @param lhs A value or the magrittr placeholder.
#' @param rhs A function call using the magrittr semantics.
#' @return The result of calling `rhs(lhs)`.
NULL

#' @importFrom stats as.formula coef model.frame predict sd update
NULL
#line 1 "/private/var/folders/x5/8z0mshcj4gb4__ckxrrmpg480000gn/T/Rtmpfwi3am/R.INSTALL1571264024fc1/MLwrap/R/zzz.R"
.onAttach <- function(libname, pkgname) {
  version <- read.dcf(
    file = system.file("DESCRIPTION", package = pkgname),
    fields = "Version"
  )
  packageStartupMessage("
*****************************************************************************

ooo        ooooo ooooo
 88.       .888   888
 888b     d 888   888         oooo oooo    ooo oooo d8b  .oooo.   oo.ooooo.
 8 Y88. .P  888   888           88.  88.  .8    888 8P  P  )88b   888   88b
 8   888    888   888            88..]88..8     888      .oP 888   888   888
 8    Y     888   888       o     888  888      888     d8(  888   888   888
o8o        o888o o888ooooood8      8    8      d888b     Y888  8o  888bod8P
                                                                   888
                                                                  o888o

*****************************************************************************

      MLwrap v", version, ": **Start simple, scale smart**
  ")
}

utils::globalVariables(c(
  ".", "x", "y", ".pred", "error", "value", "variable",
  "Class", "Importance", "Probability", "data_set", "label",
  "type", "density", "se", "S1", "ST", "prob_pred", "prob_observed",
  "truth", "estimate", "prob_estimate", "search_res", "sym",
  "tidy_object", "all_of", ".pred_class", "importance", "Variable", "StDev", "val_color", "iter",
  "Feature"
))
